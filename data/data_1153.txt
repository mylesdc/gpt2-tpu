Introduction Healthy bowel function is an important factor when judging the advisability of early enteral nutrition in critically ill patients, but long-term observation and objective evaluation of gastrointestinal motility are diffi cult. We developed a non-invasive monitoring system capable of quantifying and visualizing gastrointestinal motility in real time. In the study gastrointestinal motility was performed in patients with severe sepsis using this developed bowel sound analysis system, and the correlation between bowel sounds and changes over time in blood concentrations of IL-6, which is associated with sepsis severity, was evaluated. Methods The study was a prospective, observational pilot study conducted in our hospital. Consecutive adult patients with severe sepsis, on a mechanical ventilator with an IL-6 blood concentration ≥100 pg/ml in the acute phase, defi ned as being up to the 28th day of illness in the ICU, were entered in this study between June 2011 and December 2012. Subjects were divided into those who were treated with steroids (steroid treatment group) and those who were not (nosteroids group) during the target period, because steroids strongly aff ect IL-6 blood levels. Results The subjects were fi ve adult patients in the acute phase of severe sepsis on a mechanical ventilator. Gastrointestinal motility was measured for a total of 62,399 minutes: 31,544 minutes in three subjects in the no-steroids group and 30,855 minutes in two subjects in the steroid treatment group. In the no-steroids group, the bowel sound counts were negatively correlated with IL-6 blood concentration (r = -0.76, P <0.01), suggesting that gastrointestinal motility was suppressed as IL-6 blood concentration increased. However, in the steroid treatment group, gastrointestinal motility showed no correlation with IL-6 blood concentration (r = -0.25, P = 0.27). The IL-6 blood concentration appears to have decreased with steroid treatment irrespective of changes in the state of sepsis, whereas bowel sound counts with the monitoring system refl ected the changes in the state of sepsis, resulting in no correlation. Conclusion The new real-time bowel sound analysis system provides a useful method of continuously, quantitatively, and non-invasively evaluating gastrointestinal motility in severe patients. Furthermore, this analysis may predict disease severity in septic patients.
Introduction Severe sepsis results in ~36,800 UK deaths each year [1] . Prior studies demonstrate the benefi t of early recognition and treatment of sepsis in reducing mortality [2] . The Sepsis Six [1] bundle aims to optimise the fi rst hour of sepsis management. We assessed the proportion of emergency department (ED) patients with severe sepsis receiving the Sepsis Six bundle and whether this was improved by a combination of staff education and use of Sepsis Six management stickers in patient notes. Methods A closed loop audit was completed in the ED at Ipswich Hospital, UK. Each cycle was 14 days with interventions made in a 4-week period between the two cycles. The interventions consisted of: Sepsis Six management stickers and posters placed in the ED; two training sessions for all ED nurses on sepsis recognition and management; a teaching session for all middle-grade doctors; and a trolley in the ED with equipment required for the Sepsis Six. The notes of all patients with lactate ≥2 mmol/l were retrospectively reviewed. Those with ≥2 systemic infl ammatory response syndrome criteria and a documented suspicion of infection were deemed to have severe sepsis. The times at which these patients had each of the Sepsis Six completed were recorded, as were the fi nal diagnosis and 7/28 day mortality. Results In Cycle 1, 31/106 patients met the criteria for severe sepsis, compared with 36/120 in Cycle 2. The delivery of the Sepsis Six interventions was highly variable. In Cycle 1 lactate levels and i.v. access had the highest 60-minute completion rates (90.3%, 83.9% respectively). Blood cultures and i.v. fl uid resuscitation were completed for 61.3% and 64.5% of patients within 60 minutes. Only 38.7% of septic patients were given i.v. antibiotics within 60 minutes. In total, 58.9% of patients received antibiotics in accordance with trust guidelines. High fl ow oxygen, catheters and fl uid balance charts had the lowest 60-minute completion rates (35.5%, 6.5%, 6.5% respectively). In Cycle 2, post intervention, there was no signifi cant change in the percentage of patients receiving the Sepsis Six bundle. Conclusion The low rates of Sepsis Six completion require improvement to meet the targets set out by the College of Emergency Medicine. Our results suggest that simple interventions are ineff ective in increasing Sepsis Six completion and thus lend support to the case for integrated interventions such as electronic recording and alert systems. References wound infection with the following 14 preoperative characteristics and perioperative parameters: age >75, female gender, diabetes mellitus (DM), insulin dependence, body mass index (BMI) >30, current smokers, COPD, cardiopulmonary bypass time (CBP) >120 minutes, use of steroids, emergency operation, prolonged mechanical ventilation (>48 hours), reintubation, transfusion with more than 3 units of red blood cells, and the postoperative use of non-invasive ventilation (NIV). The chi-square test was used for statistical analysis. Results A total of 35 patients (3.44%) were complicated by deep sternal wound infections. No statistical correlation was found with age >75, gender, DM, BMI >30, steroids, emergent operation, prolonged ventilation, CBP time >120 minutes, reintubation and NIV. Factors with statistical signifi cant correlation are presented in Table 1 . Conclusion Postoperative deep sternal wound infections have statistical signifi cant correlation with the following parameters: transfusion with >3 red blood cell units, history of COPD, insulin dependence and when the patient is a current smoker. Also there is a tendency for correlation with CBP time >120 minutes (P = 0.056).
Introduction Blood culture is a critical procedure for detecting potentially life-threatening bloodstream infections (BSI). At the same time, early diagnosis and appropriate treatment of BSI are the key factors in order to improve prognosis. The purpose of the current analysis was to identify risk factors for bacteremia in adult febrile patients in emergency settings. Methods We conducted a retrospective case-control study within a population of adult patients visiting the emergency department at a community hospital (St Luke's International Hospital, Tokyo, Japan) and who underwent two sets of blood culture testing between 2003 and 2012. Among a total of 13,582 patients, 1,322 (10%) were detected as bacteremia. We included in this study 179 randomly selected patients from the bacteremia group and 321 randomly selected patients from the negative blood culture group to serve as the comparison group. Multivariate logistic regression was used to evaluate the relationship between clinical characteristics factors and bacteremia. Introduction The aim of this study was to evaluate the eff ect of the Surviving Sepsis Campaigns on mortality rates, before and after the second surviving sepsis publication, and to assess whether patients with sepsis being admitted to the ICU had a lower APACHE II score on admission. Patients with sepsis, who require ICU care, have an extremely poor prognosis. It has been shown that the mortality rates range from 20.7% (severe sepsis) to 45.7% (septic shock) [1] . The surviving sepsis campaign was initiated in 2002. The fi rst, second and third publications were published in 2004, 2008 and 2012 respectively [2] . Methods A retrospective case note review was performed, looking at a sample of 5,954 patients who were 18 years or older who had been admitted to East Surrey Hospital (ESH) ICU between 1 January 2005 and 31 October 2014. The total number of patients with sepsis was 941. We compared results before and after the second publication of the surviving sepsis campaign, looking at mortality rates, age of patients, admission length prior to ICU transfer, APACHE II score and the length of stay on the ICU. Results From the beginning of 2005 to the end of 2008, the mortality rates for septic patients was 51.9% compared with 41.3% from the beginning of 2009 to end of October 2014. Fisher's two-tailed test showed a signifi cant diff erence (P = 0.003) between the mortality before and after the second publication. The median ages before and after 2009 were 63.9 and 64.8 years. The time in hospital before admission to the ICU was greater before 2009 (6.15 days) compared with after 2009 (5.53 days). There was no signifi cant diff erence (Mann-Whitney test) between the APACHE II scores, with the mean and median score the same at 17.6 and 18 for both groups. The mean length of stay was 1 day longer after 2009 (8.07 days compared with 9.07 days). Conclusion Patients with sepsis admitted to ESH ICU had a 20% relative decrease in mortality after the second publication of surviving sepsis guidelines. The original aim of the campaign was to reduce mortality from sepsis by 25% in 5 years [3] . This decrease was not due to a signifi cant diff erence between the sets of patients. The decreased time to admittance to ICU may be due to improved recognition of the need for ICU care. Overall the surviving sepsis campaign has had a signifi cantly benefi cial eff ect on mortality rates in patients with sepsis.
Introduction Survivors of sepsis report persistent problems that can last years after hospital discharge. The main aim of this study was to investigate long-term health-related quality of life in survivors of SIRS and sepsis compared with Welsh normative data, controlling for age, length of stay and pre-existing conditions. The second aim was to investigate any diff erences in long-term health-related quality of life specifi cally with the patients categorised into three groups: SIRS, uncomplicated sepsis, and severe sepsis/septic shock. Methods A prospective study design was used in order to investigate all sepsis patients either presenting to the emergency department or admitted to the ICU of a regional trauma centre. A total of 106 patients were recruited and all patients were considered eligible as per the SIRS and sepsis criteria [1] . The Sepsis-related Organ Failure Assessment score was determined over the fi rst 24 hours to assess organ function.
Patients were assigned to groups as follows: sterile SIRS; uncomplicated sepsis; severe sepsis or septic shock as per the criteria. Assignment into groups was blinded and performed by an intensive care specialist independent of the study. Baseline demographics, clinical characteristics and outcomes were collected and surviving patients were sent a SF-12v2 survey at between 6 months and 2 years post hospital discharge. Results A total of 106 patients were included in the study. A mortality rate of 34% was recorded, leading to a fi nal response rate of 72% by the end of the data collection period. Quality of life was signifi cantly reduced in all patients when compared with local normative data (all P <0.0001). Reductions in the physical components of health-related quality of life were more pronounced in severe sepsis/septic shock patients when compared with uncomplicated sepsis and SIRS patients. Conclusion This is the fi rst observational study to specifi cally focus on the diff erent groups of SIRS and sepsis patients to assess long-term quality of life. Local population norms were used for comparison, rather than wide geographical norms that fail to refl ect the intricacies of a country's population. Signifi cant reductions in quality of life were found in severe sepsis/septic shock patients compared with in uncomplicated sepsis and SIRS patients, when controlling for age, preexisting conditions, hospital and ICU length of stay. Reference Introduction Sepsis is a high-prevalence disease in ICUs, associated with high mortality and high costs, mainly in developing countries. The aim of this study is to demonstrate the ICU costs, in a private hospital, in patients admitted with severe sepsis and septic shock. Methods A retrospective, observational, single-center study of patients admitted from November 2013 to March 2014 with severe sepsis and septic shock. The records data were taken from the Software Epimed, MV System, and IBM SPSS Statistics 21. The classifi cation was based on the Surviving Sepsis Campaign 2012. We included all 50 beds of an adult ICU, clinical and surgical. All patients older than 18 years with severe sepsis and septic shock were included. We evaluated the costs of patients during their ICU stay, and its relation to clinical presentation (severe sepsis and septic shock), antibiotic start time, permanence of ICU stay, and mortality. Only the fi rst episode per patient was recorded.
Results From November 2013 to March 2014 were included 82 patients with criteria for severe sepsis and septic shock. The mean age of patients was 62.5 ± 21.8 years, divided equally between the genres. The overall mortality rate was 34.15%. The SAPS 3 was 56.43, with death probability set to Latin America 38.83%. Patients with severe sepsis had a mortality of 23.2% and those with septic shock had a mortality rate of 58%. The average total cost during ICU admission per patient was US$17,834 and the average daily cost was US$1,641. The daily cost in patients with severe sepsis and septic shock was US$1,263 and US$2,465 (P = 0.002), respectively, and in survivors and nonsurvivors was US$1,189 and US$2,512 (P = 0.001). The length of stay of patients in the ICU was 11.09 days, being 11.3 days in patients with severe sepsis and 10.7 days in patients with septic shock (P = 0.785). The beginning of the antibiotics in nonsurvivors was 73.7 minutes and in survivors was 64.7 minutes (P = 0.757), with the earliest onset in patients with septic shock than in patients with severe sepsis (38.5 vs. 81.5 minutes, P = 0.141).
Conclusion Severe sepsis and septic shock are conditions that consume large amounts of resources. Nonsurvivors had higher average spending than survivors. Patients admitted with septic shock had higher mortality than patients with severe sepsis with high mortality in relation to the prognostic indices adopted. The beginning of the antibiotics was longer in the nonsurvivors. We should adopt measures aimed at recognizing and earlier treatment of sepsis. If we improve our treatment, especially in septic shock, we will prevent deaths and decrement costs.
Introduction The objective of this study was to compare the number of rolling and adhered leukocytes in patients with severe sepsis/septic shock with non-infected controls. Microcirculatory fl ow alterations and endothelial cell dysfunction are elements of sepsis pathophysiology. Traditionally, microcirculatory emphasis has been on red blood cell vessel perfusion. However, assessment of interactions between white blood cells and endothelial cells may be another early diagnostic modality.
We included adult (age >18 years) ED patients presenting with severe sepsis/septic shock (sepsis with elevated lactate (>4 mmol/l)) or hypotension) from the prospective clinical ProCESS trial. We studied a subset of patients with microcirculatory videos obtained along with non-infected control patients. Using a sidestream dark-fi eld videomicroscope, we obtained image sequences from the sublingual mucosa and used video stabilization and frame averaging techniques to visualize slowly-moving leukocytes. We quantifi ed the number of rolling and adhered leukocytes present per 1 mm × 1 mm visual fi eld in a standardized 3-second clip. Furthermore, we extracted the total length of vessels candidate for counting of rolling/adhered leukocytes (vessels with an adequate focus). We report sample means with standard deviation and compare them with Student's t test.
Results We included a total of 64 patients with severe sepsis/septic shock and 32 non-infected controls. The mean number of adhered leukocytes per fi eld in the sepsis group was 2.1 (SD 2.3) compared with 0.4 (SD 0.8) in the non-infected group (P <0.001). This corresponded to a mean number of adhered leukocytes per unit vessel length of 0.16/mm (SD 0.22) and 0.03/mm (SD 0.06) for sepsis and non-infected groups, respectively (P <0.001). For the rolling leukocytes, we observed a mean number of 27.8 (SD 19.4) in the sepsis group and 12.0 (SD 8.7) in the non-infected group (P <0.001) per fi eld. This corresponded to a mean number of rolling leukocytes per unit vessel length of 2.00/mm (SD 1.67) and 0.75/mm (SD 0.55), respectively (P <0.001).
Conclusion Our results show a higher number of rolling and adhered leukocytes in patients with severe sepsis/septic shock when compared with non-infected controls. This also applies when taking the total vessel length in the fi eld of view into consideration. This may hold potential as a useful tool in sepsis assessment.
Introduction Platelets are now considered to be immune and infl ammatory agents as well as key cells in coagulation, and as such have been implicated in the pathophysiology of sepsis [1] . Thrombocytopenia is associated with sepsis severity and poor prognosis, and hyperactivated platelets probably contribute to microvascular thrombosis and organ failure. In the present study, we evaluated platelet activation markers as potential predictive markers of sepsis and of mortality among four commonly encountered populations of patients admitted to ICUs. Methods Ninety-nine non-infected ICU patients were prospectively screened at day 1 (T1) and day 3 (T2) of admission after elective cardiac surgery, trauma, acute neurologic dysfunction or prolonged ventilation (>48 hours). A third sample was drawn when infection was diagnosed (Tx). We evaluated platelet activation by measuring the expression of P-selectin (CD62P) and fi brinogen binding on the cell surface before and after stimulation with major platelet agonists (ADP, collagen, and TRAP) through fl ow cytometry. Clinical scores were obtained at admission. Results Patients who developed sepsis (n = 18) presented with signifi cantly higher platelet fi brinogen binding at T1 compared with patients who did not get infected (basal: P = 0.0014, upon stimulation: P <0.0035). At T1, ROC AUC for association of basal fi brinogen binding with the occurrence of sepsis was 0.79 (95% CI: 0.68 to 0.89). Elevated basal CD62P expression level was associated with increased 90-day mortality (P = 0.042, ROC AUC = 0.78 (0.64 to 0.88)). Kaplan-Meier survival curves illustrated that mortality was signifi cantly higher after stratifi cation based on T1 basal CD62P level (cutoff MFI >31.56, HR = 13.6, P = 8.23 × 10 -6 ). Multivariate logistic regression analysis using clinical scores (SOFA, APACHE II, SAPS II, SAPS III) indicated that addition of CD62P level or of bound fi brinogen level signifi cantly improved prediction of mortality (odds ratio 1.078, P = 0.003) and sepsis (odds ratio 1.033, P = 0.0012), respectively. Conclusion Predisposition to severe infection in selected critically ill medico-surgical adults can be identifi ed on day 1 of admission based on circulating basally activated platelets. Levels of activated platelets may add incremental prognostic information to clinical scoring. Reference
Introduction C1 inhibitor (C1INH), belonging to the superfamily of serine protease inhibitors, regulates not only complement system, but also the plasma kallikrein-kinin system, fi brinolytic system and coagulation system. The biologic activities of C1INH can be divided into the regulation of vascular permeability and anti-infl ammatory functions. In recent years, hereditary angioedema (HAE), caused by an inherited defi ciency of C1INH, has been focused. During HAE attacks, vascular permeability was markedly increased, which leads to angioedema. In sepsis, signifi cant endothelial hyperpermeability is similarly observed systemically, but the role of C1INH has not been clarifi ed in the pathogenesis. The serial change of C1INH in patients with sepsis is not clear. The objective of this study was to clarify the serial change in C1INH in patients with sepsis and evaluate the impact of C1INH on their clinical course. Methods We serially examined C1INH activity values (normal range 70 to 130%) and quantitative values (normal range 160 to 330 μg/ml) in patients with sepsis during the period between December 2012 and February 2013. We also analyzed their clinical course: prognosis, volume of infusion, body weight, urine volume, catecholamine administration, and steroid administration.
Results The serial change of C1INH was evaluated in fi ve patients with sepsis (three male and two female; four survivors and one nonsurvivor; mean age, 68 ± 11 years). In the nonsurvivor, C1INH activity on admission value was 97.2% (normal range), and quantitative value was 133.1 μg/ml (below normal). In the patient with severe sepsis requiring fl uid resuscitation, catecholamine and steroid administration to maintain hemodynamics, C1INH activity value on admission was 94.4% (normal range), and quantitative value was 126.7 μg/ml (below normal range). His general condition was improved on day 6, and C1INH activity value and quantitative value increased (139.9%; above normal range, 250.1 μg/ml; normal range). In the other three patients with sepsis not requiring steroid administration, C1INH activity value on admission was 130.6 ± 8.7% (above normal range), and quantitative value was 215 ± 26.5 μg/ml (normal range). Conclusion In the nonsurvivor or the severe patient with sepsis requiring steroid administration, the enhancement of C1INH activity was not observed, and the C1INH quantitative values were low. Further evaluation of the serial change of C1INH and the validity of C1INH replacement therapy in patients with septic shock may lead to a new strategy for management in sepsis.
The infl ammatory response of sepsis is developed in two phases, an infl ammatory phase (SIRS) and a phase more variable in frequency and intensity (CARS): this balance has an important eff ect on morbidity and mortality. Lymphopenia aff ects particularly T cells, and correlates inversely with outcome. The aim of the study was to identify phenotypic and functional early markers of T cells and NK cells related to prognosis in the septic patient population. Methods We collected peripheral blood mononuclear cells from 47 patients with severe sepsis or septic shock at ICU admission (T0) and from 50 healthy controls. On these subjects we evaluated frequency and absolute numbers of CD4 + and CD8 + T cells and of NK and B lymphocytes, the rates of regulatory CD4 + CD25 + Foxp3 + T cells (Tregs), the cytotoxic potential of CD4 + , CD8 + T cells and of NK cells by evaluation of perforin (PER) and granzyme (GRA) expression and production of eff ector cytokines (namely IL-2, IL-17, IL-4, TNFα, IFNγ) by CD4 + , CD8 + T cells and NK cells upon polyclonal stimulation. The markers were compared in patients with diff erent outcome. Results Septic patients, compared with healthy donors, were characterized by global lymphopenia; we found increased frequencies of CD4 + T cells producing IL-2 (P = 0.0000000003), increased percentage of CD8 T cells producing IFNγ (P = 0.03), and reduced proportion of CD4 + T cells (P = 0.00007) and NK cells (P = 0.002) producing IFNγ. We also noticed an increased frequency of CD8 + T cells expressing PER (P = 0.00000025) and GRA (P = 0.01); moreover, the proportion of NK cells expressing GRA was also signifi cantly increased (P = 0.000019).
To establish the prognostic value of these biological markers, we compared the cytokine expression by lymphocytes in septic patients that survived with those that died (D). We found that CD4 + and CD8 + TNFα-producing T cells were signifi cantly increased in D (P = 0.01 and P = 0.0001 respectively); similarly the percentage of CD8 + T cells producing IFNγ was more elevated in D (P = 0.006). The same was observed for IL-17 production by CD4 + T cells (P = 0.03) in D. On the contrary we observed a tendency to the reduction of circulating CD4 + CD25 + foxp3 (Tregs) in D (P = 0.08).
Conclusion Septic patients are characterized by a peculiar immunophenotype which includes global lymphopenia and a specifi c pattern of cytokines. Some of the evaluated markers seem to individuate those with worse outcome; in particular, this group showed an infl ammatory phenotype with a higher expression of IFNγ, TNFα, IL-17 and a tendency to a reduction of Tregs.
Introduction Critically ill patients show signs of immune suppression, which is considered to increase vulnerability to nosocomial infections. Whole blood stimulation is a frequently used functional test for immune suppression. We here aimed to assess the association between whole blood leukocyte responsiveness to lipopolysaccharide (LPS) and the subsequent occurrence of nosocomial infections in critically ill patients admitted to the ICU. Methods All consecutive critically ill patients admitted to the ICU between April 2012 and June 2013 with two or more systemic infl ammatory response syndrome criteria and an expected length of ICU stay of more than 24 hours were enrolled. Age-matched and gender-matched healthy individuals were included as controls. Blood was drawn the fi rst morning after ICU admission and stimulated ex vivo with 100 ng/ml ultrapure LPS for 3 hours. Tumor necrosis factor (TNF)α, interleukin (IL)-1β and IL-6 were measured in supernatants.
Results Seventy-three critically ill patients were included, 10 of whom developed an ICU-acquired infection. Compared with healthy subjects, whole blood leukocytes of patients were less responsive to ex vivo stimulation with LPS, as refl ected by strongly reduced TNFα, IL-1β and IL-6 levels in culture supernatants. However, results were not diff erent between patients who did and those who did not develop an ICUacquired infection (Figure 1 ). Conclusion The extent of reduced LPS responsiveness of blood leukocytes in critically ill patients on the fi rst day after ICU admission does not relate to the subsequent development of ICU-acquired infections.
Introduction CD14/HLADR is an index of immune suppression. Heat shock proteins (hsp) regulate cell response to oxidative stress. We evaluated the relationship of CD14/HLADR and hsp70/90 in patients with SIRS and severe sepsis versus healthy volunteers. Methods We evaluated 31 patients with SIRS or severe sepsis against a group of sex-matched healthy volunteers. Demographic data were obtained for all patients. APACHE score was calculated upon admission. Blood samples were collected upon diagnosis of SIRS or severe sepsis.
To evaluate the %HLA-DR expression on monocytes, the fresh whole blood was stained with anti-CD14-FITC, anti-HLA-DR-PE and CD45-PC5 while staining with anti-CD33-PE, anti-CD45-PC7, anti-hsp70-FITC and anti-hsp90-PE allowed evaluation of the MFI expression of hsps on CD33 + monocytes. Cells were then analyzed using fl ow cytometry. ANOVA with post hoc tests was used to compare CD14/HLADR cell counts and hsp70 and hsp90 levels among the three groups.
Results Nineteen controls, six SIRS patients and 25 severe sepsis patients were studied. The percent expression of HLADR on CD14 + monocytes was signifi cantly diff erent between the three groups showing progressive decrease from controls (mean 90.5 ± 3.8%) to SIRS (mean 61.2 ± 5.9%) to severe sepsis (mean 39.2 ± 5.5%) patients (controls vs. severe sepsis, P <0.001; controls vs. SIRS, P = 0.006; SIRS vs. severe sepsis, P = 0.03). hsp70 and hsp90 MFI were signifi cantly diff erent between controls (mean 49.5 ± 4.9 and 33.5 ± 3.4 respectively), SIRS (mean 69.9 ± 16.5 and 46.5 ± 5.7 respectively) and severe sepsis patients (mean 33.3 ± 4.5 and 21.7 ± 2.7 respectively) (P <0.05 for all comparisons). Notably, the hsp level rose from controls to SIRS and fell from SIRS to severe sepsis patients. APACHE score increased signifi cantly (P = 0.023) in septic patients compared with SIRS. Conclusion There were a signifi cant diff erence in CD14/HLADR, a marker of immune paralysis, between controls and patients with SIRS or severe sepsis. hsp70 and hsp90 showed an initial stimulation followed by exhaustion as sepsis progressed.
Introduction Decreased monocyte surface HLA-DR (mHLA-DR) measured by fl ow cytometry (FCM) is an independent marker of immunosuppression in sepsis. In a previous report we demonstrated that septic patients display a strong correlation between mHLA-DR and mRNA-levels of HLA-DRA in whole blood [1] . mRNA-based HLA-DR monitoring by PCR would improve the clinical usage and facilitate conduction of multicentre studies. The primary focus in this study was to evaluate the correlation between mHLA-DR and HLA-DRA at diff erent time points during sepsis. In addition, we assessed the dynamic expression of both mHLA-DR and HLA-DRA, in relation to sepsis severity. Methods Study patients (n = 54) were included at day 1 to 2 after hospital admission if blood cultures turned positive. Repeated sampling at days 1 to 2, 3, 7, 14 and 28 was performed. mHLA-DR was monitored by FCM and HLA-DRA by quantitative RT-PCR. Mixed models for longitudinal data were used after logarithmic transformation to calculate the interactional eff ects of time and severity on HLA-DR expression. Introduction The idea of using the eosinophil count (EC) as a diagnostic marker for clarifying the nature of systemic infl ammatory response syndrome (SIRS) belongs to K Abidi, who showed that EC could be used as a diagnostic criterion of sepsis. There are no published data to defi ne the role of dynamic control of EC in the process of intensive therapy as a prognostic marker and indicator of severity condition in critically ill patients. The aim was to determine the informative value of EC in the development of SIRS as a biomarker of sepsis and indicator of the severity condition and prognosis of outcome in the pathological process. Methods A total of 143 patients were enrolled in this study who were admitted to the ICU and had SIRS. All patients were divided into a septic group -patients with community-acquired pneumonia, complicated by sepsis -and two SIRS groups of noninfectious genesis -patients who had an acute cerebrovascular accident (CVA) and an acute myocardial infarction (AMI). The absolute EC was measured at admission and in the dynamics on days 3 to 5 of stay. Results The median EC was 75 cells/mm 3 in septic patients on admission, which was signifi cantly lower than in patients with CVA (120 cells/mm 3 ) and AMI (130 cells/mm 3 ). Comparison of EC in septic patients between survivors and those who died showed signifi cant diff erences (Table 1) . Receiver operating characteristic (ROC) analysis determined a value less than 80 cells/mm 3 as the optimal diagnostic cutoff value with a high level of confi dence in the comparison of septic and noninfectious groups. Area under the ROC curves was 0.94, sensitivity of 80.8%, specifi city of 95.6%, P <0.0001. There was a signifi cant increase of EC in survivors, while the EC did not change signifi cantly among those who died in the dynamics. ROC analysis determined the cutoff values of EC, which indicated a high risk of an adverse outcome in septic patients ( Table 2) . Introduction The purpose of this study was to evaluate the diagnostic and prognostic value of the endotoxin activity assay (EAA) of sepsis in patients with systemic infl ammatory response syndrome (SIRS) and organ failure in ICU setting. Methods In total, 76 patients with SIRS and organ failure or who were suspected of sepsis during critical care were included. According to the levels of EAA, all patients were classifi ed into three groups (group L, EAA <0.4; group M, EAA ≥0.4 or EAA <0. 6 ; group H, EAA ≥0.6).
In order to evaluate the severity of illness, the Acute Physiology and Chronic Health Evaluation II (APACHE II) score, Sequential Organ Failure Assessment (SOFA) score and catecholamine (CA) index were recorded. Blood samples were obtained to measure EAA levels, infl ammatory markers (procalcitonin, C-reactive protein and white blood cells), serum lactate level as an indicator of tissue hypoxia, and for blood culture. All patients were followed up for 6 months. APACHE II score, SOFA score, CA index, infl ammatory markers, serum lactate levels and blood culture results were examined for diagnosis of sepsis, severe sepsis, septic shock and for prognosis of 30-day mortality. Each value was also compared with EAA levels.
Results Patient age was 69 ± 9.9 years (male: n = 48, female: n = 25). The total number of samples was 106 (group L/group M/group H: 35/35/36). Twenty-seven specimens were obtained from nonseptic patients and 83 specimens were obtained from septic patients. APACHE II score was highly correlated with SOFA score (P <0.05). In group H, the APACHE II score was signifi cantly higher (22.2 ± 0.8) than that in group M (18.4 ± 0.87) (P = 0.01). The SOFA score in group H was signifi cantly higher (9.9 ± 0.5) than that in group M (7.5 ± 0.6) and group L (7.9 ± 0.6) (P = 0.006). EAA levels were signifi cantly increased in septic patients (septic patients: 0.56 ±0.03, nonseptic patients: 0.42 ±0.05) (P = 0.011) and in the positive blood culture group (positive group: 0.66 ± 0.05, negative group: 0.48 ± 0.03) (P = 0.006). There was no relationship between EAA levels and other infl ammation markers or 30-day mortality. Conclusion In patients with suspected sepsis and positive blood culture, EAA levels were signifi cantly increased and had strong correlation with severity of disease. This result suggests that EAA indicates the state of sepsis regardless of the possibility of infection in patients with SIRS with organ failure.
Introduction Sepsis is a common reason for admission to ICUs throughout the world. During the past two decades, the incidence of sepsis in the USA has tripled and is now the 10th leading cause of death. As sepsis continues to impact negatively on critically ill patients, it is clear that early diagnosis and eff ective management could improve patient morbidity and mortality. Numerous studies have attempted to examine biomarkers and their ability to diagnose and prognosticate septic patients. Despite multiple eff orts, currently there are no reliable markers that can eff ectively improve our clinical eff ectiveness in diagnosing and managing septic patients. The purpose of our systematic review was to evaluate the diagnostic and prognostic value of various biomarkers used in septic patients. Methods A systematic search of the literature was performed with MEDLINE, EMBASE, and the Cochrane Central Register of Controlled Trials databases using terminology selected for biomarkers (through to and including November 2013). All articles involving neonates and not in English were excluded. Inclusion was agreed on by two independent reviewers of abstracts or full text. Assessment was based on the biomarker's ability to diagnose septic patients and its ability to predict mortality. Results Of 5,257 articles identifi ed, all abstracts were screened, and 750 full-text articles were selected for review. These included primarily randomized controlled trials, cohort studies and postmortem studies. Of 49 biomarkers examined, 72% of the studies examined procalcitonin. Comparing the serum of septic patients with that of controls, most biomarkers were elevated in septic patients, even though only a few had high sensitivity (>85%) and high specifi city (>80%). It was often Introduction Early diagnosis of systemic infl ammation, a generalised response to noxious stimuli, is fundamentally important for eff ective and goal-directed therapy. Various infl ammation biomarkers have been used in clinical and experimental practice. However, a defi nitive diagnostic tool for an early detection of systemic infl ammation remains to be identifi ed. Acetylcholine (Ach) has been shown to play an important role in the infl ammatory response. Serum cholinesterase (butyrylcholinesterase (BChE)) is the major Ach hydrolyzing enzyme in plasma. The role of this enzyme during infl ammation has not yet been fully understood. Here, we describe a correlation between the BChE activity and the early systemic infl ammatory response upon traumatic injury. Methods We measured BChE activity in patients with traumatic injury admitted to the emergency room using a point-of-care-test (POCT) system. In addition, we measured levels of routine infl ammation biomarkers during the initial treatment period. We used the Injury Severity Score to assess the trauma severity. Data were statistically analyzed using the Friedman test. Correlation analysis was performed using Spearman's rank correlation test. P <0.05 was considered statistically signifi cant.
Results Reduced BChE activity correlated with trauma severity and the resulting systemic infl ammation. Compared with serum levels of routinely measured infl ammatory biomarkers, changes in the BChE activity were detected signifi cantly earlier, suggesting that the BChE activity might serve as an early indicator of systemic infl ammation. Conclusion Our results suggest that BChE activity, measured using a POCT system, might play an important role in the early diagnosis of trauma-induced systemic infl ammation.
Introduction The apoptotic process, in which cells are actively eliminated by a programmed pathway, is increased in sepsis. Extrinsic and intrinsic apoptotic death cell pathways activate caspase-3, which leads to cell apoptosis. Cytokeratin 18 (CK-18), a protein present in most epithelial and parenchymal cells, is cleaved by the action of caspases and released into the blood as caspase-cleaved CK (CCCK)-18 during apoptotic death. The novel objectives of this study were to determine whether there are associations between serum caspase-3 levels, serum CK-18 levels and mortality in septic patients. Methods A prospective, multicenter, observational study in six Spanish ICUs, including 216 patients with severe sepsis. We collected blood samples at the severe sepsis diagnosis moment to determine serum levels of caspase-3 (to assess the main executor of apoptosis) and CCCK-18 (to assess the apoptosis level). The endpoint was 30-day mortality.
Results We found that nonsurvivor (n = 76) in comparison with survivor (n = 140) septic patients showed higher serum levels of caspase-3 (0.41 ng/ml (0.14 to 0.52) vs. 0.11 ng/ml (0.10 to 0.25); P <0.001) and CCCK-18 (448 (310 to 723) vs. 319 (236 to 445); P <0.001). Multiple logistic regression showed that serum caspase-3 levels >0.25 ng/ml were associated with mortality at 30 days (odds ratio = 6.51; 95% confi dence interval = 3.32 to 12.77; P <0.001), controlling for SOFA score and age. Kaplan-Meier survival analysis showed a higher risk of death in septic patients with serum caspase-3 levels >0.25 ng/ml than in patients with lower levels (hazard ratio = 3.80; 95% CI = 2.35 to 6.15; P <0.001). We found a positive association between serum levels of caspase-3 and CCCK-18 (ρ = 0.32; P <0.001).
Conclusion The novel fi ndings of our study were that there is an association between serum caspase-3 levels, serum CK-18 levels and mortality in septic patients. There has been reported decreased apoptosis and increased survival in septic rats with the administration of caspase inhibitors; thus, it may be interesting to explore those agents in septic patients.
Introduction Low awareness, late recognition and delayed treatment of sepsis are still common. CD64 is a marker of the innate immune response upregulated in sepsis. The primary goal of this prospective, double-blind study was to compare the diagnostic accuracy of neutrophil CD64 and other cellular markers, along with C-reactive protein (CRP) and procalcitonin (PCT) levels, in early sepsis. Methods Adult ICU patients, between 2012 and 2014 were eligible. The eight-color fl ow cytometric biomarker panel included CD64, CD163, HLA DR, CD15 and others. Diagnostic test results were compared with infection as the reference standard and sepsis as the target condition, using receiver operating characteristic curve analyses. Multivariable logistic regression was used to assess the relationship of sets of markers with the probability of sepsis, adjusting for other patient characteristics. Results A total of 219 patients were enrolled, 120 with sepsis, 99 served as controls. APACHE IV (median 70 vs. 57), SOFA (8 vs. 7), ICU (2 vs. 1) and hospital length of stay (6 vs. 4) were higher in the sepsis group.
Mortality was not diff erent. After adjustment for APACHE IV, CRP and PCT, CD64 molecules/neutrophil measure remained a signifi cant predictor of sepsis (OR = 1.852 for one-unit increase on the log scale, 95% CI = 1.083 to 3.168, P = 0.02, AUC = 0.90). See Table 1 . Conclusion Neutrophil CD64 expression is an accurate predictor of early sepsis.
Introduction Procalcitonin (PCT) is increasingly the standard in the emergency department (ED) for the diagnostic and prognostic workup of patients with suspected infections. Recently, B·R·A·H·M·S PCT direct, a new high-sensitive point-of-care test, has been developed for fast PCT measurement on capillary or venous blood samples with a measuring range of 0.1 to 10.0 μg/l. Methods This is a prospective, comparative international study conducted in three European EDs. Consecutive patients with suspicion of bacterial infection were included. Duplicate determination of PCT was performed on two distinct B·R·A·H·M·S PCT direct test devices on capillary (fi ngertip) and venous whole blood (EDTA), and compared with the reference method (B·R·A·H·M·S PCT sensitive Kryptor or Elecsys B·R·A·H·M·S PCT, respectively). The diagnostic accuracy was evaluated by correlation and concordance analyses. Results A total of 303 patients were included over a 6-month period (60.4% male, median age 65.2 years). The correlation between capillary or venous whole blood and the reference method was excellent: r 2 = 0.96 and 0.97, sensitivity 88.1% and 93.0%, specifi city 96.5% and 96.8%, concordance 93% and 95% respectively at a 0.25 μg/l threshold. No signifi cant bias was observed (-0.04 and -0.02 for capillary and venous whole blood) although there were 6.8% and 5.1% outliers, respectively. B·R·A·H·M·S PCT direct had a shorter time to result as compared with the reference method (25 vs. 147 minutes, diff erence 122 minutes, 95% CI = 110 to 134 minutes, P <0.0001). Conclusion This study found a high diagnostic accuracy and a faster time to result of the PCT direct in the ED setting. The B·R·A·H·M·S PCT direct may allow a more widespread use of PCT tests in outpatient clinics and smaller institutions.
involved in the development of septic shock, and procalcitonin (PCT) serum concentration has been strongly associated with the severity of sepsis. The eff ect of elevated endotoxin activity (EA) on PCT serum level, organ function and mortality of patients with septic shock was evaluated on the day of admission to the ICU. Methods ICU patients with diagnosis of septic shock were consecutively added to the study group within the fi rst 24 hours. Serum PCT level and whole blood EA was immediately measured in all patients on admission (n = 157). Endotoxemia was defi ned as EA >0.4 EAU.
Results Endotoxemia was present in 61% of patients (group 1, n = 95, age 66 (57 to 75)), and in 39% of patients EA was low (group 2, n = 62, age 63 (55 to 76)). Median EA was 0.57 EAU (0.46 to 0.67) in group 1 and 0.27 EAU (0.17 to 0.36) in group 2 (P <0.001). The PCT level was six times higher in group 1 than in group 2 (19.6 ng/ml vs. 3.1 ng/ml, P <0.001) and was correlated with EA (P <0.001, R = 0.5). Median APACHE II score was 23 points (16 to 29) in group 1 and 19 (16 to 25) in group 2; but observed diff erence was not signifi cant. The severity of clinical status estimated by SOFA score was similar in both groups (10 (7 to 13) in group 1 and 11 (8 to 12) in group 2; NS). Forty-six percent of patients in group 1 and 27% in group 2 required renal replacement therapy (P = 0.01). ICU mortality of patients was 41%. The mortality rate was higher in group 1, compared with group 2, and Kaplan-Meier survival analysis of time to death showed statistical signifi cance between the two groups (P = 0.001, log-rank test). A Gram-negative pathogen as the primary source of infection was identifi ed in 64% of patients in group 1 and in 44% in group 2 (P = 0.004); bacteremia was detected in 26% of cases in group 1 and in 12% in group 2 (P = 0.02). Conclusion Septic shock with endotoxemia was associated with biochemical and clinical consequences including a higher PCT level, higher frequency of bacteremia, kidney failure, and death.
We evaluated the utility of procalcitonin (PCT) as a marker of outcome in severe sepsis of abdominal origin (SSAO). SSAO is one of the most prevalent pathologies in surgical ICUs (sICUs). Mortality from 19 to 70% has been reported. Biomarkers are basic tools for diagnosis, follow-up and outcome of sepsis. One of the most studied in last decades has been PCT. Its levels and kinetics could be useful to evaluate the outcome of septic patients. PCT on day 7 best identifi ed outcome (AUC ROC 0.768). PCT ≥3.5 ng/ml predicted mortality (sensitivity 55%, specifi city 73%). Conclusion PCT on day 7 and PCT kinetics can be useful to predict outcome in SSAO. PCT is higher in community-acquired sepsis, when surgical cultures are positive, in Gram-negative isolations and in males.
Introduction Blood lactate level is a routinely used biomarker for management of patients in septic conditions in the ICU. There is a lack of clinical research data comparing lactate with novel sepsis biomarkers, such as procalcitonin, in regard to the diagnostic and prognostic potential. Herein, we investigated the diagnostic and prognostic value of initial lactate and procalcitonin levels and their kinetics within the ICU stay for prediction of positive blood cultures and fatal outcome in a well characterized cohort of sepsis patients in a US critical care setting. Methods This is a retrospective, observational cohort study of adult patients with confi rmed severe sepsis or septic shock and with at least one procalcitonin and lactate measurement on admission to the ICU of Morton Plant Hospital (Clearwater, FL, USA). Logistic regression models were calculated to assess the association of biomarkers with blood culture positivity and fatal ICU outcome with area under the curve (AUC) as a measure of discrimination.
Results The in-hospital mortality rate of the 1,075 included patients (age 68 years) was 23.8% (95% CI = 21.2 to 26.3%) and 18.4% of patients had positive cultures. In regard to the diagnostic value for bacteremic disease, initial procalcitonin had a higher discriminatory value (AUC 0.71) compared with initial lactate levels (AUC 0.52). In regard to prognosis, initial lactate level was the better mortality predictor (AUC 0.69) compared with procalcitonin (AUC 0.55), although both initial levels were signifi cantly lower compared with APACHE III (P >0.05 for both comparisons). When looking at biomarker kinetics, procalcitonin decrease was more strongly associated with fatal outcome compared with initial levels alone (AUC 0.66), but still lower compared with lactate kinetics (AUC 0.73). Conclusion Both biomarkers, procalcitonin and lactate provide diagnostic and prognostic information in ICU patients with sepsis, particularly when looking at biomarker kinetics. An evidencebased protocol incorporating both markers may further improve management of septic patients. Conclusion PCT levels in patients undergoing LVAD implantation rise signifi cantly in the fi rst 2 days after surgery. Interestingly, this elevation is much higher than after routine cardiac surgery with CPB. Recent works suggest that PCT concentrations are aff ected by SIRS caused by contact with a nonphysiological surface. In the case of LVAD this immunological stimulation is long lasting and even more potent with additional RVAD or ARF treated with renal replacement therapy.
In accordance with this hypothesis, our data show that the ability of PCT to detect infectious complication in LVAD patients is limited and its concentrations more probably correlate with postoperative complications in general.
The potentially envisaged actions of intravenous immunoglobulin (IVIg) on severe infectious disease include: virus or toxin neutralizing action; opsonic eff ect; complement bacteriolytic activity; and enhancement of sensitivity to antibiotics. In the case of severe infectious disease, antibiotics are often supplemented with administration of IVIg. Methods The changes in sepsis markers (procalcitonin, presepsin, interleukin-6, C-reactive protein) followed by IVIg administration were investigated in severe sepsis or septic shock patients. The subjects were 410 patients admitted to an ICU with a diagnosis of severe sepsis or septic shock and from whom informed consent had been obtained for the present study. IVIg was administered intravenously for 3 days (5.0 g/ day) and measurements were undertaken before administration (day 1), on the day after completion of administration (day 4), and on day 7. The items measured were procalcitonin, presepsin, IL-6, and CRP. The eff ect of IVIg administration on these markers was then studied. The IVIg studied was polyethylene glycol-treated human immunoglobulin injection fl uid (2.5 g, 50 ml, one vial).
The patient APACHE II score were 24.9 ± 8.2, the SOFA score 9.1 ± 3.7, and the survival rate after 28 days 83.4%. The values before IVIg administration were: procalcitonin 36.0 ± 463.3 (median 110) ng/ml, presepsin 4,548 ± 4,250 (median 3,337) pg/ml, CRP 15.6 ± 9.6 (median 14.7) mg/dl, and IL-6 13,860 ± 47,299 (median 630) pg/ml. All values were thus elevated. On the days after the completion of IVIg administration and on day 7, the level of almost all mediators (procalcitonin, presepsin, CRP, IL-6) decreased signifi cantly. In patients with suspected severe sepsis and septic shock, presepsin reveals valuable diagnostic capacity to diff erentiate sepsis severity compared with procalcitonin, IL-6, CRP, and WBC. Additionally, presepsin and IL-6 reveal prognostic value with respect to 30 days and 6 months all-cause mortality throughout the fi rst week of ICU treatment [1] .
The results of the present study found signifi cant decreases of procalcitonin, presepsin and IL-6 resulting from 3 days of immunoglobulin administration, but evidence is still limited and this needs to be confi rmed in larger studies. Introduction Presepsin (sCD14-ST) serves as a mediator of the response to infectious agents. First evidence suggested that presepsin may be utilized as a sepsis marker. Methods Presepsin was determined at presentation (T0), after 8, 24 and 72 hours in 123 individuals admitted with signs of SIRS and/ or infection. Primary endpoint was death within 30 days. Presepsin was determined using the POC assay PATHFAST Presepsin (Mitsubishi Chemical, Japan). Results Mean presepsin concentrations of the patient group at presentation and of the control group were 1,945 and 130 pg/ml, respectively (P <0.0001). Baseline presepsin diff ered highly signifi cant between patients with SIRS, sepsis, severe sepsis and septic shock. Twenty-four patients died during 30 days. The 30-day mortality was 19.5% in total, ranging from 10 to 32% between the fi rst and the fourth quartile of presepsin concentration. Nonsurvivors showed high presepsin values with increasing tendency during the course of the disease while in surviving patients this tendency was decreasing. See Table 1 .
Conclusion Presepsin demonstrated a strong relationship with disease severity and outcome. Presepsin provided reliable discrimination between SIRS and sepsis as well as prognosis and early prediction of 30day mortality already at admission. Presepsin showed close association with the course of the disease.
Introduction Whether the infl ammatory biomarker procalcitonin (PCT) provides prognostic information across clinical settings and diff erent acute respiratory tract infections (ARI) is poorly understood. Herein, we investigated the prognostic value of admission PCT levels to predict adverse clinical outcome in a large ARI population. Methods We analyzed data from 14 trials and 4,211 ARI patients to study associations of admission PCT levels and setting specifi c treatment failure and mortality alone at 30 days. We used multivariable hierarchical logistic regression and conducted sensitivity analyses stratifi ed by clinical settings and ARI diagnoses to assess the results' consistency. Introduction The purpose of the study was to assess the prognosis value of pro-adrenomedullin (pADM), C-reactive protein (CRP) and procalcitonin (PCT), lactate (LT), albumin (ALB), cholesterol (CHOL), white blood cell (WBC) and severity score in patients with severe sepsis or septic shock. Methods A prospective, observational study in adult patients with severe sepsis or septic shock in a polyvalent ICU. Demographics, severity scores (APACHE II and SOFA) and all of the biomarkers were studied within 24 hours from septic shock onset. Descriptive and comparative statistical analysis was performed using the statistical software packages SPSS v. 15 Introduction Early diff erentiation of bacterial from candidal bloodstream infections (BSIs) in the presence of sepsis or septic shock is crucial because of the need for appropriate treatment initiation. Clinical data, although limited, suggest a role for procalcitonin (PCT) [1] [2] [3] . The aim of this study was to investigate a possible association between the etiology of BSIs and the serum PCT levels.
Methods ICU patients with clinical and laboratory signs of sepsis or septic shock, with documented BSIs and with both serum PCT and CRP measurements on the day of the positive blood sample (±1 day), were included. Illness severity was assessed by SOFA score on both admission and BSI day. Demographic, clinical, and laboratory data including PCT and CRP levels, as well as the white blood cell (WBC) count on the day of the BSI were recorded. PCT was measured by an electrochemiluminescence analyzer and CRP by the tholosimetric method (Roche, Switzerland). Results A total of 64 ICU patients (mean age 58 ± 18 years, 39 males) with BSIs were included. SOFA sore was 9 ± 4 on ICU admission and 8 ± 4 on the day of BSI. In 30 of these patients Candida spp. were isolated in blood culture (candidemia group) whereas the remaining 34 had a bacterial etiology of BSI (bacteremia group). Serum PCT concentrations remained within normal ranges in most patients with candidemia whereas a wide range was observed in patients with bacteremia. Mean values of PCT and CRP levels were higher in the bacterial than in the candidemia BSI group: 18.5 ± 33.2 versus 0.73 ± 1.40 ng/ml, P <0.001 and 17.7 ± 10.3 versus 8.9 ± 8.0 mg/dl, P = 0.001, respectively. There was a signifi cant diff erence in WBC count between the two groups: 19,460 ± 10.174 versus 11,000 ± 5,440, P <0.001 for the bacteremia and candidemia BSI group, respectively. A ROC curve analysis of the predictive ability of PCT showed an AUC of 0.79 (P <0.001). When a cutoff point of 0.40 ng/ml was selected using Youden's J statistic, a low value of PCT had in our sample a negative predictive value of 0.76 and a likelihood ratio (negative) of 0.30. Conclusion A low serum PCT value could be considered as a diagnostic marker in distinguishing between BSIs of candidal or bacterial origin in ICU patients with varying severity of sepsis.
following: dialysis, surgery, pancreatitis, and receipt of corticosteroids, other immunosuppressive agents or parenteral nutrition. Diff erent from the original description of the score which considered only the fi rst 7 days of ICU stay, we selected patients who fulfi lled these criteria at any time during the ICU stay. Once a patient fulfi lled these criteria, AFT (anidulafungin 200 mg followed by 100 mg daily) was initiated provided that the patients also presented with any of the following: fever, hypothermia, hypotension, leukocytosis, acidosis or elevated C-reactive protein. Blood cultures (days 1 to 2) and baseline serum BDG (days 1 to 3) were performed. Patients with candidemia were treated for ≥14 days, those without candidemia but ≥1 positive BDG (≥80 pg/ml) received AFT for ≥10 days, and patients with negative blood cultures and negative BDG discontinued anidulafungin. Results A total of 2,148 patients were screened, and 85 (4%) fulfi lled entry criteria. The incidence of candidemia in these 85 patients was 8.2%, compared with 0.5% in the remaining 2,063 patients (relative risk 16.9%, 95% confi dence interval (CI) = 6.63 to 43.55). Baseline BDG was positive in 74 patients (87%), with a median number of positive tests of 3 (range 1 to 3) and a median value of 523 pg/ml (range 83 to 6,860). All seven patients with candidemia had positive baseline BDG (median value 523 pg/ml, range 203 to 3,660). The best cutoff of baseline BDG for the diagnosis of candidemia was 522 pg/ml (area under the ROC curve 0.883, 95% CI = 0.769 to 0.997), with sensitivity and specifi city of 86% and 88%, respectively. The cutoff value of 80 pg/ml had sensitivity and specifi city of 73% and 27%, respectively. Conclusion This dynamic prediction rule was able to diff erentiate a group of ICU patients at high risk to develop candidemia, with a relative risk of 16.9. BDG is frequently positive in ICU patients. A cutoff value of 522 pg/ml was able to discriminate between candidemic and noncandidemic patients. A revision of the cutoff value for BDG in the ICU is needed.
Introduction Bloodstream infections in the ICU are a major trigger of morbidity and mortality. Several risk factors for bacteremia have been previously identifi ed, such as presence of a central venous catheter or invasive ventilation [1, 2] . Iron is a key element for bacteria growth, and its metabolism is extensively altered by infl ammation. We aim to determine whether iron defi ciency is a risk or protective factor for bacteremia in the ICU. Methods We performed a retrospective analysis of patients included in the MIMIC-II database, an ICU database that collected data from patients admitted to the medical, surgical, coronary and cardiac surgery ICU of Boston's Beth Israel Deaconess Medical Center during a period of 7 years. We performed logistic regression models to assess the association between iron and bloodstream infection. Results We included 3,980 patients, 2,988 with low serum iron (<60 ng/ ml) and 992 with normal/high serum iron (≥60 ng/ml). During their fi rst stay in the ICU, 351 (8.82%) patients developed bloodstream infections. Low serum iron was associated with increased odds of bloodstream infection (OR: 1.37; 95% CI: 1.04 to 1.80). After adjusting for age, gender, Simplifi ed Acute Physiology Score, presence of central venous catheter, ICU type, transfusions performed before iron measured, neoplastic disease, diabetes mellitus, hepatic disease, congestive heart failure and ferritin levels, low levels of iron were still associated with an increased odds of bacteremia (OR: 1.41; 95% CI: 1.03 to 1.9). In contrast, low serum iron was associated with a decreased risk of death in the hospital (adj OR: 0.73, CI: 0.57 to 0.95). Conclusion Low serum iron increases the risk of bloodstream infection in the ICU, and should be considered as a risk factor to stratify patients' risk of bacteremia during ICU stay.
Introduction The aim of this study was to investigate whether clinicians can estimate, at the time of insertion, the length of time a central venous catheter (CVC) will remain in place, and to identify clinical variables which may predict CVC duration. CVC-related bloodstream infection is a known complication among critically ill patients. As infection rates may increase with duration of catheterization, more expensive antimicrobial-coated catheters may be used in patients with anticipated long duration of CVC use.
Methods We conducted a single-center, prospective study from January 2012 to November 2012. Clinicians prospectively estimated the anticipated duration of CVC at the time of line placement in an electronic procedure note. We collected demographics, past medical history, type of ICU, vital signs, laboratory values, SOFA score, mechanical ventilation and use of vasopressors at the time of placement. Continuous variables were compared with the Wilcoxon rank-sum test and categorical variables with the Fisher's exact test. Pearson's correlation coeffi cient was used to assess the correlation between estimated CVC time and actual time. Duration of CVC use was dichotomized into long (≥7 days) or short (<7 days), based on previous literature, and sensitivity and specifi city for predicting long duration was calculated. We performed a logistic regression analysis to identify variables associated with long CVC duration and calculated the area under the ROC curve (AUC). Results We enrolled 150 patients; median age was 65 (IQR: 52 to 74), 63 (42%) were female and mortality was 22%. Median time from CVC placement to removal was 5 (IQR: 3 to 8) days. The correlation between estimated CVC time and actual time was low (r = 0.36, P <0.001). Fortyeight (32%) patients had a long CVC duration. Clinician estimate had 46% sensitivity and 76% specifi city for predicting long duration of CVC. Of 30 variables tested, only temperature at the time of insertion was signifi cantly associated with long duration (OR: 1.30, 95% CI: 1.04 to 1.63, P = 0.02). The AUC for this model was 0.59 (95% CI: 0.49 to 0.69). Conclusion Our results suggest a low correlation between clinician prediction at time of insertion and actual duration of CVC. We did not fi nd any good predictors of long duration of CVC. Given our relatively low sample size, we may have been underpowered. It may not be feasible to identify patients at the time of insertion who may benefi t from antimicrobial-coated catheters.
Methods After ethics committee approval, patients admitted to the ICU, older than 18 years, who were thought to have a central venous catheter (CVC) for more than 48 hours, and whose fi rst catheter was inserted in the ICU were included in the study. Staff were educated before the study and periodically during the study. Catheter care and insertion were applied according to the guidelines. The study was planned as three sequences. In the fi rst group, catheter care was made with a sterile gauze pad. In the second and third groups, catheter care was made with chlorhexidine gluconate impregnated dressing. Also in the third group, a silver-coated needleless connector was inserted into the tip of venous catheters. Results Totally 105 patients were included in the study and every group included 35 patients. There was no diff erence between groups when evaluating reasons for catheter insertion. There was no statistically signifi cant diff erence according to emergent or elective catheterization, trying times, or catheter insertion side (P >0.05). CRBSI was determined in two patients in group 1, in one patient in group 2, and in no patient in group 3. In group 1 it was observed on the 4th and 11th days. In group 2 it was observed on the 18th day after catheterization. Before the study, a statistically signifi cant decrease was determined in CRBSI ratios before and after education (16.4/1,000, 12.9/1,000 catheter-days (P <0.001)). According to Group 1 a statistically meaningful decrease was assigned in CRBSI ratios in Groups 2 and 3 (4.84/1,000, 2.22/1,000, 0/1,000 catheter-days) (P <0.001, P <0.001, P <0.001).
Conclusion Continued education is important in preventing CRBSIs. Maximum precautions must be taken. Usage of antiseptic solutions with clorhexidine and chlorhexidine gluconate impregnated dressing decreased insertion side infections and usage of silver-coated needleless connectors reduced microorganism entry through the catheter lumen and provided a severe decrease in infection ratio.
Introduction The current CDC guideline for the prevention of intravascular catheter-related infections recommends skin preparation with a greater than 0.5% chlorhexidine with alcohol solution before central venous catheter (CVC) or peripheral arterial catheter (AC) placement. However, few studies investigated the superiority of 1% alcoholic chlorhexidine gluconate (1% CHG) over either 0.5% alcoholic chlorhexidine gluconate (0.5% CHG) or 10% aqueous povidone iodine (10% PVI) for the prevention of catheter colonization. The aim of this study is to compare the eff ectiveness of three skin antiseptic solutions for the prevention of intravascular catheter colonization. Methods This multicenter prospective randomized controlled trial was conducted in 15 Japanese ICUs from December 2012 to March 2014. Patients over 18 years of age undergoing CVC and AC placement in the ICU are randomized to have one of three skin antiseptic preparations before catheter insertion. After removal of the catheter, the distal tip is cultured using semiquantitative or quantitative techniques. The incidence of catheter colonization and catheter-related bloodstream infection (CRBSI) is compared between the three groups. Results A total of 997 catheters were placed, including 339 catheters using 1% CHG, 329 using 0.5% CHG, and 329 using 10% PVI. The median duration of catheter indwelling in the entire population was 3.7 days with an interquartile range of 2.0 to 6.7 days, with no signifi cant diff erence between the groups (P = 0.36). Thirteen catheters (5.1%) in the 10% PVI group were positive for catheter-tip colonization, whereas six catheters (2.2%) in the 1% CHG group and fi ve catheters (1.9%) in the 0.5% CHG group were positive (P = 0.07). The probability of catheter colonization was signifi cantly higher in the 10% PVI group than each CHG groups (P = 0.028, log-rank test). The incidence of catheter colonization and CRBSI is compared between the three groups. Conclusion In this randomized controlled trial comparing the eff ectiveness of three cutaneous antiseptic solutions for the prevention of catheter colonization, either 0.5% or 1.0% CHG was superior to 10% PVI. The main pathogens of CLA-BSI were CN staphylococci (22%), Staphylococcus aureus (21%), and Enterobacteriaceae (29%). In this study, the density of CLA-BSI was about 50% lower (2.75 (2.0 to 6.06)) than in our previous study and in the INICC's report (2014), but higher than in the CDC's NHSN (2012) report. Conclusion The implementation of the infection control program and preventive interventions for patients with central venous catheters improved the safety and quality of healthcare in the ICU by reducing CLA-BSI rate. Reference not easy to avoid. It is important to know what happens to neutropenic patients admitted to the ICU because of sepsis from any source, in whom catheter infection cannot be excluded. Methods A retrospective, cohort, descriptive study was carried out between January 2013 and November 2014. Epidemiology data were collected from all neutropenic patients admitted to the ICU who came from hemato-oncology services and had an implanted central venous catheter. Microbiology results and data related to the catheter removal were described. Results A total of 15 patients were included, mean age was 53 years old and 66% were male. The implanted catheter was removed in 80% of patients. Platelet transfusion was needed in 100% of patients before catheter removal and no complication was observed during catheter removal or in the insertion of a new one. In 53% of patients, catheter infection was confi rmed a posteriori. Conclusion Removal of an implanted central venous catheter from neutropenic patients admitted to the ICU due to sepsis from any source can be benefi cial for this kind of patient as it was found that in more than 50% of patients catheter infection was confi rmed a posteriori.
Introduction Femoral, jugular or subclavian central venous catheterization (CVC) is routinely performed during the care of the critically ill. These invasive procedures contribute to additional morbidity, mortality, and costs derived from the interactions between traumatic, infectious and other complications. The aim of this study is to determine whether the subclavian, jugular or femoral central venous access (CVA) routes have an eff ect on the incidence of CLABSI in critically ill patients and to compare between these routes regarding major complications and ICU mortality. Methods A retrospective observational study in a medical and surgical ICU in a tertiary care hospital on adult patients admitted from January 2010 to December 2013. The study enrolled 845 patients divided into 283 internal jugular CVC (IJC), 270 subclavian CVC (SCC) and 287 femoral CVC (FC) in which the catheters were inserted in the ICU by experienced physicians with at least 50 previously successful trials of central line insertion, using CVC bundle checklist. ICU length of stay, incidence of complications, APACHE II score adjusted severity and mortality were calculated for each group. Results Patient and catheter characteristics including the duration of catheterization were similar in all groups. The rate of CLABSI in the IJC, SCC and FC groups was 5.8 versus 7.2 versus 3.45 per 1,000 catheterdays respectively with P = 0.35. ICU mortality was 134 (47%) cases of the IJC group, 108 (39%) cases of the SCC group and 113 (39%) cases of the FC group. There was no signifi cant diff erence between the three groups of CVC in the incidence of CLABSI rate in the critically ill patients, and a slight increase in ICU mortality in the IJC group compared with the other two groups. Pneumothorax occurred in six (2.2%) cases of SCC and 11 (3.8%) cases of IJC with no signifi cant diff erence between the two groups as the P value was 0.3. Conclusion Site of insertion of CVC does not appear to aff ect the rate of CLABSI among critically ill patients. Pneumothorax was recorded in SCC and IJC groups with no statistical preference to either group.
Introduction Neutrophils work as the frontline of defense against infections and neutrophil extracellular traps (NETs) are one of the immune systems to suppress dissemination of infection by the netted chromatin decorated with antibacterial molecules. It was reported that NETs play an important role in various kinds of infections such as pneumonia. However, there is no report on NETs in patients of soft tissue infections. In this study, we evaluated NET production in pus and clarifi ed the role of NETs as a host defense in patients of soft tissue infections.
Methods This study was conducted in the ICU of the Trauma and Acute Critical Care Center at Osaka University Hospital. We collected pus from the patients of soft tissue infections at the time when drainage or debridement was performed and when clinical improvement was observed. The smears of pus specimens were examined by immunohistochemistry to visualize the major NET components: DNA, neutrophil elastase, and histone H1. Concurrently, the patients' clinical data and laboratory data of blood were recorded to analyze the relationship with NET production. Results A total of fi ve patients were included in this study and drainage of abscess or debridement of infection site was performed in all of the cases. Four patients of them were diagnosed as necrotizing soft tissue infections by Clostridium spp. (n = 1) and Bacteroides spp. (n = 3) and the other was diagnosed as cervical abscess by Streptococcus spp. In all cases, no NETs but neutrophils were identifi ed in the fi rst pus: however, NETs appeared in the later smears as the patients' condition was getting better.
Conclusion These results suggested that NETs also worked as an immune system against soft tissue infections. Drainage or debridement of infection focus might promote NET production.
Introduction In our study, we aimed to compare the application of benzalkonium chloride (BC) -a nanotechnology-based product -for 24-hour periods and didecyl dimethyl ammonium chloride (DDAC) for 12-hour periods regarding effi ciency in application of surface antiseptics in the ICU. Methods Two diff erent areas with eight beds at both sides of a common corridor in the ICU were named as areas A and B. BC was applied in area A with 24-hour periods and DDCA was applied in area B with 12hour periods for surface cleaning. Samples were taken from a total of 20 diff erent surfaces including nurse-station desks, phones, keyboards, beds, bedside monitors and ventilators by the same infection control nurse every 24 hours from area A and every 12 hours from area B for 7 days. Swab samples were cultured on 5% sheep bloody agar and McConkey agar in the laboratory. Then the cultured mediums were incubated at 35 to 37°C in an aerobic environment for 18 to 24 hours.
Statistical Software (UT, USA) programs were used for the statistical analysis. Results There were no statistical diff erences between two groups (Table 1) . Introduction Urinary catheter insertion is a common procedure in ICUs and can be an important cause of infection in the hospital environment [1, 2] . We aimed to analyze the eff ect of chlorhexidine on long-term urinary catheter insertion and urinary tract infection (UTI) during a 5-year period in patients admitted to a coronary ICU. Methods Analysis of patients admitted to a coronary ICU of a mediumsized hospital in Brazil from January 2010 to May 2014. The institutional protocol of periprocedural antisepsis was changed from iodine-based antiseptic to chlorhexidine in 2012. The UTI diagnosis was based on urine culture (>10 5 colony-forming units per ml of urine) associated with at least one clinical/laboratory abnormality (fever >38°C, urination urgency, increased urinary frequency, dysuria, or suprapubic or lumbar pain). The UTI rate represents the urinary tract infections associated with long-term urinary catheter (patient with UTI associated with long-term urinary catheter divided by patients with long-term urinary catheter × 1,000).
The urinary tract infection rates were 4.8 (year 2010: patients·day -1 (n: 2,511), long-term urinary catheter·day -1 (n: 1,455), device usage rate (958%)), 4.4 (year 2011: patients·day -1 (n: 2,529), longterm urinary catheter·day -1 (n: 1,140), device usage rate (45%)), 0.0 (year 2012: patients·day -1 (n: 2,660), long-term urinary catheter·day -1 (n: 783), device usage rate (29%)), 0.0 (year 2013: patients·day -1 (n: 2,573), longterm urinary catheter·day -1 (n: 960), device usage rate (37%)), and 0.0 (year 2014: patients·day -1 (n: 1,070), long-term urinary catheter·day -1 (n: 444), device usage rate (42%)).
Conclusion The use of chlorhexidine in the periprocedural antisepsis of urinary catheterization contributed to the decrease of urinary tract infections associated with long-term urinary catheter in patients admitted to the coronary ICU. References Introduction Whole-body skin decolonization with chlorhexidine in critically ill patients reduces multidrug-resistant bacterial colonization, and catheter-related bloodstream infection (BSI). We performed a meta-analysis of randomized controlled trials to determine whether daily bathing with chlorhexidine decreased hospital-acquired BSIs in critically ill patients. Methods We searched the MEDLINE, EMBASE, and Cochrane Central Register of Controlled Trials databases to identify randomized controlled trials that compared daily bathing with chlorhexidine and a control (daily bathing with soap and water or nonantimicrobial washcloths, or implementation of MRSA screening and isolation) in critically ill patients. The primary outcome was hospital-acquired BSIs. Secondary outcomes were adverse eff ects of chlorhexidine and the incidence of identifi ed pathogens. Results This meta-analysis included four studies. The overall incidence of hospital-acquired BSIs was signifi cantly lower in the chlorhexidine group compared with the control 0.80 (95% CI, 0.71 to 0.90; P <0.001; I 2 = 29.4%). Gram-positive (RR = 0.59, 95% CI, 0.44 to 0.79, P = 0.000; I 2 = 46.0%) and MRSA-induced (pooled RR = 0.64; 95% CI, 0.47 to 0.88, P = 0.006; I 2 = 0.0%) bacteremias were signifi cantly less common in the chlorhexidine group. Chlorhexidine did not aff ect Gram-negative bacteremia or fungemia. The overall incidence of adverse events, such as skin rashes, was similar in both groups. Introduction Good hand hygiene (HH) is critical to infection control in the ICU. Electronic HH surveillance systems are purported to improve HH practices. Such a system was recently trialed in our ICU. The system is based on radiofrequency transponders in three locations: bracelets worn by ICU personnel; on all HH product dispensers; and above each patient's bed. By correlating input from these three sources the system detects whether HH was performed before and after each patient contact. In the event that HH is not performed, the bracelet alerts the user (by vibration) in real time. This study represents a clinical validation of the system. Methods ICU staff (nurses and physicians) were followed by a trained observer over 60-minute periods. Each movement and contact during the period was documented. HH opportunities were determined according to WHO criteria and actual HH performance recorded. Observer and electronic data were compared for number of opportunities, HH performance and compliance. A satisfaction questionnaire was distributed to all users. Paired Student's t test was used for comparison of the observer and electronic data. Results Observations were made over 56 time periods that included 836 HH opportunities and 485 occasions when HH was performed. The observer recorded 10.9 ± 7.6 HH opportunities/hour compared with 6.8 ± 6.9 for the electronic system (P <0.001). HH performance occurred on 8.7 ± 3.9 occasions/hour versus 6.0 ± 3.1 occasions/hour as recorded by the electronic system (P <0.001). Overall HH compliance was 62.5 ± 17.7% versus 57.5 ± 21.0% respectively (P = 0.523). On comparison of specifi c observation periods, there was poor correlation between compliance as recorded by the observer and electronic system (r = 0.03, P = 0.915). Satisfaction questionnaires were completed by 41 personnel. Satisfaction with the system was low or very low for 21/41 (61%). System inaccuracy (either bracelet alerts without cause, or lack of bracelet alerts when HH was required) was the most common reason for dissatisfaction (31/41, 76%), followed by physical discomfort from the bracelet (18/41, 44%). Conclusion The electronic HH system consistently underestimated both HH opportunities and HH performance. The main reason for dissatisfaction with the system was inaccuracy of bracelet alerts. These data suggest that for an electronic system to be accepted by ICU staff , it has to be highly accurate and comfortable for the user.
the use of closed system transfer devices (CSTDs). To evaluate the microbial tightness of CSTDs we developed two methods which simulate the bioburden in ambient air of operating rooms and ICUs. Methods The methods simulate airborne and touch contamination. We tested the microbial tightness of the integrated Safefl ow® valve of a Mini-Spike® which is used for drug admixture. The airborne contamination was done in an exposure chamber in which a nebulizer distributed defi ned B. subtilis spore aerosols [1] . A Mini-Spike® was inserted into a vial of 0.9% sodium chloride solution (NaCl). A nebulizer with a suspension of 4.8 × 10 5 CFU spores of B. subtilis per ml was used to generate an aerosol for 1 minute. The volume of B. subtilis suspension nebulized per minute was 0.278 ml. This corresponds to 1.34 × 10 3 aerosolized spores in the exposure chamber, which has a volume of 0.24 m 3 (5.6 × 10 3 CFU per m 3 air). The used concentration was 100 times higher than the microbial burden found in hospitals [2] . After nebulization the valve was disinfected and NaCl was withdrawn into a syringe at certain time intervals. The NaCl was incubated on tryptic soy agar at 37°C for 48 hours. Results were documented as CFU. For touch contamination, a Mini-Spike® was attached to a vial of NaCl. The valve of the Mini-Spike® was contaminated with 10 5 CFU Staphylococcus aureus. The subsequent procedure was done as described above.
Results Out of nine tested valves, none showed transmission of B. subtilis spores after airborne contamination. Three out of nine tested valves were contaminated with S. aureus after touch contamination. Conclusion Our study shows that both methods are suitable for evaluating the microbial tightness of CSTDs.
Introduction We conducted a survey to assess clinicians' knowledge of personal protective equipment (PPE) requirements for infectious diseases and biochemical warfare agents. A safe level of PPE is essential when treating patients with highly infectious diseases or those contaminated with hazardous substances. The recent Ebola virus disease (EVD) outbreak in West Africa has highlighted that, although uncommon, contagious diseases with high mortality rates can be a threat to healthcare systems at local, national, and international levels [1] . Chemical, biological, radiological or nuclear (CBRN) contamination presents similar risks. Methods A validated, hand-delivered, multiple-choice questionnaire [2] was used to assess intensive care, emergency medicine, and anesthetics specialist registrars' knowledge of respiratory and skin protection needed during a resuscitation scenario with advanced life support. Participants selected the PPE required for the biological hazards: EVD, severe acute respiratory syndrome (SARS), inhalational anthrax, plague and smallpox; and the biochemical hazards: sarin, hydrogen cyanide, phosgene and mustard gas (dichlordiethyl sulfi de).
Responses were compared with UK national recommendations and a previous survey in 2009 [2] . Results Ninety-eight clinicians (anesthetics n = 51, emergency medicine n = 21, intensive care medicine n = 26) completed surveys. The best knowledge (76% correct) was for SARS, with less knowledge for anthrax, plague, EVD, and smallpox (60%). We found limited knowledge for chemical warfare agents (20 to 30%). Sixty to 80% of all incorrect responses were over-rated. There was no diff erence in knowledge compared with previous published results [2] . Conclusion Despite national and regional training since previous surveys [2] , the results indicate that further training on PPE is required for clinicians treating patients exposed to infectious diseases and CBRN agents, ideally in a simulation setting. ) and also to the combination therapy (71.8%/62.4%), but also a reduction in sensitivity to the group of beta-lactam antibiotics (58.2%/32.5%) and fl uoroquinolones (64.6%/36.4%). Conclusion The number of patients with sepsis has increased; the mortality of sepsis has decreased. The frequency of S. aureus isolation is still high, MRSA is the same. The frequency of Gramnegative fl ora isolation has increased, especially K. pneumoniae and Acinetobacter spp. The resistance of microorganisms to beta-lactams and fl uoroquinolones is rising but the sensitivity to aminoglycosides, glycopeptides, and carbapenems is still maintained.
Gram-positive bacteria (n = 24, 30%; Staphylococcus aureus (n = 16, 20%), Enterococcus faecalis (n = 4, 5%), Staphylococcus epidermidis (n = 3, 4%)), Gram-negative (n = 43, 53%; klebsiella sp. (n = 13, 16%), Pseudomonas aeruginosa (n = 7, 9%), Escherichia coli (n = 7, 9%)) and fungi (n = 5, 6%; candida sp. (n = 2, 3%), Candida albicans (n = 1, 1%), Candida dubliniensis (n = 1, 1%)). The commonly prescribed antimicrobials were piperacillin/tazobactam (n = 32, 40%), vancomycin (n = 30, 37%), polymyxin B (n = 23, 28%), cefepime (n = 16, 20%), meropenem (n = 12, 15%), cefuroxime (n = 8, 10%), ciprofl oxacin (n = 6, 7%), tigecycline (n = 6, 7%), ampicillin (n = 5, 6%), clindamycin (n = 4, 5%), chloramphenicol (n = 4, 5%), oxacillin (n = 4, 5%) and others (n = 32, 28%). There was 75% (n = 46) infection during hospitalization in the unit. Approximately 32% of infections were caused by multidrug-resistant pathogens, although there was effi ciency of 81% in the proper use of initial antimicrobials. Introduction Ventilator-associated pneumonia (VAP) is a common and serious problem in ICUs. Several studies have been conducted to determine the eff ectiveness of Gram stain of tracheal aspirates for diagnosing VAP. However, the eff ectiveness for predicting causative microorganisms and guiding appropriate initial antibiotic therapy has not been evaluated. The purpose of this study is to determine whether Gram stain of tracheal aspirates can guide appropriate initial antibiotic therapy for VAP. Methods We retrospectively assessed two hypothetical empirical antibiotic treatment algorithms for VAP on an 18-bed ICU. Data on consecutive episodes of microbiologically confi rmed VAP were collected over a period of 22 months and divided into a derivation (1 February 2013 to 30 November 2013) and validation (1 December 2013 until 15 November 2014) cohort. We constructed two algorithms in the derivation cohort. One is a local ecology-based algorithm (LEBA), according to clinical risk factors for MDR and susceptibility results in our hospital. The other is a Gram stain-based algorithm (GSBA). The selection of antibiotics on GSBA was directed against pathogens predicted from the results of bedside Gram staining of tracheal aspirates collected just before antibiotic therapy. Subsequently, LEBA and GSBA were retrospectively reviewed and compared with actually prescribed antibiotics in the validation cohort.
The fi rst 50 VAP episodes made up the derivation cohort and the subsequent 50 VAP episodes the validation cohort. Antibiotic coverage rates by applying LEBA and GSBA were identical (96% vs. 96%). GSBA proposed more narrow spectrum therapy as compared with LEBA (P <0.001). GSBA recommended carbapenems in signifi cantly less episodes than LEBA (P <0.001) and the same episodes as actually prescribed initial therapy (P = 1). However, there was signifi cant increase of antibiotic coverage rates in GSBA compared with the actually prescribed initial therapy (96% vs. 78%, P = 0.015). Conclusion Antibiotic coverage rates on GSBA were comparable with LEBA. The use of GSBA would result in a signifi cant reduction of the administration of broad-spectrum antibiotics. Bedside Gram staining may be useful to guide appropriate initial antibiotic therapy for VAP.
Introduction Urinary tract infection (UTI) is one of the most common bacterial infections in humans. Gram-negative organisms being the most common causative agent, the rising prevalence of resistance to a number of antibiotics and more importantly the production of extended spectrum beta-lactamase (ESBL) by these organisms is a growing concern worldwide. As the scenario is no better in community isolates, the choice of empirical antimicrobials for such infections becomes a great challenge for the clinicians.
Methods In this retrospective observational study we aimed at knowing the prevalence of ESBL production by organisms causing UTI in the community and to study the antibiogram of such isolates. Urine samples from patients with suspected UTI in the community were cultured for uropathogen by routine microbiological methods and susceptibility testing was done on Microscan Autoscan 4 (Siemens).
Out of 527 isolates of Enterobactereaceae, 314 (59.58%) were ESBL producers from the community samples compared with 315 (67.30%) from hospital samples, with Escherichia coli being the most commonly isolated pathogen. Enterobacter spp. showed highest prevalence (80%) of ESBL production from the community samples. Among the ESBL producing strains from the community, the sensitivity to ciprofl oxacin, levofl oxacin and nitrofurantoin was 18%, 21% and 44% respectively while in the non-ESBL producers the sensitivity rates were 52%, 51% and 73% respectively. Conclusion Organisms producing the ESBL phenotype present with an added possibility of being resistant to other broad-spectrum antimicrobial agents which are commonly prescribed in the community to empirically treat such infections. This makes the choice of empirical antibiotic much more challenging in the community, drawing errors in judgment. A possibility of frequent overcorrection lies on the other side of the coin. This study also shows the possible need for empirical institution of class I carbapenems as one of the treatment options and outpatient parenteral antimicrobial therapy.
Is it possible to predict multidrug-resistant organism colonization and/or infection at ICU admission? Whether these viral infections are associated with prolonged mechanical ventilation and worse outcomes remains to be determined.
The early identifi cation of severe sepsis and septic shock and early implementation of the SSC bundles were associated with reduced mortality [1] . The failure to initiate appropriate antimicrobial therapy increased mortality of septic shock patients [2] . We hypothesized that the parameter 'Consensus initial antimicrobial therapy with microbial cultures' correlates with outcome of septic shock patients. Introduction MDR infections in the ICU are not nosocomial all the time, as perceived commonly. We performed a 2-year retrospective study to analyze the source of culture positivity in a medical ICU and to identify which types of infections are more prevalent.
Methods The data of a 35-bed medical ICU were analyzed from November 2012 to October 2014. The source of culture positivity was divided into three groups: patients admitted from the ER to the ICU who were referred from other hospitals or direct admissions, the second group was patients admitted within the hospital but outside the ICU for the fi rst 48 hours, and the third group was ICU-acquired infections. We also analyzed the data for type of infections, whether Gram-negative, Gram-positive or fungal. Results There were 1,051 cultures positive in a 2-year period. In total, 46.8% (n = 492) of cultures were already positive on admission, which denotes community-acquired and referred patients from other hospitals. A total of 31.1% (n = 327) of cultures were positive from patients admitted to general wards for more than 48 hours and then transferred to the ICU. Twenty-two percent (n = 232) of cultures were ICU-acquired infections. The data show community-acquired and hospital-acquired infections are the bulk of the culture load in an ICU. This could be attributed to increased surveillance and adherence to infection control practices in the ICU which may not be followed stringently in other parts of the hospital. Overuse of broad-spectrum antibiotics in community and primary care hospitals has resulted in a spurt in growth of resistant infections. This has reached an alarming level in developing countries. Out of total cultures positive 78.3% (n = 822) were Gram-negative infections which included community-based and non-ICU infections. Conclusion Antibiotic stewardship and strict adherence to infection control protocols in hospitals and guidelines for general practitioners can signifi cantly reduce the load of resistant organisms in the ICU. This may eventually improve patient outcomes and help in preserving the antibiotics for future generations.
Introduction Early microbiological documentation may reduce attributable mortality and excessive use of broad-spectrum antibiotics in ventilator-associated pneumonia (VAP). Using bronchoalveolar lavage (BAL) and endotracheal aspirates (ETA), we studied a new molecular biology-based approach to detect and quantify bacteria in less than 3 hours. This prospective multicenter trial aimed at comparing the microbiological results obtained using this molecular protocol (easyMAG® system) and semiquantitative culture in suspected VAP. Methods ETA and BAL samples were consecutively collected during 10 months in adult patients in four ICUs of France. The molecular method includes a preprocessing liquefaction for ETA before DNA extraction. DNAs were extracted using the easyMAG® system. Realtime PCR (qPCR) was run using the ABI7500FastDx PCR instrument. The results presented here concern: Staphylococcus aureus, Pseudomonas aeruginosa and Enterobacteriaceae. Quantifi cation was performed using qPCR standard curves, by converting the cycle threshold to CFU/ ml. Results A total of 125 suspected VAP were included from 122 patients. In total, 125 BAL and 107 ETA were collected. Sex ratio (M/F) was 76%, and CPIS ≥6 was calculated in 74.6% of the suspected VAP patients. Mean ventilation duration before sampling was 6 days. Seventy-eight percent and 65% of the BAL and ETA culture were positive respectively. Correlations between molecular method and culture on BAL and ETA are reported in Table 1 . Conclusion Sensitivity and specifi city of the new molecular approach for these main bacteria found in VAP could enable targeted fi rst-line antibiotic therapy. In the future, the development of this approach will aim at obtaining a bedside diagnostic in only a few hours.
Use of Cepheid Xpert Carba-R® for rapid detection of carbapenemase-producing bacteria in critically ill, abdominal surgical patients: fi rst report of an observational study A Cortegiani, V Russotto, P Capuano, G Tricoli, DM Geraci, A Ghodousi, L Saporito, G Graziano, A Giarratano University of Palermo, Italy Critical Care 2015, 19(Suppl 1):P108 (doi: 10.1186/cc14188) Introduction Xpert Carba-R® (Cepheid®, USA) is a PCR-based assay for rapid (<1 hour) detection of bacteria carrying carbapenem-resistance genes (KPC, NDM, VIM, OXA-48, IMP-1). The aim of the study is to compare PCR with microbiological cultures in critically ill, abdominal surgical patients. Methods We performed an observational study at University Hospital 'P. Giaccone' Palermo. We enrolled abdominal surgical patients admitted to the ICU with suspected abdominal sepsis or developing sepsis during the ICU stay. We obtained two rectal swab specimens and two drainage samples to perform PCR assay and classic culture tests. We used Cohen's K to test concordance of results. We considered concordant those results of positive detection of carbapenemase-producing bacteria by both methods (even if a polymicrobial growth was observed by cultures) or negative results by both methods. Concordance was studied for rectal swab and drainage specimens. Antibiotic susceptibility testing was performed through a semiquantitative method. Results Eight complete samples sets were collected from seven patients. Seven rectal swab specimens were negative for both PCR and cultures. In one patient a positive culture from carbapenem-resistant P. aeruginosa was detected from the rectal swab resulting negative to PCR. In one patient a positive culture from carbapenem-resistant A. baumanii was detected by drainage culture resulting negative to PCR. In two cases a positive result was observed from both PCR and cultures of rectal swab and drainage specimens. Vim and KPC genes were detected in one case and A. baumanii and K. pneumoniae with carbapenem resistance were isolated from cultures. A KPC gene was detected by PCR in the other case, and K. pneumoniae with carbapenem resistance was isolated from cultures. In all other cases a negative result was observed by both PCR and cultures. Cohen's K of 0.71 (95% CI = 0.21 to 1) was observed for rectal swab and drainage specimens. Conclusion We need more data to evaluate the performance of PCR for rapid detection of carbapenemase-producing bacteria from rectal swabs and drainage of critically ill surgical patients even though its concordance with cultures seems to be good.
Introduction Antimicrobial resistance constitutes a growing global threat, driven in part by inappropriate antimicrobial prescribing [1] . Most hospitals implement antibiotic policies to promote antimicrobial stewardship. This audit examined the Royal Cornwall Hospital Trust (RCHT) Critical Care Department's compliance with the current standard defi ned in our local antimicrobial policy. This states that all antimicrobial prescriptions are to have an indication and review date recorded [2] . Sequential strategies to improve compliance were introduced prior to re-auditing the eff ects. Methods The RCHT Critical Care Department utilizes the Phillips Care Vue electronic patient record. Data from this system were interrogated at three stages to assess our compliance with the trust's antimicrobial policy. The fi rst data interrogation was performed prior to any intervention, and refl ected baseline antimicrobial prescribing habits. The second data interrogation was performed during a period of active antibiotic stewardship promotion. The third data interrogation was performed following the addition of a care bundle to the prescribing module of Care Vue. This daily tick-box prompt reminded clinicians to check that all antimicrobial prescriptions had an indication and review date recorded. The records of all of the patients admitted to the critical care department during the periods of data interrogations were assessed for antimicrobial indication and review date transcription. Results From the fi rst data interrogation, antimicrobial prescriptions had an indication and review date transcription in 57% and 60% of cases respectively. Following the awareness campaign, the indication and review date transcription rate increased to 78% and 85% respectively. A daily electronic prompt was then added to our care bundle list. The fi nal data interrogation, performed after this intervention, demonstrated that the transcription rates for both the indication and the review date had increased to 96%. Conclusion We have demonstrated that the use of a daily prompt within an electronic patient record can greatly improve compliance in recording the indication and review date for all antimicrobials. These data support the widespread implementation of an electronic prescribing system where daily reminders are integrated in an eff ort to improve compliance with antimicrobial stewardship.
Critical Care 2015, Volume 19 Suppl 1 http://ccforum.com/supplements/19/S1
Introduction Multidrug-resistant (MDR) bacterial pneumonia is associated with signifi cant morbidity and mortality in severely ill ICU patients. The assessment of factors associated with the onset and clinical course of MDR pneumonia may improve treatment eff ectiveness. The purpose of this study is to identify factors associated with outcome in mechanically ventilated patients with ventilatorassociated pneumonia (VAP) caused by MDR bacteria. Methods We studied retrospectively all mechanically ventilated patients treated in the A' ICU of KAT General Hospital in Athens from 1 January 2011 to 31 December 2013 and showed ventilator-associated pneumonia from MDR Gram-negative bacteria. Standard demographic and clinical data, the causative organisms and outcome were recorded. For statistical signifi cance, chi-square and Student t tests were used. Results A total of 102 mechanically ventilated patients, 75 men and 27 women, were included in the study. All patients showed VAP caused by MDR bacteria. They were stratifi ed by outcome into survivors and nonsurvivors. ICU mortality was 55%. Gender, cause of admission, the causative microbe, colonization of bronchial secretions and secondary bacteremia had no correlation with outcome. Age and APACHE II score were higher in nonsurvivors (P <0.01 and P <0.05 respectively). The time-onset of pneumonia after admission was longer in patients with VAP caused by Klebsiella or Pseudomonas than those with VAP caused by acinetobacter (P <0.01). Patients with Klebsiella or Pseudomonas pneumoniae needed more time on mechanical ventilation than those with pneumonia from acinetobacter (P <0.01). Conclusion VAP caused by MDR bacteria is a leading cause of ICU death. Age and APACHE II score are signifi cant risk factors of death.
Introduction Prescription of antifungal treatments (AFT) in ICUs in case of suspected or confi rmed invasive candidiasis (SIC or CIC) has been challenged by diff erent guidelines. The study aimed to describe the epidemiology of the invasive candidiasis (IC), analyze the criteria for the AFT initiation, the AFT type, and its changes during patient follow-up. . Candida albicans was the main pathogen (67%), then C. glabrata (16%). At inclusion, CIC were treated with caspofungin (Cas): 55%, and fl uconazole (Flu): 34%, whereas these antifungals were administered to 46% and 45% of SIC, respectively. Patients with SIC were more severe than those with CIC. The two main criteria for initiating empirically an AFT were a central venous catheter (79%) and severe septic shock (70%). The rate of change of the initial AFT was higher in the CIC group (49%) than in the SIC group (33%, P <0.0001). In the CIC group, it was mostly for changing the antifungal agent (de-escalation Cas  Flu in half of the patients) based on mycological tests results. In the SIC group, the AFT was modifi ed almost as often for changing the drugs (including 22% de-escalation Cas  Flu) as for stopping the AFT. The 28-day mortality of candidaemia was 42% in cases of C. glabrata, 40% in cases of C. albicans, and 20% in cases of C. parapsilosis. Among survivors, the median duration of treatment was 17 to 21 days according to the infection site in cases of CIC, and 10 days in cases of SIC. Conclusion French ICU patients are treated with antifungal agents selected according to the candidiasis severity, contrary to ESCMID guidelines which recommend initiating with echinocandins regardless of severity. As recommended, the therapy was secondarily adapted to microbiological results.
Introduction Micafungin (MCF) is an echinocandin agent with broad activity against Candida spp., which are frequently isolated in blood and eschar cultures of burned patients, who present diff erent pharmacokinetics (PK) characteristics. Due to the limited information about its PK, we investigate MCF levels in plasma and burn eschar tissues in this population. Methods A PK study of MCF at standard dosage (100 mg/day). Cmax (end of the infusion) and Cmin (before next dose) plasma levels of MCF were obtained after fi rst dose and at steady state (days 4 and 5 of therapy); and on day 5 in eschars (1 to 3 hours after infusion). They were measured by HPLC. Spearman's rho test was used for bivariate correlations between MCF exposure and patient's clinical factors. Results There were 10 patients (eight men; age: 18 to 77 years). Patients' characteristics and PK are shown in Table 1 . A high interindividual variability was observed in the concentrations of MCF. Peak plasma concentrations after the fi rst and repeated doses of MCF were inversely correlated with % burned TBSA (Spearman's ρ = -0.695 and -0.750 (P <0.05), respectively), but not with the time from burn injury. MCF concentrations in burn eschars were not correlated with % burned TBSA. MCF was well tolerated. One patient had candidemia. The crude mortality was 40%. Conclusion This is the largest PK study of 100 mg daily of MCF in severely burned critically ill patients. The inverse correlation between MCF exposure and % burned TBSA suggests that patients with large burned TBSA may need higher doses of MCF. Nevertheless, MCF levels in plasma and burn eschar tissues after the fi rst and multiple doses were above the MIC90 against most clinically important Candida species.
for treatment of nosocomial pneumonia, common in critically ill patients with acute kidney injury. There are limited data on tedizolid disposition in continuous renal replacement therapy (CRRT). This study's purpose was to assess continuous hemofi ltration (CHF) and continuous hemodialysis (CHD) infl uence on tedizolid clearance. Methods Validated, bovine blood-based, in vitro CHF and CHD models were used with six new HF 1400 (polysulfone) and six new Multifl ow 150 (AN 69) hemodiafi lters. Tedizolid's transmembrane clearances (CLTM) during CHF and CHD were assessed by measuring sieving (SC) and saturation (SA) coeffi cients at various ultrafi ltrate (Quf ) (1, 2, 3 l/ hour) and dialysate fl ow rates (Qd) (1, 2, 3 and 6 l/hour), using a blood fl ow rate (Qb) of 200 ml/minute. Tedizolid adsorption was tested in a 1 l recirculating CHF model at Quf of 2 l/hour and Qb of 200 ml/minute over 4 hours. Adsorption (%) was calculated after correcting for the dilution by CHF priming volume. Urea was added as a control in all experiments. Results Urea SC and SA were ~1 in all experiments. In CHF, mean tedizolid SC ranged from 0.52 to 0.57 for HF1400 and from 0.50 to 0.54 for M150. CLTM did not diff er between fi lter types for Quf of 1, 2, and 3 l/hour. In CHD, mean tedizolid SA ranged from 0.46 to 0.56 for HF1400 and from 0.38 to 0.44 for M 150. Tedizolid CLTM with the HF1400 was higher than M150 values at Qd of 6 l/hours (P <0.02). Tedizolid exhibited irreversible adsorption within 10 minutes. See Figure 1 . Conclusion Tedizolid's CLTM is dependent on hemodiafi lter type and Qd for CHD and Quf in CHF. At conventional CRRT rates, tedizolid CLTM appears modest relative to total body clearance and is unlikely to require dose adjustments. CRRT adsorption in the clinical setting is likely less than what we observed in this in vitro, continuously recirculating blood model.
Tedizolid phosphate, a novel oxazolidinone antibacterial prodrug recently approved by the US Food and Drug Administration for the treatment of acute bacterial skin and skin structure infections, is available as oral (that is, tablets) and intravenous formulations. The clinical pharmacokinetics of tedizolid, the active moiety of tedizolid phosphate, are similar when orally administered tedizolid phosphate is given as powder in a capsule or as tablets. This suggests that crushing tablets prior to administration is unlikely to alter tedizolid pharmacokinetics, provided no drug is lost during administration. To determine whether the expected dose of tedizolid phosphate can be delivered via nasogastric (NG) tube in critically ill patients who have diffi culty swallowing, this study evaluated the stability and recovery of tedizolid phosphate 200 mg tablets after crushing, dispersion in water, and passage through an NG tube. Methods For each assay, run in triplicate, one 200 mg tablet of tedizolid phosphate was crushed, dispersed in water, drained under gravity through one of two types of NG tubes (type 1, Kangaroo Nasogastric Feeding Tube, 10 Fr 43" (109 cm); type 2, Salem Sump Dual Lumen Stomach Tube, 18 Fr/CH (6.0 m) 48" (122 cm)), and collected for recovery analysis by high-performance liquid chromatography with UV detection. To analyze the chemical stability of the crushed tablet dispersed in water, the aqueous preparation was assayed initially after dispersion and again after 4 hours at room temperature, without NG tube passage. The prespecifi ed limit for tedizolid phosphate in recovery samples was 90 to 110% of the dose. Limits were also specifi ed for levels of certain impurities. Results The average and individual recovery values of tedizolid phosphate were within 90 to 110% of the 200 mg dose when crushed tablets, dispersed in water at room temperature, were transferred through the 2 NG tubes (type 1: 95.8%; type 2: 93.6%). There was no signifi cant change in recovery values after 4 hours of storage at room temperature (93.9% initially and 94.7% after 4 hours). Results for degradation products and impurities were also within specifi ed limits in NG recovery samples and in the 0-hour and 4-hour aqueous preparations.
Conclusion The stability and recovery of tedizolid phosphate were not infl uenced by crushing the tablets and passing through an NG tube. Therefore, administration of crushed tedizolid phosphate tablets to patients is unlikely to alter the pharmacokinetics of tedizolid compared with whole tablets. Introduction In vitro studies suggest that there is signifi cant adsorption of amikacin, netilmicin, gentamicin and tobramycin to polyacrylonitrile haemofi lters. This occurs rapidly and has the potential to substantially reduce the peak aminoglycoside concentration, which will reduce effi cacy [1] . However, whether signifi cant adsorption occurs in vivo is unknown. We therefore carried out a controlled in vivo study of the eff ect of amikacin adsorption by polyacrylonitrile fi lters during haemofi ltration, using a porcine model of acute renal failure. Methods A porcine model of acute renal failure was created by bilateral ligation of the renal arteries and veins. Eight pigs underwent haemofi ltration using a 0.6 m 2 polyacrylonitrile fi lter, blood fl ow 200 ml/ minute, ultrafi ltration rate 1,000 ml/hour. All ultrafi ltrate was returned to the pigs via a separate venous catheter so that any elimination of amikacin by haemofi ltration could only be due to adsorption. Another eight pigs underwent sham haemofi ltration in which blood was pumped around a haemofi ltration circuit without a haemofi lter and without ultrafi ltration. Both groups of pigs were given intravenous amikacin, 15 mg/kg body weight over 30 minutes, and blood samples were taken from the arterial limb of the haemofi lter circuit at 0, 5, 10, 15, 20, 25, 30, 40, 50, 60, 75, 90, 105, 120, 150, and 180 minutes after the start of the amikacin administration to assay amikacin concentrations.
Results Post-distribution peak concentration of amikacin was slightly, but signifi cantly, lower in the CRRT group than that in sham group (55.0 ± 4.5 vs. 61.1 ± 5.9 mg/l, P <0.05). Conclusion This study shows that the eff ect of adsorption by polyacrylonitrile haemofi lters on in vivo amikacin peak concentrations is small, and less than would be expected from in vitro data. Table 1 and compared with the ones reported in the ICU [1] . Introduction Direct hemoperfusion with a polymyxin B immobilized column (PMX-DHP) adsorbs endotoxin and has been used for the treatment of septic shock [1] . However, the mechanisms of action behind PMX-DHP are not fully understood. Therefore, the purpose of this study was to elucidate mechanisms of action behind PMX-DHP in a rat model of cecal ligation and puncture. Methods Sprague-Dawley rats were anesthetized and were mechanical ventilated after tracheostomy. The right internal carotid artery was cannulated with a catheter for continuous measurement of the arterial pressure and heart rate. The right femoral vein was cannulated with a catheter for infusion of saline (10 ml/kg/hour) during the study period. The rats were randomized into three experimental groups: cecal ligation and puncture (CLP) + dummy column (Dummy-DHP) group (n = 10), CLP + PMX-DHP group (n = 10), and sham group (n = 4). Four hours after CLP, Dummy-DHP or PMX-DHP was performed for 1 hour. Blood was drawn from the right internal carotid artery, perfused through PMX column or dummy column, and returned to the right femoral vein. The heart rate, mean arterial pressure, arterial blood gases, and plasma concentrations of creatinine, lactate, potassium, and cytokines (IL-6 and IL-10) were measured at baseline and at 4, 5, and 8 hours after CLP. At the completion of the experiment, the rats were killed overdose of pentobarbital. The kidney, liver, and lung were harvested, and histopathologic examinations of these organs were performed. Results Hypotension and metabolic acidosis occurred in the CLP + Dummy-DHP group, whereas hemodynamics and acid-base balance were better maintained in the CLP + PMX-DHP group. Plasma concentrations of lactate, creatinine, potassium, and cytokines were signifi cantly higher in the CLP + Dummy-DHP group than in the CLP + PMX-DHP group at 8 hours. Renal tubular cell death was observed in the CLP + Dummy-DHP group, but not in the CLP + PMX-DHP group.
Conclusion PMX-DHP improved hemodynamics, acid-base balance, and creatinine levels through reducing cytokines and renal tubular cell death in a rat model of cecal ligation and puncture. These fi ndings suggest the preventive role of PMX-DHP in the development of sepsisrelated acute kidney injury. Reference
Introduction The mortality rate of severe sepsis and septic shock is varied and high (25 to 70%). In our institute, the indication for polymyxin-B immobilized fi ber with direct hemoperfusion (PMX-DHP) has been that circulatory failure (systolic blood pressure <90 mmHg or required catecholamines and high lactacidemia) continued despite following early goal-directed therapy by the Surviving Sepsis Campaign guidelines 2012. Methods This study included 80 patients with severe sepsis or septic shock due to abdominal infection retrospectively. These subjects were divided into two groups: those with WBC counts <4,000 (L-group: 64 patients) and those with WBC counts >12,000 (H-group: 16 patients).
Mean arterial pressure, WBC counts, platelet counts, interleukin-6 (IL-6), and plasminogen activator inhibitor-1 (PAI-1) were measured immediately before the initiation and after the completion of PMX-DHP. Statistical analysis was performed using the chi-squared test for background factors, with Wilcoxon's rank-sum test for comparison within a group, and Mann-Whitney's U test for comparison between groups. The signifi cance level was set at P <0.05.
The mortality rate of 28 days in the L-group was 32.8%, and was 18.8% in the H-group. Mean arterial pressure increased signifi cantly (P <0.01) in the H-group compared with the L-group. WBC counts in the L-group increased and in the H-group decreased (P <0.01) during PMX-DHP treatment. Platelet counts in both groups decreased signifi cantly (P <0.01).There was no signifi cant diff erence between before and after PMX-DHP in IL-6 levels. On the other hand, IL-1ra decreased signifi cantly before and after PMX-DHP. Also, IL-6 and IL-1ra in the L-group were signifi cantly higher than those in the H-group at the start of PMX-DHP. PCT values in the L-group were increased compared with the H-group at the start of PMX-DHP (P <0.01). PCT in the L-group increased signifi cantly (P <0.01), but no signifi cant changes in the H-group. PAI-1 showed no signifi cant changes before and after PMX-DHP and no changes in both groups at the start of PMX-DHP. Conclusion The mortality rate of the L-group tended to be higher than that of the H-group. Infl ammatory and anti-infl ammatory cytokines in the L-group were higher than those of the H-group. These results indicate that leukopenia (WBC <4,000) in severe sepsis patients leads to more severe outcome and hypercytokinemia than leukocytosis (WBC >12,000) in severe sepsis patients.
exchange therapy may improve thrombotic microangiopathy [1] . The purpose of this observational cohort study is to describe whether there is an association between use of plasma exchange therapy and outcome in the Turkish TAMOF network. Methods We performed a retrospective cohort analysis in patients with TAMOF at three diff erent pediatric ICUs comparing those who received plasma exchange (+) plus standard therapies with those who did not receive plasma exchange (-) and only received standard therapies. Introduction Mortality from septic shock in the ICU remains high, ranging from 30 to 50%. In particular, Gram-negative bacilli (GNB) account for 40% of the causative bacteria of severe sepsis, which progresses to multiorgan failure due to signifi cant infl ammation. Hemoperfusion with polymyxin B-immobilized fi ber (PMX) adsorbs endotoxin and can reduce the infl ammatory cascade of sepsis due to GNB. However, the clinical effi cacy of this treatment has not been demonstrated. We aimed to verify the effi cacy of endotoxin adsorption therapy by using PMX. Methods We retrospectively evaluated 387 patients who received a broad-spectrum antimicrobial treatment for septic shock due to GNB between January 2009 and December 2012 in the ICU of 10 Japanese tertiary hospitals. After alignment of the treatment time phase for each patient, we divided the patients into two groups according to whether PMX treatment was performed within 24 hours after ICU admission (PMX group: n = 129 and non-PMX group: n = 258). The primary endpoint was 28-day mortality. Results The mean (SD) age and SOFA scores on ICU admission were 72.5 (12.5) years and 10.0 (3.4), respectively. The infection site was intra-abdominal (47.0%), pulmonary (17.6%), and urinary tract (27.8%). Two-thirds of all patients had bacteremia due to GNB. No diff erence in 28-day mortality was observed between the two groups (PMX: 33.9% vs. non-PMX: 33.1%, P = 0.87). In the Cox regression analysis adjusted for age, sex and facilities, the PMX treatment (hazard ratio = 0.87; 95% confi dence interval, 0.53 to 1.43) did not improve the outcome. Conclusion No diff erence in mortality rate was observed after adjustment for the endotoxin adsorption therapy with PMX in the patients with septic shock due to GNB.
evaluate the impact of an evolving regional cardiac catheterisation service on a regional intensive care unit (RICU) serving a population of 1.8 million.
Methods A retrospective review was carried out. Patients admitted from the regional cardiac catheterisation laboratory to the regional ICU, between September 2009 and September 2014, were identifi ed using validated RICU admission records. Clinical data were extracted from computerised patient records. [1] . Previous work from our group has shown that incorporating simulation-based teaching elements into a basic TTE course improves candidates' satisfaction [2] . We assessed the impact of introducing the ICE-BLU e-learning programme prior to our simulation-based basic TTE course. Methods Prior to the August 2014 course, all candidates were required to complete the ICE-BLU e-learning module. On the morning of the course, the candidates completed a questionnaire to assess the impact of the e-learning module. The survey included questions on the quality of content, user friendliness, whether the content was pitched at the right level and any problems faced whilst accessing the e-learning module. We also analysed candidates' feedback from our January and August 2014 courses ( Figure 1 ).
The response rate of the survey was 100%. Eighty per cent of candidates completed the e-learning module. The e-learning module was rated high by most candidates (80%). However, nearly one-half of the candidates faced problems accessing the module, online. Analysis of candidates' feedback (from the January and August 2014 courses) revealed that candidates' overall impression was better with the introduction of e-learning prior to the course.
Our survey has shown that the e-learning initiative was welcome by the candidates. We conclude that introduction of e-learning prior to a simulation-based basic TTE course enhances candidates' satisfaction and feedback.
Deep venous thrombosis (DVT) is an increasing major cause of mortality and morbidity. There is a need for quick, easy, cheap, convenient and reliable diagnostic tools. The objectives were to ascertain the diagnostic concordance of emergency doctorperformed ultrasound (EDUS) of the lower extremities with specialist doctor-performed (radiologist or vascular surgeon) echo-Doppler (SDED), in the diagnosis of DVT, and to identify possible causes of nonconcordance.
Methods A prospective, multicentre study. Adult patients (>18 years old) with clinical suspicion of DVT, with high or moderate risk (on Wells scoring), or low risk with increased D-dimer levels, were eligible for the study. Emergency doctors performed two EDUS in femoral and popliteal areas (these results were blinded). After this, echo-Doppler was performed by specialist doctors. Both procedures were done within 24 hours of each other. The fi nal result was considered nonconcordant if one or both of the EDUS did not match with the SDED. These SDED were used as reference (as standard clinical practice). Results From September 2013 to September 2014, a total of 328 patients were enrolled. Fifty-one investigators from seven hospitals performed the EDUS. Each patient had the EDUS (femoral and popliteal areas) and SDED (also in femoral and popliteal areas). Of 328 pairs of US studies, 37 were nonconcordant between EDUS and SDED. Two EDUS were incomplete, so the concordance analysis was performed with 326 US studies, with 35 discordant. The percentage of agreement between EDUS and SDED was 89.26%. The kappa index was 0.76 (95% CI = 0.69 to 0.84), and this means a substantial agreement. Conclusion There is substantial agreement between EDUS and SDED in the diagnosis of DVT, in routine clinical practice. This confi rms the results of previous papers. The largest nondiagnostic concordance in thrombus occurs in the early performances of emergency doctors, Introduction Sepsis-induced myocardial dysfunction is a wellrecognized condition and confers worse outcomes in septic patients. However, the diagnostic criteria remain poorly described. Echocardiographic assessment by conventional parameters such as left ventricular ejection fraction (LVEF) is often aff ected by ongoing changes in preload and afterload conditions. Novel echocardiographic technologies such as speckle tracking imaging have evolved for direct assessment of the myocardial function. In this study, we investigate the measurement of myocardial strain by speckle tracking imaging for the diagnosis of sepsis-induced myocardial dysfunction. Methods This is a prospective, case-control study at a universityaffi liated tertiary care adult medical ICU. Consecutive patients admitted with a diagnosis of septic shock meeting the international consensus criteria were included. Patients with other causes of myocardial dysfunction were excluded. They are compared with age-matched, gender-matched, and cardiovascular risk factor-matched controls, who were admitted to hospital for sepsis but did not develop septic shock. Conventional echocardiographic parameters, as well as speckle tracking imaging of myocardial function, were obtained within 24 hours of diagnosis. Offl ine analyses of endocardial tracings were performed by two independent operators. Results From January 2014 to December 2014, 32 patients with septic shock (study group) and 20 patients with sepsis but no septic shock (control group) were recruited. The baseline characteristics were similar. Conventional echocardiographic measurements, including LVEF (59.53% vs. 60.67% in the control group, P = 0.450) and fractional shortening (31.98% vs. 32.98%, P = 0.323), did not diff er between the two groups. The study group had a greater degree of myocardial dysfunction measured by left ventricular global longitudinal strain (-14.6 vs. -17.6, P = 0.005, with a less negative value implying worse myocardial contractility). The hemodynamic profi les (cardiac index 3.49 l/minute/m 2 vs. 3.41 l/minute/ m 2 respectively, P = 0.764) were not statistically diff erent. Conclusion This is a fi rst study in the adult population to show that the use of speckle tracking imaging can diagnose signifi cant sepsisinduced myocardial dysfunction, which was not otherwise detectable by conventional echocardiography.
Comparing group I versus group II, the mortality rate was 45%, and there was a statistically signifi cant diff erence for temperature (P = 0.001), HR (P = 0.001) and WBC count (P = 0.01) on admission. Upon comparing survivors versus nonsurvivors in group I there was a statistical diff erence in HR on day 7 (P = 0.02), successful vasopressor withdrawal (P = 0.02), P/F ratio (P = 0.02) and ScVO 2 on day 7 (P = 0.03).
Regarding IL-1α, IL-1β, TNFα and troponin I there was no statistical signifi cant diff erence between groups I and II but IL-6, IL-10 and CRP showed statistically signifi cant diff erence on admission PV and CS. Pro-BNP shows statistically signifi cant diff erence in all CS samples between septic and nonseptic groups. Regarding echo upon comparing the survivors versus nonsurvivors, E'd/t on day 0 shows a statistically signifi cant diff erence between both groups. SAPS II and seventh-day SOFA are good predictive scores for mortality in sepsis.
Conclusion Diastolic dysfunction was seen in 90% of patients. Fever, HR, and WBC counts are still good early indicators for diagnosis of sepsis. Vasopressor withdrawal on the seventh day was a good predictor for survival. Admission serum IL-6, IL-10 and CRP from PV were better indicators for sepsis than IL-1, pro-BNP and troponin I. Admission TNFα and seventh-day IL-6 levels were highly prognostic for mortality. CS samples proved that NT pro-BNP is a good indicator for sepsis diagnosis and a good predictor for survival. TNFα from CS samples was also a good predictor of mortality. SAPS II and a slower E'd/t on admission was a good predictor of mortality.
Introduction Myocardial depression is common among septic patients [1] . The aim of this study was to assess whether the values of cardiac index (CI) and velocity-time integral (VTI) calculated by echocardiography diff er between survivors and nonsurvivors of sepsis. Methods This was a prospective observational study. We included adult newly admitted septic patients, regardless of disease severity. Exclusion criteria were concomitant pregnancy or obstetric/gyneco logical sepsis and co-existing or terminal diseases that may limit life expectancy. At the moment of recruitment, additional exclusion criteria included: concomitant pulmonary embolism, trauma or acute ischemic coronary disease; pericardial tamponade; aortic valve disease; tachyarrhythmias and absence of adequate echocardiographic windows. Echocardiographic evaluations were made within the fi rst 10 minutes of initiation of fl uid therapy in the emergency room. All measurements and images were obtained with a 1.5 to 3.5 MHz phased array transducer using a standard cardiac preset. CI is the quotient of the cardiac output (CO) divided by the body surface area. The CO is the product of the stroke volume by the heart rate. Stroke volume is calculated as the product between aortic VTI (measured using pulsedwave Doppler) and aortic cross-sectional area. The latter is calculated in the long axis parasternal window using the left ventricular outfl ow tract diameter measurement. Introduction Quantifi cation of lung ultrasound (LUS) artifacts (B-lines) is used to assess pulmonary congestion in emergency medicine and cardiology [1, 2] . We investigated B-lines in relation to extravascular lung-water index (EVLWI) from invasive transpulmonary thermodilution in septic shock patients. Our aim was to evaluate the role of LUS in an intensive care setting. Methods Twenty-one patients admitted with septic shock to a general ICU underwent LUS of eight zones, four per hemithorax, within 24 hours after ICU admission. EVLWI was calculated simultaneously by transpulmonary thermodilution using a pulse-contour continuous cardiac output system, and NT-proBNP and clinical data were collected. Two physicians blinded to other data independently quantifi ed the number of B-lines. Spearman's rho was used to test the correlation of B-lines to EVLWI and clinical data, and linear regression and Bland-Altman analysis were used to assess the agreement between B-lines and EVLWI. Interobserver variability was tested using Bland-Altman analysis and intraclass correlation coeffi cient (ICC). Results Fourteen patients (67%) were male, the median age was 62 years (IQR 55 to 68) and eight (38%) patients had cardiac comorbidities. In median, SAPS 3 was 64 (IQR 60 to 74), ICU length of stay was 3 days (IQR 2 to 8) and seven patients (33%) died within 30 days of ICU admission. All patients were mechanically ventilated and treated according to guidelines [3] . The median number of B-lines was 15 (IQR 10 to 30) and the median (IQR) NT-proBNP, EVLWI and oxygenation index (OI) were 7,800 ng/l (3,690 to 15,050), 11 ml/kg (IQR 8 to 18) and 9.2 (5.7 to 15.7), respectively. None of the characteristics diff ered signifi cantly between survivors and nonsurvivors. The number of B-lines correlated to EVLWI (ρ = 0.45, P = 0.04; r 2 = 0.20, P = 0.04), but not to NT-proBNP (ρ = -0.42, P = 0.06), OI (ρ = 0.25, P = 0.31) or ICU length of stay (ρ = 0.14, P = 0.57). On Bland-Altman analysis, mean diff erences and 95% limits of agreements between B-lines and EVLWI was 4.9 (-14.5 to 24.5), and 5.9 (-3.5 to 15.3) when assessing observer agreement. The ICC between methods was 0.52 (95% CI = -0.17 to 0.81) and 0.90 (95% CI = 0.73 to 0.92) between observers. Conclusion LUS non-invasively and user-independently quantifi es lung water in concordance with, but does not replace, invasive measurements. Further studies are needed establish the role of LUS as a monitoring and diagnostic tool in septic shock patients.
Introduction Near-infrared spectroscopy (NIRS) with a vascular occlusion test (VOT) can be used to extrapolate information regarding the tissue oxygen extraction rate. We explored the meaning of variations in tissue oxygen saturation downslope (StO 2 down) during a VOT in critically ill patients.
Methods In this prospective observational study, NIRS (thenar eminence) was applied every day in 93 patients admitted to the ICU. A VOT was performed using a 40% StO 2 target. The slope of the desaturation curve was assessed separately for the fi rst part (StO 2 Down1) and the last part (StO 2 Down2) of the curve and the diff erence between Down2 -Down1 was calculated.
Results No signifi cant diff erences were seen in StO 2 Down1 or Down2 between ICU survivors (n = 76) and ICU nonsurvivors (n = 17) over the fi rst 10 days in the ICU, while Down2 -Down1 was higher in ICU nonsurvivors ( Figure 1 ). Patients in the upper quartile of mean Down2 -Down1 showed the highest 90-day mortality (P = 0.014).
Conclusion ICU nonsurvivors tended to show a fl attening in the last part of the desaturation curve during a VOT, suggesting a reduced tissue oxygen extraction. This may depend on microvascular dysfunction and/or cellular hypometabolic status.
Introduction It is well known that oxygen acts as a vasoconstrictor.
We evaluated the impact of normobaric hyperoxia on the sublingual microcirculation in critically ill patients. Methods Forty mechanically ventilated (FiO 2 ≤50%) patients with hemodynamic stability were enrolled in a prospective observational study. The fi rst 20 patients underwent a 2-hour period of hyperoxia (FiO 2 = 100%), and 20 patients were studied as controls (no FiO 2 variations). The sublingual microcirculation (three sites) was evaluated with sidestream dark-fi eld imaging at baseline (t0), after 2 hours of hyperoxia (t1), and 2 hours after return to baseline (t2). Continuous video recording was also performed during FiO 2 variations on one and the same area (2-minute video).
No changes in mean arterial pressure were observed. The perfused small vessel density tended to decrease at t1 and normalize at t2 (Figure 1 ) in the hyperoxia group. These variations appeared early after 2 minutes of FiO 2 changes. A signifi cant increase in lactate levels over time (from 1.1 (0.9 to 1.7) at t0 to 1.4 (1.1 to 1.9) mmol/l at t2, P = 0.01) was seen in the hyperoxia group.
Conclusion Hyperoxia induces an early decrease in microvascular perfusion, which appears to go back to normality at return to normoxia. Healthy volunteers (n = 27) were studied as controls. The slope of the desaturation curve was assessed separately for the fi rst (StO 2 Down1) and the last part (StO 2 Down2) of the curve and the diff erence between, Down2 -Down1, was calculated. Results StO 2 Down1 was lower in healthy volunteers as compared with septic patients (P <0.05); no diff erence was seen between ICU survivors (n = 7) and nonsurvivors (n = 7). StO 2 Down2 was similar between healthy volunteers and ICU survivors, while it was higher in nonsurvivors (P <0.01 vs. healthy). ICU nonsurvivors showed higher Down2 -Down1 as compared with ICU survivors (P <0.01, Figure 1 ). Conclusion Tissue oxygen extraction was reduced in septic patients. Nonsurvivors showed a fl attening in the last part of the desaturation curve during a VOT, while the fi rst part of the StO 2 downslope did not show any diff erence between survivors and nonsurvivors. This may refl ect a tissue hypometabolic status, which may be better elicited in the fi nal part of the ischemic challenge.
Prospective nonrandomized observational study of the use of an impedance threshold device in patients with spontaneous respiration and hemodynamic instability C Pantazopoulos 1 is an indicator of end-organ hypoperfusion. We hypothesize that the lactate (LAC) normalization had prognostic value in this cohort of patients.
We performed a retrospective observational study on patients admitted to the ICU for refractory CS from January 2010 to November 2014. Patients with postcardiotomy and/or post-transplant CS were excluded. Demographics, clinical, hemodynamic and biochemical values were collected. LAC was measured on arterial blood before ECMO institution (LAC0) and after 48 hours (LAC48). Lactate clearance was calculated as follows: (LAC0 -LAC48) / LAC0 × 100. Data were analyzed by comparative statistic; sensibility and specifi city were tested with ROC. Results Twenty-three patients underwent VA ECMO for refractory CS in the study period. Etiologies of CS were: 11 acute myocarditis, fi ve acute myocardial infarction and seven acute decompensation of chronic cardiomyopathy. The median time of ECMO was 10 days (4 to 15). Thirteen patients died during hospital stay and 10 survived. Three patients were bridged to LVAD and two to heart transplant; eight were bridged to recovery. The main cause of ICU death was multiple organ dysfunction (12/13). Nonsurvivors showed signifi cantly higher LAC0 (5 (2 to 6) vs. 8 (5 to 11), P = 0.021). Lactate clearance at 48 hours was not signifi cantly diff erent between survivors and nonsurvivors (79%, 95% CI = 67 to 86 vs. 60%, 95% CI = 32 to 72, P = 0.08). However, LAC48 was predictive for ICU mortality (AUC 0.82; 95% CI = 0.64 to 1.0; P = 0.011). ROC curve analysis identifi ed the accuracy was highest by setting the lactate <2 mmol/l. Patients that did not normalize lactate (LAC <2 mmol/l) after 48 hours despite hemodynamic restoration had poorer outcome at 30 days, as is shown in the Kaplan-Meier curve (logrank P = 0.006) ( Figure 1 ). Conclusion Failing to normalize patient's LAC in the fi rst 48 hours of VA ECMO assistance for CS is a predictor of ICU mortality. Targeting LAC level <2 mmol/l at 48 hours post ECMO institution might be a reasonable goal for these patients.
Is an inotrope score a predictor of mortality and morbidity in children with septic shock? Introduction Inotropes and vasoactive drugs in septic shock are commonly used to maintain cardiac output, tissue perfusion and oxygenation. We undertook this study with the purpose of evaluating an inotropic score as a predictor of mortality and morbidity among children who diagnosed septic shock. Methods A multicenter retrospective chart review was performed in two pediatric ICUs. A total of 93 children with septic shock were recruited. Hourly doses of following inotropes were recorded for the fi rst 48 hours after admission: dopamine, dobutamine, adrenaline and noradrenaline. The inotrope score for every hour, minimum, maximum and mean values for the fi rst 24 hours, and subsequent 24 hours were calculated. In our analysis, the inotrope score was calculated as described by Wernovsky. We expanded this formula to include norephinephrine as follows: Wernovsky Inotrope Score = dopamine dose (μg/kg/minute) + dobutamine dose (μg/kg/minute) + 100 × epinephrine dose (μg/kg/ minute). Our adjusted inotrope score = Wernovsky Inotrope Score + 100 × norepinephrine dose (μg/kg/minute).
Results Forty-two of 93 patients died. Age and sex were not diff erent between survivors and nonsurvivors. Signifi cantly higher mean and maximum inotropic score for the fi rst 24 hours and fi rst 48 hours were found in nonsurvivors than those of survivors (P <0.05). Using 15 as a cutoff point for predicting mortality, the sensitivity and specifi city were 69.76% and 50.98% respectively. The association between Prism scores and minimum, mean and maximum inotrope scores were statistically signifi cant for 0 to 24 hours, 25 to 48 hours and 0 to 48 hours. Mean 0 to 24 hours and maximum 0 to 48 hours inotrope scores were weakly associated with prolonged ICU stay (P = 0.047, P = 0.042 respectively). There were no signifi cant relationships between inotrope scores and receiving mechanical ventilation. Conclusion The mean and maximum inotropic scores in the fi rst 24 hours and 0 to 48 hours are an independent predictor of mortality in critically ill children with septic shock. 
The shock index (SI; heart rate/systolic blood pressure) is a widely reported tool to identify acutely ill patients at risk for circulatory collapse in the emergency department (ED). Because old age, diabetes, essential hypertension, and β-/Ca 2+ channel-blockers might reduce the compensatory increase in heart rate and mask blood pressure reductions in shock or pre-shock states, we hypothesized that these factors weaken the association between SI and mortality, reducing the utility of SI to identify patients at risk. Methods This was a cohort study from Odense University Hospital of all fi rst-time visits to the ED between 1995 and 2011 (n = 111,019). The outcome was 30-day mortality. We examined whether age ≥65 years, diabetes, essential hypertension, and use of β-/Ca 2+ channel-blockers modifi ed the association between SI and mortality. The prognostic value of SI ≥1 was evaluated with diagnostic likelihood ratios.
We observed a 30-day mortality of 3%. With SI <0.7 as reference, a SI of 0.7 to 1 was associated with an adjusted OR of 2.9 (CI 2.7 to 3.2) for 30-day mortality while the adjusted OR for SI ≥1 was 10.3 (CI 9.2 to 11.5).
ORs for SI ≥1 were reduced (but still signifi cant) in patients who were older, hypertensive, or on β-/Ca 2+ channel-blockers, whereas diabetes had no eff ect. The OR for SI ≥1 in patients ≥65 years was 8.2 (CI 7.2 to 9.4) compared with 18.9 (CI 15.6 to 23.0) in younger patients. β-/Ca 2+ channel-blocked patients had an OR of 6.4 (CI 4.9 to 8.3) versus 12.3 (CI 11.0 to 13.8) in nonusers, and the OR for hypertensive patients was 8.0 (CI 6.6 to 9.4) versus 12.9 (CI 11.1 to 14.9) in nonhypertensive patients.
The OR for SI ≥1 of 9.3 (CI 6.7 to 12.9) in diabetics did not diff er from the OR of 10.8 (CI 9.6 to 12.0) in nondiabetic patients. A SI of 0.7 to 1 was associated with ORs signifi cantly greater than 1 (range: 2.2 to 3.1) with no evident diff erences within the subgroups. A SI measurement ≥1 was associated with lower positive likelihood ratios in patients ≥65 years, with hypertension, diabetes or using β-/Ca 2+ channel-blockers (range 4.9 to 6.5) compared with patients not exposed to these factors (range 7.6 to 11.6). Introduction Vasodilatory shock is a well-known complication in patients who undergo cardiac surgery with cardiopulmonary bypass (CPB) and its occurrence is associated with higher morbidity and mortality. Despite that, clinical characteristics of vasoplegic shock and its spectrum of severity are poorly described. The aim of this study was to compare patients who developed mild to moderate vasoplegic shock with patients who developed a severe form and to identify predictive factors for the severe form of vasoplegic shock.
Methods We performed an observational study in 300 patients who underwent cardiac surgery with CPB and presented within the fi rst 24 hours after surgery with refractory hypotension and used a vasopressor agent. Severe vasoplegic shock was defi ned as a requirement of norepinephrine higher than 1 μg/kg/minute or the use of two or more vasopressors. Baseline characteristics, laboratorial, clinical and intraoperative data, such as amount of fl uids, bleeding, blood transfusion, inotropes and length of CPB were collected at ICU admission. Logistic regression was performed using severe vasodilatory shock as the outcome. Results There were 46 (15%) patients who develop the severe form of vasodilatory shock within 24 hours after cardiac surgery. In a univariate analysis, patients with the severe form were more likely to be older, to receive more blood transfusion and inotropic agents, to have higher levels of serum lactate, lower hemoglobin concentration and lower SvO 2 at the end of the procedure, lower cardiac output index, higher heart rate and higher levels of reactive C protein at ICU admission. These patients also experienced more postoperative organ dysfunction, had a longer length of ICU stay and higher mortality. There were no diff erences between patients regarding amount of fl uids and length of CPB. In a multivariate analysis we identify age (OR = 1.04, 95% CI = 1.01 to 1.08, P = 0.016), intraoperative use of epinephrine (OR = 5.49, 95% CI = 2.42 to 12.43, P <0.001), higher serum lactate at the end of the procedure (OR = 1.04, 95% CI = 1.01 to 1.06, P = 0.001) and intraoperative blood transfusion (OR = 5.06, 95% CI = 2.19 to 11.69, P <0.001). Conclusion This study demonstrated that older patients, intraoperative blood transfusion and utilization of epinephrine were independently associated with a more severe form of vasodilatory shock after cardiac surgery with CPB. Also, we identifi ed that a higher lactate at the end of the procedure was an independent predictive factor for this severe form of shock. Reference We analyzed patients who used levosimendan compared with those that used dobutamine in the fi rst hours after cardiac surgery, discarding patients in which neither of these two drugs were used or surgical cases that arrived at the ICU with both inotropics. We analyzed demographic variables as well as clinical complications in the ICU and overall perioperative mortality of patients. We performed a second analysis using the propensity score, obtaining the probability of patients being treated with either drug, pairing each patient who received levosimendan with its nearest neighbor receiving dobutamine.
We collected 875 patients: 331 received one of the two drugs, 50 received both drugs and 494 did not receive any drug. ICU mortality was 7.2% (levosimendan group) and 12.5% (dobutamine group), P = 0.1. After adjustment for severity and type of surgery, the use of levosimendan in the postoperative period was not a protective factor for ICU mortality (P = 0.18, OR = 0.5, 95% CI = 0.18 to 1.3). In the matched sample, mortality was 7.4% (levosimendan group) and 5.9% (dobutamine), P = 0.73. After logistic regression adjusted for severity, measured with EuroSCORE and type of surgery, levosimendan was not a protective factor for ICU mortality (P = 0.8, OR = 1.2, 95% CI = 0.26 to 5.45).
Conclusion In our environment, we have observed diff erences in the use of levosimendan compared with dobutamine (higher rate of men undergoing CABG, diabetes and worse EF). After homogenizing the sample of patients by propensity score, an eff ect on mortality is discarded and we observed a signifi cant need for use of norepinephrine and a nonsignifi cant trend for prolonged mechanical ventilation and renal failure requiring renal replacement therapy, both probably related with the greatest need for vasopressors observed.
Introduction Levosimendan was originally developed for the treat ment of decompensated heart failure in situations for which conventional therapy is not suffi cient. It is an eff ective calciumsensitising drug with vasodilatory and inotropic eff ects and improves cardiac contractility. Trials have shown positive outcome benefi t with the use of levosimendan [1] . We reviewed the usage levosimendan at our institution and outcome of these patients.
We reviewed the use of levosimendan at Harefi eld from January 2013 through December 2013. Patient demographics, logistic EuroSCORE ( Figure 1 ), diagnosis, surgical or intervention details, inotropic support, dosage and duration of levosimendan use, length of stay in the ICU, cost (Table 1 ) and patient outcome were collected.
Results Levosimendan was used in 30 patients, 23 (77%) male and seven (23%) female. Median age was 69 (59 to 72.8). Levosimendan was used post cardiac surgery, post angioplasty and in patients with ventricular assist devices (VAD) and extracorporeal membrane oxygenator (ECMO). Most of the patients received a standard regimen of 12.5 mg administered at a dose of 0.1 μg/kg/minute for 24 hours. Concurrent noradrenaline was used in most of the patients ranging from 0.02 to 0.2 μg/kg/minute. The median length of stay in the ICU was nine (6 to 14.5) days for survivors and 23.5 (7.5 to 36) days in nonsurvivors. Sixteen patients (55%) survived and were discharged from the hospital. Introduction Levosimendan is an inotropic seldom used in emergency departments (EDs). It has shown improved mortality in patients with shock [1] with few adverse eff ects [2] . This work denotes the experience of levosimendan use in the ED of Hospital Universitario Mayor between 2012 and 2014.
Methods We present a retrospective study which analyzes the eff ect, after 24 hours, of levosimendan administration on perfusion parameters of 166 ED patients. Patients had to have shock diagnosis of any cause. Diff erences between the initial and fi nal mean value of the following parameters were evaluated: lactate, central venous oxygen saturation (ScvO 2 ) and venoarterial diff erence of CO 2 (DvaCO 2 ). Data were stratifi ed according to levosimendan categories (initial or rescue). In addition, association between diff erent variables with mortality was sought. Diff erences were considered statistically signifi cant at probability levels below 0.05. Results There were no diff erences in APACHE II values between patients who received levosimendan as initial therapy from those who received it as a rescue measure. A total of 41 patients fulfi lled lactate normalization requirements (lactate <2.0 or clearance >50%) ( Table 1) . Forty-four patients reached normal values of SvcO 2 and 37 patients of DavCO 2 after levosimendan initiation. There were no associations between the normalization of lactate, SvO 2 and DvaCO 2 and diff erent types of shock. Twenty-nine patients who received initial therapy with levosimendan normalized their lactate values and 12 who received it as a rescue therapy (P <0.05). Sixty-three patients developed hypotension, and none had adverse eff ects requiring discontinuation of the drug. Hospital mortality was 47.7%. Variables associated with mortality in the study group were lactate value at admission (OR = 1.3, 95% CI = 1.0 to 1.7), the use of vasopressin after start levosimendan (OR = 7.5, 95% CI = 1.9 to 28.6) and the use of norepinephrine before starting (OR = 10.8, 95% CI = 1.9 to 60.7). 
The ECRI Institute has identifi ed alarm fatigue as the number one health technology hazard [1] . A recent study on 461 ICU patients investigated 2,558,760 alarms [2] . In total, 88.8% of the annotated 12,671 arrhythmia alarms were false positives (FPs). It was concluded that the excessive number of alarms is 'a complex interplay of inappropriate user settings, patient conditions, and algorithm defi ciencies' . Nine conditions causing alarms, four of which were ECG algorithm related, were reported [2] . In this study, we investigated a new algorithm in which improvements targeting three of the reported four ECG-related conditions were implemented: low amplitude QRS; wide QRS; nonactionable ventricular tachycardia (VT). Methods The false alarm rate of the new algorithm (GE Carescape, 2012) was compared with that of the algorithm evaluated in the study (GE Solar, 2003) [2] on the collected ECG waveform data. User settings such as QRS detection sensitivity (high/normal) were not available. Therefore, normal sensitivity was assumed for both versions. With the old algorithm, 10 patients with low QRS amplitudes gave a signifi cantly higher number of FPs than were reported [2] . For those patients, both sensitivity modes were tested with the old algorithm. Sixty-six percent of patients with a pacemaker did not have the pacemaker mode selected [2] . Outlier patients in which false alarms were due to user settings (20 patients with a pacemaker) or patient condition (four patients with a bundle branch block) rather than algorithm defi ciency were excluded. Results Improved algorithm resulted in 66% reduction of FP alarms. When using the high-sensitivity mode for the 10 patients with low QRS, FP reduction was 18%. No compromises regarding detection of true events were found. Introduction Cardiac arrhythmias may be observed at any time during the ICU stay. The prognosis may suff er due to these arrhythmias. In our study, we aimed to evaluate incidence and risk factors of arrhythmias occurring in patients in the ICU. Methods Patients treated in the ICU were included in the study if they fulfi lled the following: age >18, no cardiac valvular disease, no cardiac surgery in the recent 6-month period, no history of myocardial infarction (MI), need for mechanical ventilation, and one or more organ failure. Demographic, hemodynamic and laboratory parameters, APACHE II score, presence of sepsis, acute renal failure, MI, and VIP during the ICU stay were recorded. Therapies used for arrhythmia and response to therapies were also recorded. Results Two hundred and fourteen patients were included in the study. Twenty-six percent (n = 56) of patients had arrhythmias. Incidence was higher in females (P = 0.045). Average age of arrhythmic patients was 69 (19 to 86), and they were older than nonarrhythmic patients (P <0.001). APACHE II scores were higher in arrhythmic patients (P = 0.001).
Admission to the ICU with cerebrovascular event (CVE) and trauma was related to arrhythmia (P = 0.021, P = 0.032, respectively). There was a signifi cant relationship between VIP and sepsis presence (P <0.001, P <0.001). Atrial fi brillation was the most frequent type of arrhythmia (53%), and the most frequently used medication was diltiazem (28.5%). The patients who had arrhythmias had a longer ICU stay (P = 0.021). The mortality rate for all patients was 48.1%. There was no statistically signifi cant relationship between arrhythmia and mortality (P >0.05). Conclusion Older age, higher APACHE II scores, trauma, CVE, VIP and sepsis increases arrhythmia risk in critically ill patients. Atrial fi brillation is most common and the most preferred treatment for all arrhythmias is diltiazem.
Introduction Pulmonary hypertension and right ventricular dysfunction (RVD) are frequently encountered in patients with acute heart failure. We sought a better understanding of the coupling between RVD and pulmonary hypertension in the setting of acute decompensated heart failure (ADHF) as it might improve the prognostic stratifi cation and infl uence the survival rates.
Methods Echocardiography was performed in 329 patients with ADHF and right ventricular function was assessed by measuring the right ventricular fractional area, and a right ventricular ejection fraction (RVEF) <35% was taken as the cutoff value for RV systolic dysfunction. The systolic pulmonary pressure (PASP) was calculated from the tricuspid regurgitation signal applying the modifi ed Bernoulli equation, Methods A total of 112 patients undergoing PPCI and MIH were compared with 32 comparable consecutive patients who underwent PPCI but no MIH. We hypothesized that combining both methods lead to better survival rate. MIH was induced (propofol, fentanyl, saline 4 ml/kg BW, 2°C) and maintained for 24 hours, targeting 32 to 34°C. Spontaneous rewarming was allowed (0.5°C). Results There were no signifi cant diff erences between the MIH and Control group in general characteristics, cardiac arrest circumstances and angiographic features. Except for decreases in heart rate during MIH, there was no diff erence between MIH and no MIH groups in arterial pressure, peak lactate (7.7 vs. 6.2 mmol/l; P = 0.36), need for vasopressors (57% vs. 41%; P = 0.09), aortic balloon counterpulsation (13% vs. 22%; P = 0.19), repeat cardioversion/defi brillation (17% vs. 25%; P = 0.30). There was lower incidence of inotropic use (36% vs. 59%; P = 0.01) and use of antiarrhythmics (11% vs. 53%; P = 0.002). There was no diff erence in FiO 2 during mechanical ventilation and in renal function. See Table 1 . Introduction Standard 12-lead electrocardiogram (ECG) is, with biomarkers, the most accurate method in the diagnosis of acute coronary syndrome (ACS). However, posterior (V7-V8-V9) and right (V3R-V4R-V5R) derivations are not systematically performed due to the time-consuming procedure involved, despite major therapeutic implications (fl uid loading instead of nitrates use in right ventricular involvement) and published guidelines [1] . Recently, an 18-lead ECG system, standard 12-lead ECG and six additional synthesized leads (assessing posterior and right ventricular areas) in only one recording procedure has been developed. The reliability of this material (ECG 2550; Nihon Kohden Co. Ltd, Japan) was already validated in this indication in an Asian population [2, 3] . Methods We conducted a prospective, observational study with patients admitted to our emergency department (ED), during a 6-month period. Requirement for ECG was guided by physician's discretion according to patient's history. All patients with chest pain, dyspnea, palpitations, disturbance of consciousness, malaise or abdominal complaint underwent synthesized 18-lead ECG within 10 minutes of ED arrival. The aim of the study was to evaluate the eff ectiveness of the synthesized 18-lead ECG as an ischemia triage tool in the ED, and particularly the ability to early detect a right ventricular involvement.
Of the 3,835 nontraumatic patients treated in the ED, 3, 196 were adults. In this adult population, 500 ECGs were performed in patients whose symptoms suggest ACS. The median age was 62.3 years and the sex ratio was 1.16. Clinical presentation was chest pain (31%), dyspnea (14%), palpitations (5%), disturbance of consciousness (3%) or others (47%). Fifty-six (11.2%) were diagnosed as ACS, including 20 ST-elevation myocardial infarction (STEMI), 28 non-STEMI and eight unstable angina. Of the 20 STEMI patients, eight (40%) and fi ve (25%) were diagnosed as STEMI complicated by right ventricular and posterior wall ischemia respectively, which means that these complications could have been missed by standard 12-lead ECG. Conclusion Eighteen-lead ECG with synthesized right-sided and posterior precordial leads was an effi cient method to diagnose ACS in a Caucasian population within 10 minutes of ED arrival. It is particularly performant to detect right ventricular ischemia early, which can modify acute therapeutic strategy.
Introduction After an acute myocardial infarction with ST-segment elevation (STEMI) treated with percutaneous coronary intervention (PCI), the left ventricle (LV) can undergo negative remodeling (R-). We aimed to investigate whether global longitudinal strain (SGL) of the left ventricle (LV) predicts remodeling. Methods Transthoracic echocardiography with speckle tracking imaging (TTE-STI) was performed 2 to 3 days after primary PCI and 6 months later in patients with diagnosis of STEMI. LV R-criteria were: LVEF increase ≤5% and end-diastolic volume increase ≥15%. Logistic regression and ROC curve analysis was used for the statistical analysis.
Results Eighty-three patients (56 ± 11 years) with STEMI at any LV localization and subjected to primary PCI were studied during 2012: LV (Figure 1 ). Conclusion SGL assessment in the fi rst days after primary PCI is useful in the prediction of LV R-independently of the myocardial infarction localization. Vasopressor management (noradrenaline dose) (P = 0.0001), fl uid balance (P <0.001) and E/E' (P = 0.00004) were independent predictors of plasma BNP concentration.
Conclusion Diastolic dysfunction as evaluated by E/E' and E' constitutes a major independent predictor of outcome in septic shock, compared with cardiac biomarkers, suggesting that echocardiographic techniques assessing diastolic dysfunction in sepsis may replace cardiac biomarkers for mortality prediction. Fluid balance, vasopressor management and diastolic dysfunction are independent predictors of BNP elevation in septic shock. Our fi ndings should be confi rmed by an extended prospective study.
Introduction Depending upon the medication administered, accidental administration of medication into the arterial line can cause devastating complications. This wrong-route injection is a never event in the UK but may be under-reported especially when occurring in the unconscious patient who may not notice associated pain temporally. Under-reporting may occur because resultant complications may be delayed a number of hours and the accountable healthcare worker may not recognise or choose not to report the error. In 2008 the UK National Patient Safety Agency (NPSA) reported only 76 incidents related to poor sampling technique but few wrong route arterial injections. Of these 21% suff ered moderate to severe harm [1] . The NPSA suggests that training and the use of clear labelling alongside red arterial tubing and standard red lock caps be used to prevent arterial sampling errors.
Methods In 2014, we conducted a national postal survey of ICUs in the UK to attempt to determine the rate of accidental intra-arterial injections. The survey was sent to the clinical director of every ICU and they were asked whether they were aware of any unintentional arterial line injection having occurred in their hospital in the last 5 years.
Of the 56 ICUs that responded, 16 (28.5%) reported that they had personally seen an accidental injection into the arterial line. Conclusion Despite the arterial line safety recommendations made by the NPSA in 2008, we demonstrate that intra-arterial injection is still a problem and that it remains under-reported. Our incidence is likely to be an underestimate as it relies on the recollections of a single individual in each institution. Medical errors can be mitigated by consideration of human factors and system engineering to improve patient safety. A focus on clinical awareness, colour coding and training may lead to improvements; however, institutions and clinical directors also bear a responsibility to prevent never events and a number of engineered solutions are now available such as needle-free non-injectable arterial sampling devices to protect the healthcare environment and make this error impossible [2, 3] . Introduction Use of ice-cold saline is assumed to provide best accuracy of TPTD to obtain the cardiac index (CI), global end-diastolic volume (GEDVI) and extravascular lung-water (EVLWI). However, roomtemperature injectate might facilitate TPTD outside the ICU. A recent study [1] showed acceptable bias and percentage error (PE) for CI-room derived from TPTD with 15 ml room temperature saline compared with CI-cold using 15 ml iced saline for TPTD. However, GEDVI-room and EVLWI-room had borderline PE values close to 30%, and the bias of GEDVI-room markedly increased with higher values of GEDVI and in case of femoral CVC. Since imprecision of TPTD-room might be reduced by a larger volume of injectate, it was the aim of our study to compare CI, GEDVI and EVLWI derived from TPDT using 20 ml room temperature injectate with standard TPTD with 15 ml iced saline. Introduction There are few methods of cardiac output (CO) estimation validated in children. The aim of this study is to investigate the reliability of an uncalibrated pulse contour method of CO estimation, the pressure recording analytical method (PRAM), in pediatric patients scheduled for diagnostic right and left heart catheterization, compared with the oxygen-direct Fick method. Methods Cardiac index (CI) was simultaneously estimated by Fick, and PRAM applied to pressure signals recorded invasively from a femoral catheter. All measurements were performed in steady-state condition. PRAM CI measurements were obtained for 10 consecutive beats simultaneously during the Fick CI estimation. Agreement between Fick and PRAM was assessed using the Bland-Altman method. Correlation coeffi cient, bias, and percentage of error were calculated. Results Forty-three CI measurements were performed in 43 patients. The data showed good agreement between CIFick and CIPRAM: r 2 = 0.98; bias -0.0074 l/minute/m 2 ; limits of agreement from -0.22 to 0.22 l/minute/m 2 . The percentage error was 8%. Figure 1 shows the Bland-Altman plot. Introduction Although recognized as a questionable indicator of the intravascular volume, central venous pressure (CVP) is integrated in many therapeutic algorithms for hemodynamic resuscitation of critically ill patients [1] . In an attempt to simplify CVP estimation, several clinical and ultrasonographic approaches have been suggested [2] [3] [4] [5] .
Nonetheless, the external jugular vein (EJV) circumference and area have not been evaluated. Considering the role of EJV visual assessment in the clinical estimation of CVP, we hypothesized that EJV ultrasound evaluation could be used to reliably estimate CVP. Methods Patients with a CVC placed as part of clinical management were evaluated. EJV and internal jugular vein (IJV) measurements were performed at the left cricoid level. IJV and EJV were visualized in short axis view; diameters, circumferences and areas were obtained at end expiration with simultaneous CVP measurement. Measures were performed by a single trained operator, who was blind to CVP values. Results Forty-eight patients were included. A poor correlation was found between CVP and IJV and EJV circumference and area in mechanically ventilated patients. A strong correlation was found between CVP and EJV circumference (r: 0.74; P = 0.0004; 95% CI: 0.421 to 0.897) and area (r: 0.702; P = 0.0012; 95% CI: 0.35 to 0.88) in spontaneously breathing patients. Conventional receiver-operating characteristic curves were generated to assess the utility of EJV circumference and area to predict low (≤8 mmHg) versus high (>8 mmHg) CVP values. AUC for EJV circumference and area was 0.935 (P <0.0001; 95% CI: 0.714 to 0.997) and 0.87 (P <0.0001; 95% CI: 0.63 to 0.98) respectively ( Figure 1 ). Conclusion These results highlight a potentially evolving role of EJV circumference and area in the hemodynamic management Critical Care 2015, Volume 19 Suppl 1 http://ccforum.com/supplements/19/S1 S59 of spontaneously breathing patients. An important aspect of the suggested approach is its simplicity, requiring basic technical skills and making it suitable in any scenario where an ultrasound machine is available.
Introduction Hypovolaemia is generally believed to induce centrali sation of blood volume. Therefore, we evaluated whether hypovolaemia and hypervolaemia result in a change in central blood volume (that is, pulmonary blood volume (PBV)) and we explored the eff ects on the distribution between PBV and circulating blood volume (Vd circ).
Committee approval, blood volume was altered in both directions randomly in steps of 150 ml (mild) to 450 ml (moderate) either by haemorrhage, retransfusion of blood, or infusion of colloids in six Foxhound dogs. The anaesthetised dogs were allowed to breathe spontaneously. Blood volumes were measured using the dye dilution technique: PBV was measured as the volume of blood between the pulmonary and aortic valve, and Vd circ by two-compartmental curve fi tting [1, 2] . The PBV/Vd circ ratio was used as a measure of blood volume distribution. A linear mixed model was used for analysing the infl uence of blood volume alterations on the measured haemodynamic variables and blood volumes. Results A total of 68 alterations in blood volume resulted in changes in Vd circ ranging from -33 to +31% ( Figure 1 ). PBV decreased during mild and moderate haemorrhage, while during retransfusion PBV increased during moderate hypervolaemia only. The PBV/Vd circ ratio remained constant during all stages of hypovolaemia and hypervolaemia ( Figure 1 ).
Mild to moderate alterations of blood volume result in changes of PBV and Vd circ. However, against the traditional belief of centralisation we could show that the cardiovascular system preserves the distribution of blood between central and circulating blood volume in anaesthetised dogs. References Introduction Permissive hypotension, which results in avoidance of intravascular overpressure and thereby avoidance of platelet plug dislodgement early in the clotting mechanism, improves the results after trauma and hemorrhage. The research hypothesis is that augmentation of negative intrathoracic pressure with the use of an impedance threshold device (ITD) will improve hemodynamic parameters, without aff ecting permissive hypotension or causing hemodilution. On the other hand, aggressive resuscitation with Ringer lactate will cause hemodilution and intravascular pressures that are very high for permissive hypotension, capable of platelet plug dislodgement.
Methods Twenty anesthetized Landrace/Large-White pigs (19 ± 2 kg, 10 to 15 weeks) were subjected to a fi xed hemorrhage (50% over 30 minutes). The pigs were randomly allocated into two groups (n = 10 per group). In group A, ITD was the only treatment for hypotension, while in group B, an intravenous administration of 1 l Ringer lactate was applied for treatment of hypotension. Hemodynamic parameters were continuously assessed for the fi rst 30 minutes after blood loss. Results Mean systolic arterial pressures (SAPs) 30 minutes after the intervention in each group were as follows: group A 80 ± 5 mmHg and group B 90 ± 4 mmHg. Maximum SAPs during the assessment period were: group A 89 ± 2 mmHg and group B 128 ± 5 mmHg. Mean pulse pressure was higher in the ITD group versus the fl uid resuscitation group (P <0.05). After the assessment period, mean hematocrit in group A was 24 ± 2%, while in group B it was 18 ± 1% (P <0.001). Conclusion In our study, the ITD increased SAP and pulse pressure without overcompensation. On the other hand, aggressive fl uid resuscitation led to a signifi cant increase of SAP >100 mmHg capable of clot dislodgement and in addition led to hemodilution. Introduction Measurement of global end-diastolic volume (GEDV) is provided by cardiovascular monitoring devices using thermodilution procedures. The aim of this study was to assess the relation between this clinically available index and left ventricular end-diastolic volume (LVEDV), which is typically not available at the patient bedside. Methods Measurements were performed on six anaesthetised and mechanically ventilated pigs. Volume loading via successive infusions of saline solution was fi rst performed and was followed by dobutamine infusion. These two procedures provided a wide range of LVEDV values. During these experiments, GEDV was intermittently measured using the PiCCO monitor (Pulsion AG, Germany) during thermodilutions and LVEDV was continuously measured using an admittance catheter (Transonic, NY, USA) inserted in the left ventricle.
Results Table 1 presents the linear correlations obtained between LVEDV and GEDV. These correlations are good to excellent, with r 2 values from 0.59 to 0.85. However, the coeffi cients of the linear regressions present a large intersubject variability, which prevents the precise estimation of LVEDV using GEDV. Nevertheless, variations in LVEDV are well reproduced by the GEDV index. The variations in LVEDV actually equal 21 to 48% of those in GEDV. The coeffi cient b is always nonzero, indicating that some proportion of the GEDV index is actually not linked to LVEDV. 
The results show that GEDV and LVEDV are generally well correlated, but the correlation coeffi cients are subject specifi c. A preliminary calibration step (for instance using echocardiography) is thus necessary to infer LVEDV from GEDV. 
We performed an evaluation of three devices used for assessment of volume status in critically ill patients in our institution: transthoracic echocardiography (TTE) (CX50; Philips Ultrasound), bio reactance (NICOM; Cheetah Medical) and pulse contour-based thermodilution (PiCCO; Pulsion Medical). Methods Ten mechanically ventilated critically ill patients with PiCCO monitoring in situ and a good quality of images on transthoracic view were included. All study measurements were made in triplicate. A single trained cardiologist, blinded to the results from the other monitors, performed the TTE study. Diff erences among the three methods were assessed for signifi cance using one-way ANOVA, Spearman's coeffi cient and Bland-Altman analysis. All statistical analyses were performed using Graph-pad Prism 5 and P <0.05 was taken as signifi cant.
Results Ninety measurements were obtained. NICOM and TTE-derived stroke volume appeared well matched but PICCO-derived values showed signifi cant variation (F = 2.4, P = 0.09). There was no correlation between TTE velocity time integer (VTI) and NICOM stroke volume variation (SVV) (r = 0.24, P = 0.20; Figure 1A ) but a good correlation and small bias between TTE-VTI and PiCCO-SVV (r = 0.76, P <0.0001; Figure 1B ). Applying the following indications for volume expansion (PiCCO and NICOM SVV >15% and TTE VTI variability >15%) we found an agreement in 71% of cases between TTE and PiCCO and in 42% of cases between echocardiography and NICOM. Conclusion Stroke volume produced by bioreactance appeared to be comparable with that measured by echocardiography but not with PiCCO. There was a good agreement between decision-making as regards fl uid administration between PiCCO and echocardiography. NICOM appeared unreliable in this setting.
Bioreactance-based passive leg raising test can predict fl uid responsiveness in patients with sepsis C Hu 1 , H Tong 1 , G Cai 1 , J Teboul 2 , J Yan 1 , X Lv 1 , Q Xu 1 , J Chen 1 Introduction Fluid administration is always important and diffi cult during the therapy of patients with sepsis. Accurately predicting fl uid responsiveness and thus estimating whether the patient will benefi t from fl uid therapy seems particularly important. The present study intended to predict fl uid responsiveness in patients with sepsis using a bioreactance-based passive leg raising test, and to compare this approach with the commonly used central venous pressure (CVP) approach.
Methods This prospective, single-center study included 80 patients with sepsis from the Department of Critical Care Medicine of Zhejiang Hospital. Patients were randomly assigned to either Group A or Group B, with patients of in Group A fi rst taking the passive leg raising test and then taking the fl uid infusion test, while patients in Group B followed the opposite protocol. NICOM was used to continuously record hemodynamic parameters such as cardiac output (CO), heart rate (HR) and central venous pressure (CVP), at baseline1, PLR, baseline2, and volume expansion (VE). Fluid responsiveness was defi ned as the change in CO (ΔCO) ≥10% after VE. Results CO increased during PLR (from 5.21 ± 2.34 to 6.03 ± 2.73 l/ minute, P <0.05); and after VE (from 5.09 ± 1.99 to 5.60 ± 2.11 l/minute, P <0.05). The PLR-induced change in CO (ΔCOPLR) and the VE-induced change in CO (ΔCOVE) were highly correlated (r = 0.80 (0.64 to 0.90)), while the CVP and ΔCOVE were uncorrelated (r = 0.12 (-0.16 to 0.32)).
The areas under the ROC curves of ΔCOPLR and ΔCVP for predicting fl uid responsiveness were 0.868 and 0.514 respectively. ΔCOPLR ≥10% was found to predict fl uid responsiveness with a sensitivity of 86% and a specifi city of 79%. Conclusion Bioreactance-based PLR could predict fl uid responsiveness in patients with sepsis, while CVP could not.
The detection of heart response to fl uid administration is still a challenge in clinical practice. Changes in metabolic parameters may be useful to detect changes in cardiac output (CO) after fl uid expansion.
Methods This is a prospective observational study in adult critically ill patients. CO was measured either by echocardiography or by a thermodilution method (PiCCO, Swan-Ganz catheter). Hemodynamic measurements and blood gas analysis were obtained before and after a fl uid challenge with either 1,000 ml crystalloid or 500 ml colloid. Arterial and central venous blood gas samples were taken simultaneously. Oxygen delivery (DO 2 ), oxygen consumption (VO 2 ) and carbon dioxide production (VCO 2 ) were calculated according to well-known formulas. Patients were divided into three groups (high responders, mild responders and nonresponders) according to their change in CO (>20%, 10 to 20%, <10%, respectively).
Results We evaluated 27 patients, age 68 (95% CI: 61 to 74) and APACHE II score 22 (95% CI: 18 to 26). Seven patients were high responders, eight patients were moderate responders and 12 were nonresponders. DO 2 was signifi cantly increased in high responders (37 ± 35%, P <0.01) as compared with moderate responders or nonresponders. Furthermore, nonresponders had a decrease in their DO 2 (-10 ± 7%, P <0.01), while moderate responders showed no change in their DO 2 (1.6% ± 10, P = 0.73) after fl uid challenge. We found no diff erences in changes in lactate levels and central venous oxygen saturation (ScvO 2 ) between high responders, moderate responders and nonresponders. No diff erences in the changes of VCO 2 or VO 2 /VCO 2 ratio were found between high responders, mild responders and nonresponders too. Changes in DO 2 /VCO 2 ratio were found to be signifi cantly increased only in high responders (47 ± 73% vs. -14 ± 31%, P = 0.02) and not in mild responders (15 ± 54% vs. -14 ± 31%, P = 0.15) as compared with nonresponders.
Conclusion Only signifi cant increases of CO (>20%), after fl uid administration, lead to improved oxygen delivery; DO 2 may be decreased in nonresponders. The changes of ScvO 2 and lactate levels do not track the changes of CO after fl uid challenge. The DO 2 /VCO 2 ratio may be a useful index to identify signifi cant increases of CO after fl uid challenge in cases where CO measurement is not feasible. Introduction The aim of the study was to assess the value of the global end-diastolic volume (GEDV) evaluated by transpulmonary thermodilution as an indicator of cardiac preload comparing with stroke volume variation (SVV) in patients with septic shock. Methods A prospective, observational study performed in an interdisciplinary ICU including 91 patients with septic shock. Hemodynamic monitoring was performed with a new calibrated pulse wave analysis method (VolumeView/EV1000; Edwards Lifesciences) in 37 patients (group EV1000) or with an uncalibrated method (FloTrac/ Vigileo; Edwards Lifesciences) in 54 patients (group Vigileo) during the fi rst 72 hours. All patients were receiving mechanical ventilation and vasopressors. Measurements were performed before and immediately after volume loading using 500 ml Ringer solution over a short period (<30 minutes). Results A total of 211 fl uid challenges were studied in 91 patients. We observed a signifi cant relationship between the GEDV index before volume loading and the percentage increase in GEDV index in the EV1000 group and changes in GEDV index were signifi cantly correlated with changes in stroke volume index (r = 0.75, P <0.001), but an insignifi cant relationship between SVV variation and cardiac index variation (P >0.05) in the Vigileo group.
The transpulmonary thermodilution GEDV is a better indicator of cardiac preload than SVV in patients with septic shock.
Introduction Dynamic parameters are becoming standard for fl uid responsiveness assessment. Cutoff values are diff erent in the literature.
The aim was to assess the accuracy of diff erent preload parameters to predict fl uid responsiveness using pulse power analysis and to compare diff erent levels of hemodynamic response due to passive leg raising (PLR) against the eff ect of a fl uid challenge (FC). Methods A prospective study in a 17-bed mixed ICU. Patients were fully ventilated and CO monitored with LiDCOplus® and underwent a FC due to hypotension and/or hypoperfusion and preload dependence (SVV and/or PPV >10%). PLR was performed before FC. Hemodynamic data were recorded prePLR, postPLR and postFC with 0.5 l crystalloids. We compared diff erent cutoff values of increase in CO and SV (10 to 15%) to assess the ability of PLR, SVV, PPV and CVP to predict the response to FC. Statistical analysis: continuous variables expressed as mean ± SD. Comparison before and after was done using a paired Student's t test, and receiver operating characteristic (ROC) curves were generated by varying the discriminating threshold of each variable. Results Thirty-one patients were included. Baseline parameters: MAP 70.5 mmHg (SD 13.3) 87% under catecholamine, SV 55.32 ml (SD 20.2), CO 5.2 l (SD 2), SVV 16.8% (SD 12), PPV 19.1% (SD 14), HR 96 bpm (SD 18) and CVP 9.2 mmHg (SD 2.5). In total, 41.9% of patients increased 15% CO after FC (selected as responders), 38.7% after the PLR. Diff erences in responders versus nonresponder patients were: baseline SVV (23.9 vs. 11.6; P = 0.02) and PPV (28.4 vs. 12.4; P = 0.01). Diff erences in SV and CO were not statistically signifi cant. The best parameter to predict positive response to FC was PLR with cutoff 12.6% for CO increase: sensitivity 84.6% (95% CI = 65 to 104), specifi city 94.4% (95% CI = 84 to 105) and AU ROC 0.94 (95% CI = 0.86 to 1.0). ROC was also good for SVV 0.835 (95% CI = 0.66 to 1.0; P = 0.002) and PPV 0.833 (95% CI = 0.681 to 0.985; P = 0.002) in this cutoff value. In SV increase, PLR, SVV and PPV had P <0.05, but with worse ROC. In addition, SVV <13% identifi ed patients who will not increase MAP with FC: sensitivity 91.7% (95% CI = 76 to 107.3%), negative predictive value 93.5 (95% CI = 80.7 to 106). CVP failed to distinguish responders from nonresponders. Conclusion Our results support the idea that a reversible FC (PLR; CO cutoff 12.6%) is best at identifying responder patients to a FC. Dynamic parameters (SVV/PPV) are also eff ective when appropriate. Beat-tobeat SV and CO using pulse power analysis is a valid tool for these tests. Introduction Prediction of fl uid responsiveness is defi ned by an increase in stroke volume (SV) of at least 10% after volume expansion. Dynamic [1] and static [2] esophageal Doppler (OD) parameters have been proposed in mechanically ventilated children to guide fl uid therapy. This study aimed to compare dynamic parameters using the respiratory variation in aortic blood fl ow with static parameters using Doppler corrected fl ow times (FTc) obtained by OD. Methods A prospective, observational and interventional study was conducted in our pediatric ICU from March 2012 to September 2014. We investigated 18 mechanically ventilated children with acute circulatory failure (ACF) -tachycardia, hypotension, oliguria, delayed capillary refi lling or hemodynamic instability despite vasopressor drugs -using OD for each patient. Intervention: standardized volume expansion (VE).
The VE-induced increase in stroke volume was ≥10% in 14 patients (responders) and <10% in four patients (nonresponders). Before VE, the DELTA Vpeak ao in responders was higher than in nonresponders (19.5% (12 to 29) vs. 11.5% (7 to 13)), whereas FTc was lower in responders than in nonresponders (262.5 milliseconds (180 to 340) vs. 285 milliseconds (205 to 300)). The prediction of fl uid responsiveness was higher with DELTA Vpeak ao (ROC curve area 0.964 (95% CI = 0.756 to 1.000); P = 0.0001) than with FTc (ROC curve area 0.562 (95% CI = 0.314 to 0.790); P = 0.7203). The best cutoff value for DELTA Vpeak ao was 13% with sensitivity and specifi city predictive values of 85.7% and 100%, respectively; and the best cutoff value for FTc was 265 milliseconds with sensitivity and specifi city predictive values of 57.1% and 75%, respectively. Conclusion In our study, DELTA Vpeak was the most appropriate variable to predict fl uid responsiveness by OD in ventilated children with ACF.
The pleth variability index (PVI) is a new dynamic index obtained by automatic estimation of respiratory variations in the pulse oximeter waveform amplitude. This noninvasive and continuous hemodynamic monitoring has been recently proposed in mechanically ventilated patients to guide fl uid therapy. We recently acquired a PVI monitor in 2014. PVI is calculated by measuring changes in perfusion index (PI) during the respiratory cycle as follows: PVI = ((PImax -Pimin) / PImax) × 100. This study aimed to investigate whether PVI at baseline can predict fl uid responsiveness. Methods In our pediatric ICU we started a prospective and observational study. Between January and November 2014, nine mechanically ventilated children were investigated using PVI and transthoracic echocardiography for each patient with acute circulatory failure (ACF): tachycardia, hypotension, oliguria, delayed capillary refi lling or hemodynamic instability despite vasopressor drugs. Intervention: standardized volume expansion.
Results Signifi cant changes in stroke volume were observed after volume loading (VL) ≥10% in eight patients (responders (R)) and <10% in one patient (nonresponder (NR)). Before VL, PVI was signifi cantly higher in R than NR at baseline ((19.75 ± 3.15%) vs. (9% ± 0.00%), P <0.0001), and decreased signifi cantly in R from baseline to after VL ((19.75% ± 3.15) vs. (12.5% ± 2.828), P <0.0001). Conclusion In this study, PVI seems to predict fl uid responsiveness in ventilated children with ACF.
The accuracy of predicting fl uid responsiveness (FR) using IVC collapsibility is high in patients on controlled mechanical ventilation, but remains unknown in spontaneously breathing patients with mechanical ventilation. Also, adequate ultrasound images of IVC are diffi cult to obtain in a substantial number of patients. The aim of the current study is to evaluate utility of collapsibility of jugular veins (IJV) and subclavian veins (SCV) in comparison with collapsibility of IVC in patients on pressure support ventilation. Methods Patients on pressure support ventilation were prospectively included when fl uid challenges were clinically indicated. Bilateral IJV were examined at the level of cricoid cartilage. Bilateral SCV were measured where the veins crossed the clavicle. Anteroposterior diameter, cross-sectional area (CSA) of IJV and SCV were measured using frame by frame analysis. IVC was measured 2 cm from the right atrial border in a long axis view. Fluid responsiveness was defi ned as 8% increase of stroke volume calculated by the Vigileo monitor (Vigileo, FloTrac; Edwards Lifesciences) after passive leg raising (started from supine position). Receiver operating characteristic (ROC) curves were generated using EZR. Introduction Preventable postsurgical complications are increasingly recognized as a major healthcare burden. A recent meta-analysis showed a 17 to 29% decrease in complications after major surgery with perioperative goal-directed therapy (PGDT) [1] . We assessed the fi nancial consequences of postsurgical complications in a large population from 541 US hospitals in order to predict potential savings with PGDT. Methods Data from adults who had any one of 10 major noncardiac surgical procedures between January 2011 and June 2013 were selected from the Premier research database. Twenty-six postsurgical complications were tabulated. Hospital costs, length of stay, and readmission rates were compared in patients with and without complications. Risk ratios reported by Pearse's meta-analysis were used to estimate the expected reduction in postsurgical morbidity with PGDT. Potential cost-savings were calculated from the actual and anticipated morbidity rates using the mean diff erence in total costs. Introduction This study attempts to determine a vascular pedicle width (VPW) cutoff value that identifi es a fl uid replete state defi ned as an IVC diameter ≥2 cm and ≤15% respiratory variation. Methods In a cross-sectional design, consecutive, critically ill patients underwent simultaneous chest radiographs and ultrasounds. The Research Ethics Committee approved the study.
Results Eighty-four data points on 43 patients were collected. VPW correlated with IVC diameter (r = 0.64, P ≤0.001) and IVC variation (r = -0.55, P ≤0.001). No correlation was observed between VPW and number of lung comets (r = 0.12, P = 0.26) or positive fl uid balance (r = 0.3, P = 0.058). On multivariate linear regression, standardized coeffi cients demonstrated that a 1 mm increase in IVC diameter corresponded to a 0.28 mm (Beta) increase in VPW. ROC curve analysis yielded an AUC of 0.843 (95% CI = 0.75 to 0.93), P ≤0.001 and provided the best accuracy with a cutoff VPW value of 64 mm (sensitivity 81%, specifi city 78%, PPV = 88.5%, NPV = 66%, correct classifi cation rate = 79.6%). See Figure 1 . The objective of this study was to determine the minimum volume of intravenous fl uid required to signifi cantly increase the Pmsf. Methods Patients following cardiac surgery were randomly allocated to receive 1, 2, 3 or 4 ml/kg (body weight) of crystalloid over 5 minutes using a 60 ml syringe. Pmsf was measured using the arterial pressure after stopping blood fl ow in the arm with a pneumatic tourniquet infl ated for 1 minute. Cardiac output (CO) was also recorded at baseline and immediately after the fl uid infusion. CO was measured with LiDCO or pulmonary artery catheter, and a positive response was considered an increase of 10% from baseline. From previous data, the least signifi cant change for Pmsf was 15%. Medians were compared using the independent samples media test, and proportions were compared using a chi-square test. Statistical signifi cance was considered when P <0.05. Results Fifty patients were included, 40.8% of them were responders. The proportion of responders increases with the increase of dose of fl uids ( Table 1 ). The regression equation was: change of Pmsf (%) = 4.4 (dose of fl uids ml/kg, 95% CI 2.3 to 6.5) -1.6 (95% CI 7.4 to 4.3, R 2 = 0.28, F(1.47) = 17.8, P <0.001). The predicted dose of fl uids required to achieve a change in Pmsf of 15% is 3.7 ml/kg crystalloids. 
The minimum volume required to perform an eff ective fl uid challenge is 4 ml/kg infused in 5 minutes. However, only 30% of the variation of change in Pmsf can be explained by the dose of i.v. fl uid given. The proportion of responders increases with the volume of fl uids. Critical Care 2015, Volume 19 Suppl 1 http://ccforum.com/supplements/19/S1 injury (AKI) develops, positive fl uid balance has been described as a risk factor for overall mortality and delayed renal recovery. We hypothesized that fl uid overload may be an independent risk factor for AKI in the critically ill. Methods In a cross-sectional design, we collected data on consecutive, critically ill, adult patients admitted over a 5-month period to the medical and surgical ICUs of a single center. AKI was defi ned according to the RIFLE Classifi cation. Logistic regression analysis was performed to determine the predictive ability of variables for AKI. Conclusion Positive fl uid balance after cardiac surgery is an independent risk factor for mortality and for acute kidney injury in patients presenting vasoplegic shock. Conclusion Postsurgical complications have a signifi cant impact on hospital margins. Enhanced Recovery Programs have the potential not only to improve quality of care but also to improve hospital margins. References were categorized into clinical meaningful categories: 0 to 2 mmol/l (normal), 2 to 4 mmol/l (elevated) and ≥4 mmol/l (high) to allow for nonlinear eff ects. The unadjusted association between lactate group and length of stay was assessed with the Kruskal-Wallis test and post hoc Wilcoxon rank-sum tests. To assess the association between postoperative lactate levels and hospital length of stay we performed multivariable Poisson regression with robust variance estimates. We adjusted for more than 30 variables including patient demographics, comorbidities, cardiac characteristics (for example, New York Heart Association class and ejection fraction), and surgical characteristics (for example, year, status (elective, urgent, emergent), type of procedure, perfusion time, and cross clamp time).
We included a total of 1,267 patients. The median age was 68 (quartiles: 59, 76), 32% were female, 68% underwent coronary artery bypass grafting and 59% underwent valve surgery. Median length of hospital stay was 6 days (quartiles: 5, 9) . Median length of stay in the normal, elevated and high lactate groups were 5 days (quartiles: 4, 7), 6 days (quartiles: 5, 9) and 9 days (quartiles: 6, 17), P <0.001 for comparison. In multivariable analysis, patients with an elevated lactate had a 1.12 times (95% CI: 1.02 to 1.23, P = 0.02) longer length of stay compared with those with normal lactate. Patients with a high lactate had a 1.30 times (95% CI: 1.10 to 1.53, P = 0.002) longer length of stay compared with those with normal lactate. Conclusion Postoperative lactate levels are associated with increased length of hospital stay in patients undergoing major cardiac surgery.
Interventions aimed at decreasing postoperative lactate levels may decrease hospital length of stay. 
The potential benefi ts of a protocol of intensive alveolar recruitment may be outweighed by its detrimental eff ects in hemodynamic stability after cardiac surgery. The aim of this study was to analyze the hemodynamic behavior of patients included in a trial of intensive alveolar recruitment after cardiac surgery. Methods In this randomized trial, we assigned adult patients with PaO 2 / FIO 2 <250 at a PEEP of 5 cmH 2 O to either intensive alveolar recruitment or a standard protocol, both using low-tidal volume ventilation (6 ml/ kg/ibw) after adequate volemia status. Our hypothesis was that an intensive alveolar recruitment protocol with controlled pressure of 15 cmH 2 O and PEEP of 30 cmH 2 O during 1 minute, repeated three times at 1-minute intervals between each maneuver, would not cause hemodynamic instability. Results In total, 163 patients were included in the standard and 157 in the intensive group. Patients of the intensive group had a signifi cant reduction of the MAP at T1, T2 and T3 (1 hour, 2 hours and 3 hours of the protocol), returning to baseline after T4 ( Figure 1 ). No patients had severe hypotension (MAP <65 mmHg) and the study was not stopped in any case. The length of the hospital stay was shorter among patients in the intensive group (10.9 (9.9 to 11.9) vs. 12.4 days (11.3 to 13.6); P = 0.045). Conclusion An intensive alveolar recruitment protocol did not result in hemodynamic instability in hypoxemic patients after cardiac surgery (NCT01502332). Introduction National University Hospital, Singapore, recently formed a Division of Critical Care -Respiratory Therapy. This service rapidly expanded to provide 24/7 Respiratory Therapy Services in the cardiothoracic intensive care unit (CTICU). One goal of service expansion was a reduction in duration of mechanical ventilation after cardiac surgery. We hypothesized that introduction of a teambased extubation protocol would reduce the duration of mechanical ventilation and ultimately aff ect ICU length of stay. Methods A multidisciplinary group created a team-based extubation protocol. The protocol was applied to all elective postoperative cardiac surgery patients. To assess the protocol's impact, data were collected in a registry 3 months before and 3 months after protocol initiation. Data collection included cardiopulmonary bypass time, McCormack airway assessment, ICU admission time, initial pH, lactate, inotropes upon arrival at the CTICU, blood gas analysis prior to extubation, time of extubation and length of stay. Patients were excluded from data analysis if they experienced events which contraindicated application of the protocol, such as signifi cant intraoperative or postoperative complications. These events were explicitly stated in the extubation protocol. Singapore's Domain-specifi c review board granted waiver of patient consent to analyze and present these data. Results A total of 201 patients undergoing elective open cardiac surgery were included; 99 patients before protocol implementation (pre-protocol) and 102 patients after implementation (post-protocol). There was no signifi cant diff erence in mean age (60 vs. 61 P = 0.823), gender (79.8% vs. 79.4% P = 1.00), EuroSCORE (26 vs. 32 P = 0.576) and proportion receiving bypass surgery (72% vs. 80% P = 0.206) or valve surgery (21% vs. 19% P = 0.722) between the two groups. Median extubation time was reduced by 3.5 hours (620 minutes vs. 408 minutes P <0.001). ICU length of stay was also reduced following introduction of the pre-protocol 48 hours versus 24 hours post protocol (P <0.05). Conclusion A team-based extubation protocol signifi cantly reduced the duration of mechanical ventilation and this translated to reduced ICU length of stay in patients undergoing elective open-heart surgery. Introduction Frailty is defi ned as a multidimensional syndrome involving loss of physical and cognitive reserve leading to greater vulnerability to adverse events [1] . Such events include susceptibility to unplanned hospital admissions, and death [1] [2] [3] . Frailty is associated with increased ICU and 6-month mortality, and reduced quality of life [4] . The aim of this study is to investigate the impact of baseline frailty on postoperative quality of life indicators and postoperative frailty following cardiothoracic surgery. Methods Adult patients undergoing cardiac surgery or thoracic surgery (involving thoracotomy) were included in this study. Baseline measures of frailty [4] and performance status were prospectively recorded using validated tools. Informed consent was obtained prior to inclusion. Outcome measures of APACHE II scores, duration of ventilation, length of ICU stay and mortality were recorded. Follow-up at 6 months was conducted by telephone to assess recovery patterns. Results A total of 120 patients were included in this study, including 100 patients who underwent cardiac surgery and 20 patients who underwent thoracic surgery. Eighty-fi ve patients (70.8%) were male. The mean age was 65.4 years (range 25 to 89 years). The mean baseline frailty score also varied widely within our cohort. Four patients died in the ICU following their surgery (3% ICU mortality rate). Mean length of ICU stay was 2.7 days (range 0 to 20 days), with a mean duration of ventilation of 20 hours (range 0 to 264 hours). Follow-up of these patients at 6 months following their surgery is currently underway. Conclusion Owing to advances in life expectancy, health and perioperative medicine, it has become more diffi cult to determine fi tness for major surgery. Our data suggest that frailty may be a useful prognostic measure to help inform such decisions. References The probability of placing IABP in the preoperative period has been calculated, making a propensity analysis to obtain two homogeneous groups treated with or without the IABP, based on personal history, functional status and type of surgery. Seventy-seven patients with preoperative IABP were matched with 77 patients without BCIAO with the nearest propensity score. We used the chi-square test or Student t test as needed and binary logistic regression for multivariate analysis so we can rule out possible confounding variables. We used the statistical package R v2.12 for MAC. Results A total of 8,026 were recorded, in 77 of them an IABP was inserted before the surgery. We performed a propensity score analysis by pairing 72 patients with and without BCIAO based on epidemiological factors and type of surgery. In the analysis of all-cause 30-day mortality, 27% of patients in whom IABP was inserted prior surgery died versus 13.1% of patients without IABP preoperative implantation (P = 0.043). A combined endpoint that included need for prolonged mechanical ventilation over 24 hours or reoperation or mediastinitis or stroke after surgery or 30-day mortality was performed and occurred in 58.3% of patients with preoperative IABP versus 41.7% without it (P = 0.046). When stratifi ed by preoperative risk (analyzed with EuroSCORE), no diff erence between groups was observed (P = 0.62, OR 0.75 (0.23 to 2.35)) for mortality rate and (P = 0.11, OR 0.47 (0.19 to 1.18)) for the combined endpoint. The patients with preoperative IABP implantation had a higher ICU length of stay (10.6 ± 7.7 vs. 4.6 ± 6.7, P = 0.046) with no diff erences in terms of overall hospital stay (21.8 ± 18.7 vs. 18.9 ± 22.08, NS). Conclusion The use of IABP prior to cardiac surgery in patients at high risk does not reduce the mortality rate nor the combined endpoint described above. ICU length of stay was greater in those patients in whom IABP was implanted prior to surgery; there were no diff erences in overall hospital stay.
Intraortic balloon pump use in cardiac surgery: analysis of data from the ARIAM Registry of Cardiac Surgery J Muñoz-Bono, MD Delgado-Amaya, E Curiel-Balsera, C Joya-Montosa, G Quesada-García Besides mortality was higher, the later IABP was implanted the higher the mortality rate was (29.6% of the preoperative, 44.2% of surgical and 54.4% of those starting in ICU, P = 0.015). The ICU length of stay was 9 ± 22 days while the hospital length of stay was 21 ± 28 days. In patients who needed IABP, the ICU stay was higher than for those who did not need it (9 ± 22 vs. 5 ± 10 days, P = 0.002) whereas there was no diff erence in hospital stay (21 ± 28 vs. 20 ± 24 days, P = 0.054).
Conclusion The intra-aortic balloon pump was used by 4.5% of surgeries performed during the study period and in patients with an increased risk of perioperative complications, estimated by EuroSCORE. ICU length of stay was higher in patients requiring IABP, with no diff erences in overall hospital stay. Mortality rate was 40% higher, and increases with the delay in the implantation. 
We hypothesised that pharyngeal oxygen concentrations would be maintained higher and for longer with transnasal humidifi ed rapid insuffl ation ventilatory exchange (THRIVE) than conventional bag-mask pre-oxygenation (CPO). CPO requires the mask to be removed during laryngoscopy; this means that air may enter the mouth so subsequent apnoeic oxygenation will be less eff ective. Oral suctioning could exacerbate this process. However, if high pharyngeal oxygen concentrations and an open airway are maintained, apnoeic oxygenation could be substantially improved. Methods used have included NO-DESAT [1] and recently THRIVE [2] , which has been shown to extend apnoea times for up to 1 hour. Methods A volunteer with a nasopharyngeal sampling catheter underwent simulated emergency airway management (EAM), using both CPO and THRIVE, with and without suction. Following 3 minutes of pre-oxygenation with CPO (FiO 2 = 1, FEO 2 >0.8) or THRIVE (60 l/minute; Optifl ow, Fisher and Paykel), EAM was simulated by voluntary apnoea and pharyngoscopy with the laryngoscope blade tip placed 2 cm from the posterior pharyngeal wall. Capnography at the laryngoscope tip confi rmed apnoea. Pharyngeal gas samples (20 ml) were collected during apnoea, and after 5 seconds of oropharyngeal suctioning. Preoxygenation was repeated between sampling. Samples (n = 100) were analysed using calibrated fuel cells.
Results Pharyngeal oxygen concentrations (mean and SEM) are shown in Figure 1 (all points are signifi cant P <0.05). Conclusion Pharyngeal oxygen concentration rapidly falls following CPO. This may be detrimental for apnoeic oxygenation during con ventional laryngoscopy. Conversely, THRIVE maintains high pharyngeal oxygen concentrations over time. Suction has an immediate negative eff ect on pharyngeal oxygen concentration that is attenuated by THRIVE. Assessment of NO-DESAT (15 l/minute) was abandoned due to discomfort. References Critical Care 2015, Volume 19 Suppl 1 http://ccforum.com/supplements/19/S1 S69 is located in the same position as the light source on the standard Macintosh blade thus providing a view angle of up to 290° and the USB camera is connected to a laptop. A total of the fi rst 50 patients who presented to the emergency department over a period of 6 months in need of intubation were included in the study and every alternate patient participated in the evaluation of the assembled video laryngoscope (VAL). Information about patient demographics and airway characteristics, Cormack-Lehane (C/L) views and the ease of intubation using the VAL was collected. Failure was defi ned as more than one attempt at intubation.
Results Excellent (C/L1) or good (C/L2) laryngeal exposure was obtained in 92% and 8% of patients respectively. In 25 patients in whom VAL was performed, there was a comparable or superior view. Intubation with direct laryngoscopy was successful in 95.2% of patients and VAL was successful in 95.4% of patients. Three patients from the VAL group and four patients from the direct laryngoscopy group were excluded. See Figure 1 .
Conclusion This new assembled VAL is the cheapest video-assisted laryngoscope available costing around $60, which can even be introduced into primary healthcare setup in developing countries. VAL consistently yielded a comparable or superior glottic view compared with direct laryngoscopy despite the limited or lack of prior experience with the device. Because the device can be used for both routine as well diffi cult tracheal intubation, it may be a helpful tool to intubate trauma cases where C-spine immobilization is unavoidable. The presented video-assisted laryngoscope is a useful tool for documentation, teaching and monitoring tracheal intubation. Introduction Airway complications are more common outside the operating theatre and in emergency situations. Capnography remains the gold standard of confi rming correct endotracheal tube (ETT) placement, retaining high sensitivity and specifi city in cardiac arrest [1] . The 2010 European Resuscitation Council guidelines for adult advanced life support recommended waveform capnography in this setting [2] . Failure to use capnography was also identifi ed as a major contributor to airway-related morbidity and mortality in a national UK audit [3] . We sought to investigate current practice relating to the availability and use of capnography equipment cardiac arrest within UK hospitals.
Methods Between June and November 2014, a telephone survey was conducted of all UK acute hospitals with adult level 3 ICUs and an emergency department (ED). Hospitals were identifi ed using nationally available data. A standardised telephone questionnaire was developed examining practice regarding intubation for cardiac arrest and the availability and utilisation of capnography within the ED, ICU and general wards. Questions were directed at the anaesthetist or intensive care doctor 'responding to cardiac arrest calls' . The respondent was given the option to decline participation. All data were anonymised. Results A total of 211 hospitals met the inclusion criteria. The response rate was 100%. Arrest calls were mainly attended by anaesthesia (47.8%) and ICU doctors (38.3%) with around 2% physicians only.
Most were a registrar grade (56.3%). The ability to measure ETCO 2 was available in all but four EDs; most used waveform capnography. A similar pattern was seen was seen in the vast majority of ICUs: a single institution reported no capnography available. However, in 141 (66.8%) of the hospitals surveyed, no facility to measure ETCO 2 was present on the general wards. Where available, 86.7% used capnography to confi rm ETT placement. Less than 50% used ETCO 2 to determine CPR eff ectiveness and 8% to prognosticate. Conclusion We believe this is the fi rst study of its kind to follow NAP4 and investigate the availability of capnography throughout for use during cardiac arrest. Whilst equipment levels appear adequate (albeit not perfect) in resuscitation areas, there appears a lack of availability of suitable devices on general wards.
Introduction Nasogastric (NG) tube insertion is necessary in a variety of critically ill patients for intra-abdominal decompression, prevention of aspiration, route of medication administration and nutrition. However, it often fails in patients who showed sedated or comatose mentality with poor cooperation during the procedure. Although there are many reports inserting a NG tube in diffi cult cases, most methods need a special guide wire, tube or nasoendoscope. We report a case of NG tube insertion in a comatose patient using a laryngoscope and endotracheal tube which are easily available. Introduction Bantry General Hospital (BGH) is a small rural hospital serving a large, geographically isolated part of southwest Ireland. Following an infl uential national review of adult critical care services [1] , a protocol was introduced in late 2010 mandating the immediate transfer of all medical patients intubated on an emergency basis to a large critical care centre 100 km away. Similar mandatory transfer protocols were introduced at the same time throughout the island of Ireland but few data are available regarding patient outcomes. We designed a study to look at the outcomes of all patients encompassed by the protocol at BGH. Methods We retrospectively reviewed the charts and electronic data of medical patients requiring emergency intubation at BGH from November 2010 to December 2013. We recorded the following data: age, sex, admission diagnosis, comorbidities, time delay to transfer, intransit mortality, length of stay, survival to discharge and 1-month and 6-month mortality. Results Forty-fi ve patients (31 male) were included with a mean age of 67 ± 15 years. The commonest admission diagnoses were sepsis (10), cardiogenic shock (10), primary respiratory failure (nine) and intracranial haemorrhage (eight). The median transfer delay time was 47 minutes. Only 27 (60%) patients were actually transferred and they were signifi cantly younger than nontransferred patients (62 vs. 73 years, P = 0.02). In-transit mortality was zero. Mean length of stay in the critical care centre was 14.8 ± 16.8 days. Survival to discharge was signifi cantly higher in transferred (14/27) compared with nontransferred (3/18) patients (52% vs. 17%, P = 0.017). Overall mortality rates were 62% and 69% at 1 and 6 months respectively and were signifi cantly lower in the transferred group (P = 0.02). Conclusion Overall mortality rates of medical patients intubated urgently at BGH were high. Forty per cent of intubated patients were not transferred, indicating signifi cant modifi cation of the protocol over time. Patients transferred to the critical care centre were younger and had signifi cantly better outcomes than patients remaining in BGH, probably due to decisions not to transfer patients with poor prognoses. Most patients who survived to discharge were still alive 6 months later. Reference
Introduction Video laryngoscopy (VL) is known to improve glottic visualization and the fi rst-attempt success rate compared with direct laryngoscopy (DL) in emergency tracheal intubations (ETIs). Since VL does not align the oral, pharyngeal, and laryngeal axes of the upper airway, it sometimes leads to failed intubation despite good glottic visualization. We tested the hypothesis that VL has a lower fi rstattempt success rate of ETI than DL among patients with good glottic visualization.
Methods We performed a prospective observational study examining ETIs at our tertiary care institution from July 2012 to June 2014. All consecutive patients who underwent ETIs in the emergency department and ICU were included. Patients under 18 years of age, intubated with VL not using C-MAC, were excluded. After each ETI eff ort, the operator completed a standardized data collection form. We classifi ed glottic visualization as good (C-L grade 1 or 2), and poor (C-L grade 3 or 4). The primary outcome was the fi rst-attempt success rate. Introduction Rapid sequence induction (RSI) in the ICU, emergency department (ED) and operating room (OR) carries the risk of hypoxemia if laryngoscopy is prolonged especially in high-risk patients. Bag and mask pre-oxygenation is normally used to extend the apnoea time; however, arterial desaturation may still rapidly occur. Transnasal humidifi ed rapid insuffl ation ventilatory exchange (THRIVE) is a new technique that provides modest CPAP during pre-oxygenation and crucially also continuous oxygenation of the pharyngeal space throughout the apnoeic period. In elective surgery, THRIVE provides apnoea times as long as 60 minutes due to apnoeic oxygenation [1] . We report the fi rst implementation of THRIVE with emergency patients into the ICU, ED and OR. Methods Following training a THRIVE system was installed in each location either as a fi xed system on the anaesthetic machine (OR) or a mobile solution on a wheeled stand (ICU, ER). This was a simplifi ed Optifl ow system (Fisher and Paykel, New Zealand) consisting of a high-fl ow rotameter, a reusable humidifi er, a reusable circuit and a disposable nasal interface. Anaesthetists of all grades were encouraged to use THRIVE (60 l/minute) prior to and during all high-risk intubations. Introduction β-Adrenergic agonists increase the ciliary beat frequency in experimental models, raising the possibility that they may be useful for airway hygiene [1] . Salbutamol increases large airway mucociliary clearance [2] , although this may not be true for smaller airways [3] . There are no data from ICU patients, so we decided to test the eff ectiveness of transtracheal instillation of a mixture of ipratropium and salbutamol, assessing the reduction of the number of aspirations and hours of ventilation.
Methods An open randomized prospective study was held during 2014. All admitted patients were alternately selected as the study or control group and included if submitted to invasive ventilation for at least for 24 hours. Four patients who had secondary cardiac rhythm eff ects attributable to β-adrenergic agonists were excluded. In the study group, 3 ml of a dilution of 2.5 ml ipratropium (0.52 mg) plus salbutamol (2.5 mg) with 2.5 ml distilled water was instilled after the aspiration of secretions. During the ventilation period we noted for both groups, in addition to the demographic data, the number of tracheal aspirations by day and hours of ventilation. Conclusion The preliminary analysis, referring to the fi rst 6 months of study, showed a tendency in the reduction of both ventilation hours and length of stay in the study group. No signifi cant diff erence was found in the number of aspirations, which may be explained by ICU nursing routines. Further studies will try to fi nd whether signifi cant diff erences in the incidence of VAP exist, allowing this procedure to be implemented in ICU routines.
We hypothesised that the simultaneous use of a heat and moisture exchanger (HME) and a heated humidifi er (HH) would increase the incidence of airway occlusion over a 24-hour period in comparison with each device in isolation. This bench study compares the incidence of airway occlusion when using (group 1) no airway humidifi cation, (group 2) a HME alone, (group 3) a HH alone and (group 4) both a HME and a HH in combination. Tracheal intubation requires the use of artifi cial humidifi cation systems. HMEs are less effi cient but convenient especially for a short period of intubation and HHs are commonly more expensive. Both devices are often used in close proximity on the ICU depending on the particular clinical scenario and/ or clinical practitioner. Following a critical incident of HME obstruction due to waterlogging on our ICU we realised that HH and HME may be used together inadvertently. This airway obstruction was only resolved by the removal of the HME from the patient's breathing circuit. Introduction Ventilator-associated pneumonia (VAP) is a common complication in mechanically ventilated patients. Frequently the pathogens responsible derive from aspirated secretions of the upper respiratory tract or the stomach. In order to prevent aspiration, two missions should be attained: a good tracheal cuff seal with a welltolerated pressure, together with continuous evacuation of secretions from the subglottic space. These two goals can be achieved using the AnapnoGuard system and its related endotracheal tube (ETT). Methods A single-center, open-label study in a general ICU. Control group: (retrospective data) mechanically ventilated patients on standard of care regular ETT, manual suction of the trachea and oralpharyngeal space by nursing staff . Study group: (prospective data) connected at all times to the AnapnoGuard system: an ETT with two above-the-cuff suction ports and a third port and lumen for rinsing and CO 2 measurement. A triple lumen harness is connected to a control system designed to measure CO 2 levels above the cuff (to identify leaks), infl ate the cuff accordingly, rinse and suction secretions above the cuff . To be included in the study patients had to have no pneumonia on admission and at least 3 days of mechanical ventilation. VAP was diagnosed for a new chest X-ray infi ltrate accompanied by fever, leucocytosis and positive sputum culture. 
Since the introduction and development of percutaneous dilatational tracheostomy (PDT), this procedure is accepted and incorporated in ICUs worldwide. In spite of obvious benefi ts for the patients, who obtain more comfort and mobility and less use of sedatives, the procedure also implies the risk of several complications, some of which may be lethal. Severe complications include hemorrhage, displacement and pneumothorax. Diff erent methods of PDT are described in the literature, each with disadvantages and benefi ts. The aim of this study was to analyze complications due to PDTs performed without the use of bronchoscopic assistance. Methods The study was conducted in a Danish eight-bed, nonuniversity ICU. Since 2007, all patients admitted to the ICU have been registered on an electronic patient record system, in which daily vital values, diagnoses, procedures and healthcare providers' notes are entered. When searching for 'percutaneous dilatation tracheostomy' in the electronic system, we found all patients who had undergone this specifi c procedure. Afterwards we analyzed each of these patients' hospital records, looking for any periprocedure or postprocedure complications noted within 7 days. In addition we registered patients' age, sex, BMI, SOFA score, methods used in procedures and experience of operators. Results A total of 136 patients admitted to the UCI had undergone a PDT between 2007 and 2014. Of these, two were excluded due to the PDT being performed in another hospital before admission to our ICU. All 134 PDTs were performed with the Ciaglia Blue Rhino Method.
No PDTs were performed with bronchoscopic guidance. In 12 cases some kind of complication due to the PDT was registered: six cases with need of surgical hemostasis, three cases of bleeding with need of transfusion of blood products, one case of PDT displacement, one case of ventilation-related problems during procedure and, fi nally, one case of tracheal cartilage fracture. There were no incidents of pneumothorax. No PDTs had a lethal outcome due to the procedure itself. The total complication rate was 9.0%. Of the 12 cases, four (33%) complications occurred during the procedure, the rest (66%) occurred after the procedure. The overall periprocedure complication rate was 3%.
Conclusion In this study, PDTs without the use of bronchoscopic guidance were performed safely with a low rate of complications. have outlined a decisional algorithm to choose the most appropriate technique in each case to reduce the incidence of complications. Methods A retrospective review was performed using data from the last 14 years. Two hundred patients were selected. Patients were divided into two groups: one including the fi rst 100 PDTs treated without the algorithm (nA-group) and the other including the last 100 patients treated with the algorithm (A-group). Valuation of clinical and anatomical features of the patients, neck ultrasound and fi brobronchoscopy came before the procedure [1] . The algorithm was formulated by our experience with PDT techniques, comparing the specifi c characteristics of each one with the physiopathological characteristics of each patient.
We recorded complications (bleeding, tracheoesophageal fi stula, subglottic stenosis, tracheal rings' fracture, diffi culty of placement, change of procedure) related to PDTs performed with and without applying the algorithm. We considered complications that occurred in our experience and we changed our modality in technique choice ( Figure 1 ). Compared with the complications reported in the nA-group, use of the algorithm as a guide to choose the kind of PDT technique seems to reduce the incidence of complications (37% vs. 19%; P = 0.001 chi-square test). Conclusion In our experience the application of the proposed algorithm may reduce the incidence of complications related to PDT in the ICU. However, a randomized controlled multicenter study would be necessary in order to confi rm the effi ciency and validity of the proposed algorithm. Reference Introduction Percutaneous bedside tracheostomy (PBT) is a frequently done procedure in the ICU. PBT is a clean-contaminated procedure, and the duration of the procedure is 15 to 20 minutes depending on the physician's procedural skills. The rate of infectious complications and effi cacy of perioperative therapy in reducing infections after PBT is currently unknown. Currently there have been no defi nitive recommendations for prophylactic antibiotic therapy before PBT in the ICU. Methods All clinical and microbiological data were retrospectively collected and analyzed during the ICU stay before PBT performance and 72 hours after the PBT procedure from 110 patients in our ICU. Controls were defi ned as patients in whom the PBT procedure was performed in the ICU, with antibiotics administered 72 hours prior to and during the procedure (Group 1, n = 82). Cases were defi ned as patients in whom the PBT procedure was performed in the ICU without antibiotics administered 72 hours prior to and during the procedure (Group 2, n = 28). Secondary bacteremia, line sepsis and VAP during the 72 hours after PBT were considered infectious complications. Twotailed P <0.05 was considered to be signifi cant.
Results No diff erences were found in age, gender, admission diagnoses, length of ICU stay and in-hospital mortality rate between the two study groups. Overall Gram-negative, Gram-positive and fungal fl ora were similar in both groups before and after PBT. Patients who received antibiotic therapy had a lower incidence of new ventilator-associated pneumonia (VAP) episodes (15/82 (18.2%) in Group 1 vs. 14/28 (50%) in Group 2, P <0.001 (0.23, 0.87 to 0.13)) ( Table 1 ). There were no diff erences in the incidence of bacteremia or line sepsis (Table 1) . Conclusion Our fi ndings highlight the importance of conducting a prospective randomized control trial to better understand the role of antibiotic prophylaxis in PBT. Introduction Respiratory failure is a well-known complication of aortic aneurysm surgery. We describe the impact of a protocol, using CPAP after elective surgery to reduce the need for unplanned invasive ventilation.
Methods In 2012 we introduced a CPAP protocol for patients undergoing elective aortic aneurysm surgery, either open (AAA) or as an endovascular repair (EVAR). According to pre-existing risk factors (see Table 1 ) and arterial blood gas analysis in the anaesthetic room, they were assigned to two alternative options on the ITU: prophylactic CPAP for 9 hours in each of the fi rst two postoperative nights or oxygen via face mask. CPAP was applied at any time in the patients stay, if their P/F ratio dropped below 40. Criteria to stop CPAP were also predefi ned. Previously, CPAP was initiated at the discretion of nursing staff , P/F ratios were not utilised. Table 2 presents requirements for invasive ventilation (IPPV) and length of stay (LOS) for both patient groups. Conclusion There is a clear reduction in the need for unplanned IPPV in both patient groups. An audit in 2013 showed incomplete protocol adherence in the ITU, therefore benefi ts may be underestimated.
Is the gastric tube a burden for noninvasive ventilation? Although NIV is thought to be more comfortable for patients than invasive mechanical ventilation, its failure rates in the ICUs range between 10 and 40%. Except for the interface-related problems, there are some specifi c considerations for the patient-ventilator interaction and the applied mechanical forces. During NIV there is a predisposition for the stomach to be infl ated with gas, which could cause severe respiratory complications, especially in COPD patients, and thus prolong the mechanical ventilation and the weaning process. This remains one of the major causes for NIV failure. Although a lot of face masks with diff erent interfaces are available on the market, just a few have additional ports for a NGT. They are characterized by higher price and a complex setup. In order to perform NIV in patients, requiring NGT placement, without additional air leaks and to be able to ensure their enteral nutrition and/or stomach drainage, we installed a port for a NGT on a standard face mask. Methods In this study, six of the COPD patients admitted to our ICU, who required NGT placement, were ventilated with the Draeger Evita 2 dura through a modifi ed reusable silicone face mask (UMDNS code: 12-453 with 22 mm ID connection; sizes 4 and 5) with silicone headgear and a hook ring. All of them had a NGT during their stay in the ICU. We evaluated the effi cacy of our modifi cation comparing the achieved Vt with modifi ed and unmodifi ed face mask, during two periods of 10 minutes. The mode and parameters of ventilation were not changed.
We assessed patient comfort with a visual analogue scale.
The average duration of NIV was 3.5 days (SD = 1.6). We examined two sets of 10 consequent breathing cycles for every patient. The mean Vt was 472 ml (SD = 76 ml) with standard face mask and 460 ml (SD = 86 ml) with the modifi ed one. There was statistically signifi cant correlation between the two datasets (P <0.05). No additional leaks were detected. According to the VAS evaluation, fi ve of the patients (83%) had comfort improvement with the modifi ed mask. Conclusion With this modifi cation of the face mask we achieved adequate drainage of the stomach and/or the enteral nutrition of the patients and improvement in their comfort during NIV, compared with the ventilation with a standard mask, without additional air leaks and at a low cost. Introduction Lung ultrasound (LUS) allows semiquantifi cation of lung aeration in PEEP trials [1] , pneumonia [2] and weaning [3] . LUS score is based on number/coalescence of vertical artifacts (B-lines) in longitudinal scan (LONG) [4] : the pleura is identifi ed between two ribs and its visualization limited by intercostal space (ICS) width. We hypothesized that a transversal scan (TRANSV) aligned with ICS would visualize longer pleura and a higher number of artifacts, with better assessment of loss of aeration (LoA). Methods LONG and TRANSV were performed in six areas per lung (anterior, lateral and posterior, each divided into superior and inferior). Once LONG was performed, TRANSV was obtained by a probe rotation until the ribs disappeared. We considered pleural length, B-line number/coalescence, and subpleural/lobar consolidations. LUS score was assigned: 0 normal lung, 1 moderate LoA (≥3 well-spaced B-lines), 2 severe LoA (coalescent B-lines), 3 complete LoA (tissue-like pattern). Results We enrolled 38 patients (21 males, age 60 ± 16 years, BMI 24.7 ± 4.7 kg/m 2 ) corresponding to 456 ICSs. In 63 ICSs, a tissue-like pattern was visualized in both techniques. In the other 393, LONG versus TRANSV pleural length was 2.0 ± 0.6 cm (range 0.8 to 3.8; variance 0.31) versus 3.9 ± 0.1 cm (range 3.0 to 4.3; variance 0.1) (P <0.0001), B-lines per scan were 1.1 ± 1.6 versus 1.8 ± 2.5 (P <0.0001), coalescent B-lines were detected in 24 versus 30% (P <0.05) and subpleural consolidations in 16 versus 22% (P <0.05), respectively. LUS scores' prevalence signifi cantly diff ered in LONG versus TRANSV (Figure 1 ). Conclusion TRANSV visualizes signifi cantly longer pleura and greater number of artifacts useful for lung disease assessment.
Ultrasound assessment for extravascular lung water in patients with septic shock P Pirompanich 1 , A Wattanathum 2 1 Thammasat University, Pathumthani, Thailand; 2 Phramongkutklao Hospital, Bangkok, Thailand Critical Care 2015, 19(Suppl 1):P223 (doi: 10.1186/cc14303) Introduction Extravascular lung water (EVLW) refers to fl uid within the lung but outside the vascular compartment. Increment of EVLW was associated with mortality in critically ill patients. Extravascular lung water index (EVLWI) >10 ml/kg was found in patients with cardiogenic pulmonary edema and correlated with pulmonary capillary wedge pressure >20 mmHg. Measurement of EVLW needs sophisticated tools and an invasive method by transpulmonary thermodilution (TPTD) technique. In contrast, multiple B-lines by lung ultrasound (LUS) have been recently proposed to correlate with increased EVLW in patients with pulmonary edema. This study aims to compare three methods of LUS and EVLWI measured by TPTD to assess pulmonary edema in patients with septic shock. Methods The authors prospectively enrolled 17 patients with septic shock who were admitted to the medical ICU, Phramongkutklao Hospital between September 2013 and June 2014. EVLWI was measured by TPTD (VolumeView Set, EV1000; Edwards Lifesciences) method. According to international evidence-based recommendations for point-of-care lung ultrasound 2012, three methods of LUS (LOGIQ e ultrasound; GE Healthcare) were compared to assess EVLW daily in each patient until no indication for invasive blood pressure monitoring [1] . Firstly, B-lines were measured in 28 lung zones. The total numbers of B-lines seen in each patient were counted as total B-line scores (TBS). Secondly, upper and lower BLUE points were anterior two-region scans each side marked by physician hands. Pulmonary edema was diagnosed if three or more B-lines were presented in all regions. Lastly, scanning eight regions, two anterior and two laterals per side, was considered abnormal if more than one scan per side had three or more B-lines. Results A total of 40 comparisons were obtained. Signifi cant positive linear correlations were found between TBS and EVLWI determined by TPTD (r = 0.637, P <0.001). The TBS ≥39 has sensitivity of 91.7% and specifi city of 75.0% to defi ne EVLWI >10 ml/kg. There was low sensitivity (33.3% and 50.0% respectively) but high specifi city (100% and 96.0% respectively) of the positive BLUE points and eight regions to defi ne EVLWI >10 ml/kg. Conclusion TBS is the best method for assessing EVLW compared with BLUE points and eight regions. These data support the benefi t of LUS with summation of B-line scores of 28 rib interspaces for assessment of the increment of EVLW in septic shock patients. Reference
Introduction Mechanical ventilation (MV) induces diaphragmatic muscle atrophy and contractile fi bre dysfunction, the so-called ventilator-induced diaphragm dysfunction (VIDD). Although diaphragmatic atrophy can be assessed using ultrasound, the biggest trial in humans published so far included seven patients and only measuring the thickness at two moments during the disease process [1] . We aimed to assess the time course of diaphragm atrophy in a larger cohort of MV patients using ultrasound. Methods A total of 54 patients from an adult ICU were included in this prospective single-centre cohort trial. Patients who needed <72 hours of MV or had been recently admitted to an ICU were excluded. Patients were ventilated in a controlled, assisted, and/or hybrid ventilation mode. The thickness of the diaphragm was assessed daily; the fi rst recording was within 24 hours after the start of mechanical ventilation and we continued the measurements until the patients were extubated or tracheotomised. We measured the diaphragm at the zone of apposition, as described by McCool and colleagues [2] using a linear 13 MHz ultrasound probe. Figure 1 shows a sample measurement.
We were successfully able to record the diaphragm thickness in all included patients. Median time on the ventilator was 9 days (IQR 4 to 15 days). Mean baseline thickness was 1.9 mm (SD ±0.4 mm), and mean nadir was 1.3 mm (SD ±0.4 mm), corresponding with a mean change in thickness of 32% (SD ±18%). As early as after only 72 hours of MV, we already noted an average drop of diaphragm thickness of 20%, illustrating the rapid progression of the atrophy in VIDD. Conclusion On average, diaphragm thickness decreased 32% in our cohort. The decrease occurred rapidly, with two-thirds of the maximal thinning already present after 72 hours of MV. References Introduction Chest-X-ray is recommended for routine use in patients with suspected pneumonia, but its use in emergency settings is limited. In this study, the diagnostic performance of a new method for quantitative analysis of lung ultrasonography was compared with bedside chest X-ray and visual lung ultrasonography for detection of community-acquired pneumonia, using thoracic computed tomography as a gold standard.
Methods Thirty-two spontaneously breathing patients with suspected community-acquired pneumonia undergoing computed tomography examination were consecutively enrolled. Each hemithorax was evaluated for the presence or absence of abnormalities by chest X-ray and quantitative or visual ultrasonography. Results Quantitative ultrasonography showed higher sensitivity (93%), specifi city (95%), and diagnostic accuracy (94%) than chest X-ray (64%, 80%, and 69%, respectively), or visual ultrasonography (68%, 95%, and 77%, respectively), or their combination (77%, 75%, and 77%, respectively). Conclusion Quantitative lung ultrasonography was considerably more accurate than either chest X-ray or visual ultrasonography in the diagnosis of community-acquired pneumonia and it may represent a useful fi rst-line approach for confi rmation of clinical diagnosis in emergency settings. Introduction One of the aims of lung recruitment is to improve oxygenation [1] , but it has not yet been investigated in spontaneously breathing patients. Our objective was to evaluate the eff ects of recruitment maneuvers on oxygenation in patients ventilated in CPAP/ pressure support (CPAP/PS) mode. Methods In a prospective, observational study, 30 patients with a Lung Injury Score ≥2 were recruited. Following baseline measurements (t 0 ) PEEP was increased by 5 cmH 2 O (t 1 ). Recruitment maneuver was applied for 40 seconds with 40 cmH 2 O PS. Measurements were taken immediately after recruitment (t 2 ) then 15 minutes (t 3 ) and 30 minutes later (t 4 ). Results According to the diff erence of PaO 2 /FiO 2 between t 2 and t 0 , three groups were defi ned: nonresponders (NR: diff erence of PaO 2 /FiO 2 ≤0%, n = 8), low responders (LR: diff erence of PaO 2 /FiO 2 = 0 to 50%, n = 11) and high responders (HR: diff erence of PaO 2 /FiO 2 >50%, n = 11). In the NR-group, PaO 2 /FiO 2 decreased signifi cantly: median (interquartile), PaO 2 /FiO 2 = 178 (159 to 240) versus 165 (118 to 210) mmHg; in the LRgroup and in the HR-group there was signifi cant improvement: 119 (98 to 164) versus 161 (123 to 182) mmHg and 141 (130 to 183) versus 239 (224 to 369) mmHg, P <0.05, respectively. Dynamic compliance (C dyn ) signifi cantly dropped at t 2 as compared with t 0 in the NR-group, C dyn = 62 (48 to 87) versus 53 (43 to 78) ml/cmH 2 O, while there was no signifi cant change in the LR-and HR-groups, P <0.05. At the same time points the dead space to tidal volume ratio (Vds/Vte) signifi cantly increased in the NR-group, Vds/Vte = 30 (23 to 37) versus 37 (26 to 42)%, but not in the LR-and HR-groups, P <0.05. Conclusion Recruitment maneuvers improved PaO 2 /FiO 2 in the majority of patients (73%) without aff ecting C dyn or Vds/Vte; therefore it may be a safe approach to improve oxygenation in patients ventilated in CPAP/PS mode. Reference traditional parametric sample size estimations depend upon restrictive assumptions that often do not hold in real data. This study estimates N to detect changes in length of mechanical ventilation (LoMV) using Monte-Carlo simulation (MCS) and mechanical ventilation (MV) data to better simulate the cohort. Methods Data from 2,534 MV patients admitted to Christchurch Hospital ICU from 2011 to 2013 were used. N was estimated using MCS to determine a sample size with power of 80%, and compared with the Altman's nomogram for two patients groups, (1) all patients and (2) targeted patients with 1 <LoMV ≤15 days. MCS allows any range of intervention eff ect to be simulated, where this study tested a 10 and 25% diff erence in LoMV (0.5 to 1.25 days for mean LoMV of 5 days). The simulated LoMV for the intervention group is compared with the LoMV in a control group using the one-sided Wilcoxon rank-sum test, Student t test, and Kolmogorov-Smirnov test to assess central tendency and variation. Results The distribution of LoMV is heavily skewed. Altman's nomogram assumes a normal distribution and found N >1,000 to detect a 25% LoMV change. Figure 1 panels (1) and (2) show N for 80% power if all patients were included, and panels (3) and (4) for the targeted patient group. Panels (1) and (3) show that it is impossible to achieve 80% power for a 10% intervention eff ect. For 25% eff ect, MSC found N = 400/arm (all patients) and N = 150/arm (targeted cohort).
Conclusion Traditional parametric sample size estimation may overestimate the required patients. MCS can estimate eff ective N/arm and evaluate specifi c patient groups objectively, capturing local clinical practice and its impact on LoMV. It is important to consider targeting specifi c patient groups by applying patient selection criteria that can be easily translated into trial design. Introduction Benzodiazepines are used in many of settings to induce sedation, but can cause a reduction in respiratory drive. Objective monitoring of the eff ect of benzodiazepines on respiratory status in non-intubated patients has been diffi cult, putting patient safety at risk. A non-invasive respiratory volume monitor (RVM) that provides continuous measurement of minute ventilation (MV), tidal volume (TV) and respiratory rate (RR) was used to quantify the eff ects of midazolam on respiratory status in spontaneously breathing patients. Methods An impedance-based RVM (ExSpiron; Respiratory Motion Inc., Waltham, MA, USA) was used in 30 patients who received 2 mg midazolam prior to induction of anesthesia and were sedated but spontaneously breathing. Eleven of these patients (58 ± 19 years, average BMI 27.7) received midazolam at least 20 minutes prior to induction. Digital RVM data were collected and MV, TV and RR calculated and evaluated from 30-second segments 10 minutes before and after the fi rst dose of midazolam. Ten patients were analyzed as a group and one patient was analyzed separately (due to idiosyncratic reaction). Results Following administration of midazolam, the group MV and TV decreased an average of 19 ± 7% and 16 ± 5%, respectively (mean ± SEM, P <0.01, both) while RR remained essentially unchanged (decrease of 3 ± 8%, P >0.3). In the younger half of the cohort (45 ± 16 years), the decreases in MV and TV were not signifi cant, only 6 ± 3% and 8 ± 5%, respectively. The older half of the cohort (72 ± 8 years) displayed fourfold greater MV and TV decreases (32 ± 11%, P <0.05 and 25 ± 6%, P <0.05), when compared with the younger cohort, P <0.01, Figure 1 ).
Conclusion Continuous monitoring with RVM provides a valuable depiction of hypoventilation from benzodiazepines, not demonstrated by other methodologies such as pulse oximetry and RR alone. RVM monitoring can help uncover potentially life-threatening hypoventilation in older patients. Further studies are ongoing to quantify hypoventilation after administration of other anesthetic medications.
Introduction Mechanical ventilation has the potential to induce pulmonary coagulopathy. Local treatment by nebulization of heparin could be benefi cial in ventilated patients. The aim of this data metaanalysis is to determine the association between nebulization of heparin and outcome of mechanically ventilated critically ill patients. Methods PubMed, Scopus, EMBASE, and Web of Science were searched for relevant articles. Articles were selected if they compared nebulization of heparin with standard care. The primary endpoint was overall mortality. Secondary endpoints included occurrence of pneumonia and number of ventilator-free days and alive at day 28. Results Six articles were found: fi ve retrospective cohorts with historical controls, one randomized controlled trial, covering 423 patients. Dosages of nebulized heparin varied from 30,000 to 150,000 IU/day. Fifty out of 222 patients (22.5%) receiving nebulized heparin and 48 out of 201 patients (23.9%) receiving standard care died (risk ratio (RR) 0.79 (95% CI 0.47 to 1.35)) (see Figure 1 ). Occurrence of pneumonia (RR 1.36 (95% CI 0.54 to 3.45); I 2 = 59%), and number of ventilator-free days and alive at day 28 (standardized mean diff erence 0.11 (95% CI -0.14 to 35); I 2 = 0%), were not diff erent between the two groups. Conclusion Nebulization of heparin is not associated with improved outcome in mechanically ventilated critically ill patients. This metaanalysis is limited by methodological problems in most included studies. Only one randomized controlled trial could be included. Also, most patients in the meta-analyzed studies suff ered from inhalation trauma, and heparin dosages diff ered widely.
The most frequent reasons for hypercapnic respiratory failure (HRF) in ICUs are COPD and in recent years obesity hypoventilation syndrome (OHS) and obstructive sleep apnea (OSA). Even 15 to 30% of COPD patients also have accompanying OSA. Due to increased upper airway resistance, those patients require higher expiratory pressures (EPAP) during noninvasive ventilation (NIV). In order to prescribe optimal mode and pressures during the ICU stay and at discharge, the intensivist should diagnose the underlying OSA. Portable recording devices have been developed and they were approved at least for the diagnosis in high pretest probability patients with results equal to in-laboratory polysomnography. The aim of this study is to assess whether respiratory polygraph (RPLG) can be used for obtaining diagnostic information of OSA in hypercapnic ICU patients. Methods Patients, with HRF requiring NIV, were included in the study. RPLG studies were conducted under nasal oxygen before NIV, using the Philips Respironics Alice PDx® device, which provides the records of pulse oximetry with derived heart rate; snoring and nasal airfl ow with nasal pressure transducer and nasal thermistor; rib cage, abdominal motion and body position with abdominal and thoracic belts. American Academy of Sleep Medicine 2014 recommendations were used for the diagnosis of OSA and OHS. Because of the diagnostic diffi culties of hypopnea in hypoxemic patients, we evaluated only the obstructive apnea index (OAI) instead of the apnea hypopnea index (AHI). Results Thirty-one patients with the mean age of 67 ± 9 years were included in the study. Their mean APACHE II score was 16 ± 5 and BMI was 33 ± 9 kg/m 2 . Admission arterial blood gases were as follows (mean ± SD); pH: 7.33 ± 0.07, PaO 2 : 74 ± 12 mmHg, PaCO 2 : 69 ± 11 mmHg, HCO 3 -: 31 ± 5, O 2 Sat%: 92 ± 4. Admission diagnoses of the patients were OHS (36%) and COPD (68%). Mean OAI was 13 ± 6 in patients with OAI >5. Eighty-one percent (n = 25) of the recordings were interpretable and clinical and RPLG data supported a new diagnosis of OSA in 14 (56%) patients, and EPAP levels were increased. Laboratory sleep study was recommended to 19% of the patients. At the end of the study 56% of the COPD and 72% of the OHS patients were identifi ed to have OSA. Conclusion Although it underestimates AHI, RPLG is important and technically feasible in ICU patients in suggesting the presence of OSA and in providing information for appropriate NIV management. Introduction Respiratory muscle weakness is common in mechanically ventilated patients and impairs liberation from ventilation. Inspiratory muscle training (IMT) might accelerate liberation from mechanical ventilation. We undertook to summarize previously published IMT protocols and the impact of IMT on respiratory muscle function and clinical outcomes. Methods We searched multiple databases using a sensitive search strategy combining MeSH headings and keywords for studies of IMT during MV. Studies were adjudicated for inclusion and data were abstracted independently and in duplicate. Methodological quality was assessed using the GRADE system. Results Eleven studies met the inclusion criteria; of these, six were randomized controlled trials and fi ve were observational studies. Critical Care 2015, Volume 19 Suppl 1 http://ccforum.com/supplements/19/S1 S80 A variety of IMT techniques were employed including inspiratory threshold loading (eight studies), biofeedback to increase inspiratory eff ort (one study), chair-sitting (one study) and diaphragmatic breathing pattern training (one study). Threshold loading was achieved by application of an external device (six studies) or increases in the inspiratory pressure trigger setting (two studies). Most studies implemented IMT in the weaning phase (n = 5) or after diffi cult weaning (n = 5); one study implemented IMT within 24 hours of intubation. IMT was associated with greater increases in maximal inspiratory pressure compared with control (six studies, mean diff erence 7.6 cmH 2 O (95% CI 5.8, 9.3), I 2 = 0%). There were no signifi cant diff erences in the duration of MV (six studies, mean diff erence -1.1 days (95% CI -2.5, 0.3), I 2 = 71%) or the rate of successful weaning (Figure 1 ; fi ve studies, risk ratio 1.13 (95% CI 0.92, 1.40), I 2 = 58%). The GRADE quality of evidence was low for all these outcomes; risk of bias was high for most studies and summary eff ects were imprecise and inconsistent. No serious adverse events related to IMT were reported. Conclusion IMT in mechanically ventilated patients appears safe and well tolerated and improves respiratory muscle function. IMT was not associated with accelerated liberation from mechanical ventilation. However, because the included studies had important methodological limitations and employed varying methods of IMT, we cannot draw fi rm conclusions about the eff ect of IMT on clinical outcomes.
Prospective assessment of the ability of rapid shallow breathing index computed during a pressure support spontaneous breathing trial to predict extubation failure in ICU G Besch 1 , J Revelly 2 , P Jolliet 2 , L Piquilloud-Imboden 2 1 CHRU Besançon, France; 2 CHUV, Lausanne, Switzerland Critical Care 2015, 19(Suppl 1):P232 (doi: 10.1186/cc14312) Introduction As the objective clinical criteria [1] are imperfect to assess patients before extubation, simple physiological parameters are used to try to improve extubation failure (EF) prediction. The rapid shallow breathing index (RSBI) (respiratory rate (RR) over tidal volume (VT) ratio) recorded during a T-piece spontaneous breathing trial (SBT) is known as the most reliable physiologic predictor. However, RSBI is nowadays usually computed during a pressure support (PS) SBT using the values displayed on the ventilator screen and not based on spirometry measurements without any assist as initially published. The aim of the present study was to prospectively assess the ability of currently measured RSBI to predict EF. Methods Retrospective analysis of prospectively collected data from patients intubated for more than 48 hours admitted in the medicosurgical ICU of Lausanne, Switzerland, from January 2007 to December 2008. EF was defi ned as the need for reintubation within 48 hours after extubation. Reintubations for a procedure requiring general anesthesia were not considered as EFs. RR and VT during the PS SBT were recorded from the ventilator and RSBI was computed accordingly. Baseline characteristics and currently measured RSBI were compared between patients who experienced EF versus success (t test or chi-square test as appropriate). The ability of currently measured RSBI to predict EF was assessed using ROC curve analysis. Results A total of 478 extubated patients were included, 25 of whom (5.2%) were reintubated. ICU mortality (ICU-m) and in-hospital mortality (H-m) were higher in reintubated patients: ICU-m = 6 (24) versus 22 (5), P = 0.002 and H-m = 9 (36) versus 63 (15), P = 0.009. The reasons for EF were: acute lung failure (n = 15), congestive heart failure (n = 4) and aspiration/bronchial congestion (n = 6). Demographic data were similar between patients successfully and nonsuccessfully extubated: age: 58 ± 17 versus 58 ± 19 years, P = 0.85; male gender: 15 (60) versus 263 (61), P = 0.99. SAPS II score was higher in the EF group: 30 ± 22 versus 42 ± 27, P = 0.04. RSBI were signifi cantly higher in patients who experienced EF: RSBI=59 ± 44 versus 43 ± 26, P = 0.04. The area under the ROC curve for currently measured RSBI was: 0.617 (95% CI = 0.571 to 0.662), P = 0.035. Conclusion In a cohort of 458 medico-surgical ICU patients, RSBI measured during a pressure support SBT was higher in patients experiencing EF but very imperfect to predict EF. Introduction Neutrophil extracellular traps (NETs) are fi brous structures that are produced extracellularly from activated neutrophil. They can trap and kill various pathogens, and their release is one of the fi rst lines of immune system. Meanwhile, recently it was reported that NETs also exert adverse eff ects in infl ammatory diseases. Acute respiratory distress syndrome (ARDS) and acute exacerbation of idiopathic pulmonary fi brosis is acute-onset respiratory failure. It is characterized by excessive neutrophil infi ltration into the alveolus, and great amounts of neutrophil elastase are released. The purpose of this study is to evaluate whether NETs are produced in bronchial aspirate of patients with ARDS or acute exacerbation of idiopathic pulmonary fi brosis, and to identify correlations with the respiratory function. Methods This was a prospective observational study of seven patients admitted to the ICU of a large urban tertiary referral hospital. All patients were mechanically ventilated at the time of admission. The bronchial aspirates were collected serially from the patients by suction through a tracheal tube. To identify NETs, extracellular components including DNA, histone H3, and citrullinated histone H3 were simultaneously detected using immunohistochemistry. The respiratory function was evaluated by PaO 2 /FiO 2 ratio (P/F ratio). Results The study group was comprised of four men and three women. Median age was 69 (IQR: 61.0 to 72.5) years old. The reason for admission was ARDS (n = 5) and acute exacerbation of idiopathic pulmonary fi brosis (n = 2). The reasons for ARDS are bacterial pneumonia (n = 3), necrotizing fasciitis (n = 1) and extensive burn (n = 1). We identifi ed NETs in the bronchial aspirates of all the patients. NET formation had persisted in four cases during the study period, their P/F ratio did not improve and all patients were dead due to respiratory failure. On the other hand, NETs decreased and vanished in three cases, their P/F ratio improved and all patients recovered from ARDS. Conclusion NET formation was observed in bronchial aspirates of all the patients diagnosed with ARDS or acute exacerbation of idiopathic pulmonary fi brosis. It may be one of the prognostic factors of ARDS or acute exacerbation of idiopathic pulmonary fi brosis.
New molecules controlling endothelial barrier S Jalkanen University of Turku, Finland Critical Care 2015, 19(Suppl 1):P234 (doi: 10.1186/cc14314) Introduction Acute lung injury (ALI) and its hypoxemic form, acute respiratory distress syndrome (ARDS), has no approved pharma cological treatment and is a condition with high mortality. Vascular leakage is one of the early events of ALI/ARDS. CD73 activity (ecto-5'-nucleotidase) maintains the endothelial barrier function of lung capillaries via its enzymatic endproduct adenosine. Interferon-beta (IFNβ) increases synthesis of CD73 and has been eff ective in a mouse model of ALI [1] . Methods Therefore we conducted a phase I/II trial [2] , in which intravenously administered human recombinant IFNβ1a (FP-1201) was used in the study, which consisted of dose escalation (phase I) and expansion (phase II) parts to test the FP-1201 safety, tolerability and effi cacy in ALI/ARDS patients. CD73, MxA (a marker for IFN β response) and other biomarkers were measured to follow pharmacokinetics/ dynamics of the intravenously administered FP-1201 and therapeutic effi cacy. Results The optimal tolerated dose of FP-1201 (10 μg/day for 6 days) resulted in maximal MxA stimulation. Also soluble CD73 values increased, while IL-6 decreased in sera of the FP-1201-treated ALI/ARDS patients. The overall mortality of the 37 patients treated with FP-1201
Introduction Mesenchymal stem cells (MSCs) have potent stabilizing eff ects on the vascular endothelium injury, inhibiting endothelial permeability in lung injury via paracrine hepatocyte growth factor (HGF). Recently, it has been indicated that MSCs secreted more factors by MSC-endothelial cell (MSC-EC) interaction. We hypothesized that MSC-EC interaction restored endothelial permeability induced by lipopolysaccharide (LPS) via paracrine HGF. Methods We investigated the endothelial permeability induced by LPS under two co-culture conditions in transwells. HPMECs were added into the upper chambers of cell-culture inserts, while there two diff erent co-culture conditions in the lower side of transwells as follows: MSC-EC interaction group: MSCs and HPMEC contact coculture in the lower chambers; and MSC groups: MSCs only in the lower chambers. The endothelial permeability in the upper side of transwells was detected. Then the concentration of HGF was measured in the culture medium using an enzyme-linked immunosorbent assay kit, following by neutralizing HGF with anti-HGF antibody in the co-culture medium. In addition, VE-cadherin protein expression were measured under the co-culture conditions by western blot, adherens junctions (AJs) protein including F-actin and VE-cadherin were detected by immunofl uorescence technique as well. Results The permeability signifi cantly increased after LPS stimulation in a dose-dependent and time-dependent manner (P <0.01). Meanwhile, MSC-EC interaction more signifi cantly decreased endothelial permeability induced by LPS (P <0.05 or P <0.01). Moreover, HGF levels in the MSC-EC interaction group were much higher than those of the MSC group (P <0.01). However, neutralizing HGF with anti-HGF antibody inhibited the role of MSC-EC interaction in improving endothelial permeability (P <0.05). Compared with the MSC group, MSC-EC interaction increased VE-cadherin protein expression (P <0.01), and restored remodeling of F-actin and junctional localization of VEcadherin. However, the MSC eff ect was signifi cantly blocked by anti-HGF antibody (P <0.05 or P <0.01). Conclusion These data suggest that MSC-EC interaction decreased endothelial permeability induced by LPS, which was mainly attributed to HGF secreted by hMSCs. The main mechanisms of HGF restoring the integrity of EC monolayers are remodeling of endothelial intercellular AJs and decreasing caveolin-1 protein expression. is a transmembrane receptor expressed in the lung and primarily located on alveolar type I cells. RAGE is implicated in acute respiratory distress syndrome to alveolar infl ammation and, when its soluble form sRAGE is assayed in plasma or pulmonary edema fl uid, as a marker of AT I cell injury. Functional activity of AT 1 cells can be assessed by the measurement of the alveolar fl uid clearance (AFC) rate [1] , but the relationship between sRAGE plasma levels of sRAGE and AFC rates has never been investigated. Our objectives were to report plasma levels of sRAGE in a mouse model of direct acid-induced epithelial injury, and to test their correlation with AFC rates. Methods Forty-one CD-1 mice were divided into two groups: an HCl group underwent orotracheal instillation of hydrochloric acid on day 0, and a group control. Mice were evaluated on days 0, 1, 2 and 4 after a 30-minute period of mechanical ventilation. Blood and lung edema fl uid (EF) were sampled. Before initiation of MV, all mice received a tracheal instillation of bovine serum albumin (5%) to detect changes in alveolar protein levels over 30 minutes. Plasma levels of sRAGE and total protein levels were measured. AFC rate values were corrected after measurement of mouse serum albumin in EF. Results Basal AFC rate was 35% over 30 minutes in HCl-injured mice, but it was signifi cantly depressed on day 1 (16% over 30 minutes; P = 0.02). Over time, AFC reached basal levels again. Plasma levels of sRAGE were higher in HCl-treated animals than in control animals on day 1 (P = 0.03) and day 2 (P = 0.02). The rate of AFC was inversely correlated with sRAGE levels in the plasma (Spearman's ρ = -0.73, P <0.001). See Figure 1 . Conclusion The highest impairment in AFC is reported on day 1. sRAGE levels are also higher in injured mice and may be a good surrogate marker of AT I cell injury. This is a newly described relationship between AFC rates and sRAGE plasma level in a mouse model of direct epithelial injury. Our results support further translational investigation on the role of RAGE in alveolar injury and recovery. Reference Introduction Cigarette smoking slowly and progressively damages the respiratory system [1] . In surgical critically ill patients, whether active cigarette smoking until admission to the surgical intensive care unit (SICU) is associated with increased risk of acute respiratory distress syndrome (ARDS) is not clearly identifi ed. Methods We conducted a cohort study using the THAI-Surgical Intensive Care Unit (THAI-SICU) study databases [2] , which recruited 4,652 Thai patients admitted to the SICUs from nine university-based hospitals in Thailand (April 2011 to November 2012). The enrolled patients were divided into three groups (active smokers, exsmokers, and nonsmokers). Primary outcome was the incidence of patients diagnosed with ARDS and the secondary outcomes included 28-day mortality, incidence of systemic infl ammatory response syndrome (SIRS), SICU length of stay (LOS), and total SICU cost. Results Of those 4,652 patients, there were 2,947 nonsmokers, 1,148 exsmokers, and 557 active smokers. There was no diff erence of APACHE II score between three groups of patients. The active smokers exhibited the highest incidence of ARDS (active smokers 5.4%, exsmokers 4.8%, and nonsmokers 3%, P = 0.003). There was no diff erence of 28-day mortality between the three groups of patients. Active smokers had the highest incidence of SIRS (active smokers 41%, exsmokers 37%, and nonsmokers 34%, P = 0.006). Compared with nonsmokers and exsmokers, active smokers had a longer SICU LOS (P <0.01) and higher total SICU cost (P = 0.02). Patients who smoked more than 15 packyears were 2.5 times more likely to develop ARDS than patients who smoked ≤15 pack-years (95% CI: 1.65 to 3.66, P <0.001). In multivariate analysis we found that every 1 pack-year of cigarette smoking before admission to the SICU is associated with increased risk of new ARDS with a hazard ratio of 1.02 (95% CI: 1.01 to 1.02, P = 0.001) after adjustment for APACHE II score, age, gender, and chronic obstructive pulmonary disease. Conclusion In surgical critically ill patients, active smokers are associated with increased risk of new ARDS, longer SICU LOS, and higher total ICU cost, compared with exsmokers and nonsmokers. Our fi ndings emphasize the essential need for a smoking cessation program. References Introduction Traditionally, ARDS obese patients are ventilated with higher tidal volumes and higher PEEP due to expected increased in pleural pressure. However, data from the literature regarding the infl uence of body mass on lung mechanics and, particularly, on chest wall elastance are not univocal [1, 2] . Failure to account for how the increase in body weight could aff ect the respiratory function can result in injurious mechanical ventilation and in the onset of VILI. The aim of this study was to evaluate the role of the body weight on respiratory mechanics in ARDS patients. Methods A group of deeply sedated and paralyzed ARDS patients was divided into three classes according to the body mass index: normal weight (between 18.5 and 24.9 kg/m 2 ), overweight (between 25.0 and 29.9 kg/m 2 ) and obese (between 30.0 and 39.9 kg/m 2 ). They were mechanically ventilated in volume-controlled mode with a tidal volume of 6 to 8 ml/kg according to predicted body weight. Respiratory mechanics and gas exchange were assessed at PEEP levels of 5 and 15 cmH 2 O. Results A total of 101 ARDS patients was enrolled; 44 (43.6%) patients were normal weight, 36 (35.6%) overweight and 21 (20.8%) obese. Lung and chest wall elastance were not diff erent between groups, both at PEEP levels of 5 cmH 2 O and 15 cmH 2 O (P = 0.580 and P = 0.113, respectively), and the end-inspiratory transpulmonary pressure was also similar. We did not observe any diff erence between groups regarding PaO 2 /FiO 2 ratio (P = 0.178 at PEEP 5 cmH 2 O; P = 0.073 at PEEP 15 cmH 2 O) and PaCO 2 (P = 0.491 at PEEP 5 cmH 2 O; P = 0.237 at PEEP 15 cmH 2 O). Conclusion In ARDS obese patients the chest wall elastance and the end-inspiratory transpulmonary pressure are not aff ected by the body weight, suggesting that normal weight and obese patients present similar risks for lung stress and VILI onset. The severity of hypoxemia was not diff erent between groups. Methods The observational study in ICU ventilated septic patients with peritonitis (70%), pancreonecrosis (25%) and mediastinitis (5%) was done in 2010 and 2014. ARDS was diagnosed and staged according to the V.A. Negovsky Research Institute criteria and the Berlin defi nition. Plasma SPD was measured on ARDS diagnosis (day 0) and days 3 and 5 by the immunoenzyme essay (BioVendor, USA). Patients were treated according to the international guidelines. Data were statistically analyzed by STATISTICA 7.0, ANOVA and presented as median and 25 to 75th percentiles (ng/ml); P <0.05 was considered statistically signifi cant. Areas under the receiver operating curves were calculated. Results Sixty-fi ve patients (out of 450 screened) were enrolled in the study according to the inclusion/exclusion criteria. Patients were assigned into groups: NP + ARDS (n = 43, 43 ± 4.9 years old, M/F 39/4, mortality 23%) and NP (n = 22, 40 ± 5.1 years old, M/F 20/2, mortality 18%). Groups were comparable in APACHE II and SOFA scores on the baseline. In the NP + ARDS group SPD was higher at all points than in the NP group. Plasma SPD on day 0 >111. 2 
The soluble form of the receptor for advanced glycation endproducts (sRAGE) is a promising marker for epithelial dysfunction, but it has not been fully characterized as a biomarker during ARDS. Whether sRAGE could inform on the response to ventilator settings has been poorly investigated, and whether recruitment maneuver (RM) may infl uence plasma sRAGE remains unknown. Methods Twenty-four patients with moderate/severe, nonfocal ARDS were enrolled in this prospective monocentric crossover study and randomized into a 'RM-SHAM' group when a 6-hour-long RM sequence preceded a 6-hour-long sham evaluation period, or a 'SHAM-RM' group (inverted sequences). Protective ventilation was applied, and RM consisted of the application of 40 cmH 2 O airway pressure for 40 seconds. Arterial blood was sampled for gas analyses and sRAGE measurements, 5 minutes pre RM (or 40-second-long sham period), 5 minutes, 30 minutes, 1 hour, 4 hours and 6 hours after the RM (or 40-second-long sham period). Results Mean PaO 2 /FiO 2 , tidal volume, PEEP and plateau pressure were 125 mmHg, 6.8 ml/kg (ideal body weight), 13 and 26 cmH 2 O, respectively. Median baseline plasma sRAGE levels were 3,232 pg/ml. RM induced a signifi cant decrease in sRAGE (-1,598 ± 859 pg/ml) in 1 hour (P = 0.043). At 4 and 6 hours post RM, sRAGE levels increased back toward baseline values. Pre-RM sRAGE was associated with RMinduced oxygenation improvement (AUC 0.87). See Figure 1 . Conclusion We report the fi rst kinetics study of plasma sRAGE after RM in ARDS. Our fi ndings could help to design future studies of sRAGE as a marker of response to therapeutic interventions during ARDS. Introduction Oesophageal pressure is increasingly used to monitor and manage mechanically ventilated patients. Even if the oesophageal balloon catheter is correctly positioned, the measurement can be aff ected by inappropriate balloon fi lling and/or oesophageal reaction to balloon infl ation. We aimed to assess the oesophageal reaction to oesophageal balloon fi lling in mechanically ventilated patients. Methods An oesophageal balloon catheter (NutriVent; Sidam, Mirandola, Italy) was introduced in mid/distal thoracic position in 31 patients under invasive mechanical ventilation for acute respiratory failure. At ambient pressure, the balloon of the NutriVent catheter can be infl ated up to 6 ml without generation of recoil pressure. The balloon was progressively infl ated in 0.5 ml steps up to 9 ml and end-expiratory values of balloon pressure were used to assemble the balloon pressure-volume curve. The minimum slope section of the curve was graphically detected and infl ation volumes corresponding to this part of the curve were considered appropriate. Overdistension of the balloon being excluded by defi nition in this section of the curve, its slope was attributed to the oesophageal reaction to balloon infl ation. Results Forty-fi ve oesophageal balloon pressure-volume curves were obtained in 31 patients undergoing controlled mechanical ventilation (PEEP 12 ± 5 cmH 2 O, FiO 2 0.7 ± 0.2, tidal volume/ideal body weight 8.0 ± 1.6 ml/kg). According to the graphically detected minimum slope section of the curve, the minimum and maximum appropriate balloon volumes were 1.5 ± 0.6 ml and 5.3 ± 0.9 ml, respectively. Between these two volumes, the slope of the curve was 1.1 ± 0.5 cmH 2 O/ml, ranging from 0.3 to 3.1 cmH 2 O/ml. Conclusion The oesophageal artefact -that is, the reaction of the oesophageal wall to balloon infl ation -may be clinically signifi cant, being on average 1 cmH 2 O for each millilitre of volume injected in the catheter, but reaching values as high as 3 cmH 2 O/ml. The pressure generated by the oesophageal reaction leads to overestimation of pleural pressure. Therefore, the oesophageal artefact may signifi cantly aff ect clinical decision-making based on absolute values of oesophageal pressure. Critical Care 2015, Volume 19 Suppl 1 http://ccforum.com/supplements/19/S1 S84 demonstrated recently its eff ectiveness on prognosis. Extrapulmonary etiologies of ARDS include abdominal emergencies. In cases of severe hypoxemia in the early postoperative period, intensivists discuss prone positioning based on the risk/benefi t ratio. Methods We conducted a retrospective two-center study of 5 years. The aim was to compare the prevalence of surgical complication potentially related to prone positioning between patients who had at least one prone positioning session and patients that remained in a supine position after abdominal surgery. Patients with ARDS in a context of recent (<7 days) abdominal surgery (except laparoscopy) were included. The primary outcome was the number of patients who had at least one surgical complication potentially related to prone positioning. We defi ned a priori these complications: scar dehiscence, abdominal compartment syndrome, stoma leakage, stoma necrosis, scar necrosis, wound infection, displacing of a drainage system, removal of gastro-or jejunostomy feeding, digestive fi stula, evisceration. Results We identifi ed 43 patients with postoperative ARDS (62 ± 8 years, SAPS II 50 ± 13), among whom 34 (79%) had emergent surgery. Fifteen patients had at least one stoma after surgery. Nineteen patients (44%) had at least one prone positioning session (number of sessions: 2 (1 to 3)). At baseline, prone group patients had minimum PaO 2 /FiO 2 ratio lower than the supine group (77 ± 23 vs. 110 ± 46 mmHg, P = 0.005). Plateau pressure was higher in the prone group (28 ± 4 vs. 23 ± 5 cmH 2 O, P = 0.002). The fi rst prone positioning session signifi cantly increased the PaO 2 /FiO 2 ratio: 106 ± 52 vs. 192 ± 90 mmHg (P = 0.001). Mean duration of the fi rst prone positioning session was 20 ± 10 hours. In the prone group, 11 patients (58%) had at least one surgical complication, in comparison with nine (38%) in the supine group (P = 0.2). These complications resulted in revision surgery for two (10%) patients in the prone group and two (8%) in the supine group (P = 0.8). Mortality in the ICU was respectively 42% and 38% in prone group and supine group (P = 0.8). Conclusion These preliminary results confi rm the eff ectiveness of prone positioning in terms of oxygenation in ARDS after abdominal surgery without signifi cant increase in surgical complications and no eff ect on the need for surgical revisions. Hence, if necessary, clinicians should not refrain from proning patients with postabdominal surgery ARDS. Introduction ARDS is characterized by edema diff use to all lung fi elds. Distribution of excess tissue mass had been studied with CT scan in a few patients on a single slice, comparing with data obtained in healthy controls. Methods ARDS patients underwent CT scan imaging during their ICU stay at 45 cmH 2 O end-inspiratory pressure. After hospital discharge, patients underwent a follow-up CT scan performed at end inspiration. Each lung was divided into three sections along the apex-base axis and into three sections along the sternum-vertebral axis (nine regions per lung). Excess tissue mass in each lung region was defi ned as the diff erence in lung tissue (grams) between the CT scan performed during ARDS course and the follow-up CT scan. Results are presented as mean ± SD. Results We studied eight ARDS patients (55 ± 18 years) with a BMI of 27 ± 6 kg/m 2 . At ICU admission, patients had the following clinical parameters: PaO 2 /FiO 2 106 ± 33 with PEEP 15 ± 5 cmH 2 O; PaCO 2 43 ± 10 mmHg; pH 7.35 ± 0.05. The average increase in lung weight during ARDS compared with follow-up CT scan was 68 ± 40% (680 ± 320 g). Figure 1 presents the tissue volume during ARDS (white bars) and after ARDS resolution (black bars) and compares the ratio between the two (*P <0.01 vs. dependent region). Conclusion The excess tissue mass was not diff erent between apex, hilum and base, but was increased in the dependent lung regions at apex and hilum, being uniformly distributed at the lung base. Introduction ARDS has a wide variability of lung morphological characteristics. Both alveolar collapse and airway narrowing or closing are present, often heterogeneously. Despite advances in ARDS imaging, we have thus far been unable to distinguish regional airway opening from airway dilatation in PEEP-induced lung recruitment. We demonstrate the technique of functional respiratory imaging (FRI) to diff erentiate these two entities. Methods Six patients with early-stage ARDS were included in this prospective single-centre cohort trial. The lower infl iction point on a pressure-volume curve was considered as the clinically acceptable minimal PEEP value. Subsequently, four distinct PEEP levels were chosen to perform CT scans: at 20 cmH 2 O; median value between 1 and 3; clinically acceptable minimal; and 0 cmH 2 O. FRI methods as described by De Backer and colleagues [1] were used to evaluate airway opening and airway dilatation. Results Airway stretching (that is, bronchodilatation) could be quantifi ed and distinguished from airway recruitment with this technique. Higher PEEP pressures not only recruit, but also expand the bronchi. The ratio of dilation/recruitment of bronchi was higher in the upper lobes than in the lower lobes, as illustrated in Figure 1 . We were able to phenotype each patient, allowing a prediction on when an increase in PEEP further recruits atelectasis/bronchi or distends certain airway regions. Conclusion The novel technique of FRI can be used to visualise the airway structures in ARDS and distinguish airway stretching from Critical Care 2015, Volume 19 Suppl 1 http://ccforum.com/supplements/19/S1 S85 airway recruitment. This pilot study shows that, in ARDS, the upper lung regions are subject to airway dilation, whereas the lower (atelectatic) lung lobes have more airway opening with higher PEEP levels. Introduction Mechanical ventilation during anesthesia leads to the development of atelectasis, poor oxygenation and postoperative pulmonary complications. Application of PEEP and recruitment maneuver (RM) can signifi cantly reduce the severity of atelectasis and improve lung function. But the application of this strategy often leads to hemodynamic instability, which may be associated with impaired reactivity of the cardiovascular system. The purpose of this study was to evaluate the effi cacy and safety of RM in patients with increased sensitivity of peripheral chemoreceptors (SPCR), which refl ects the decreasing reactivity of the cardiovascular system. Methods We conducted a prospective study in 116 patients with high SPCR, evaluated using the breath-holding test. The test was performed by measuring of voluntary breath-holding duration (BHD) after twothirds of maximal inspiration. The end of breath-hold was determined by a palpation of contraction of the diaphragm. BHD <38 seconds was the marker of high SPCR [1] . All patients received a major abdominal surgery and were randomized into an open lung ventilation group or a PEEP group. The concept of open lung ventilation was performed as follows: PEEP was increased from 4 to 10 cmH 2 O for three breaths, from 10 to 15 cmH 2 O for three breaths, and from 15 to 20 cmH 2 O for 10 breaths [2] . Then PEEP was reduced to 12 cmH 2 O. This RM was repeated every hour. In the PEEP group PEEP was maintained at 12 cmH 2 O during the whole anesthesia. Hemodynamics, blood gases and dynamic compliance were evaluated. Results RM improved oxygenation compared with the PEEP group. The mean increase in the oxygenation index at the end of surgery was 31% (from 340 to 445 mmHg, P <0.05), in the PEEP group the increase was less signifi cant and amounted to 12% (from 330 to 370 mmHg, P <0.05). Dynamic compliance increased by 35% in the RM group and did not change in the PEEP group. Hemodynamic changes at RM were more pronounced. So CI on average decreased by 34% (from 3.7 to 2.5 l/minute/m 2 ) compared with 10% with no RM (P <0.05), and SVR decreased by 19% (from 1,310 to 1,150 dyn × sec -1 × cm -5 , P <0.05), while in the PEEP group it did not change. No signifi cant diff erences between groups in the incidence of complications, length of stay in the ICU and in the hospital were noted. Conclusion RM patients with high SPCR and with reduced reactivity of the cardiovascular system improve lung function, but this is associated with the risk of hemodynamic instability. 
Mechanical ventilation performs at each breath mechanical work on the lung parenchyma. Part of this energy is recovered and part is dissipated into the respiratory system. The purpose of this study is to assess the respective role of three known determinants of energy dissipation: lung opening and closing, strain and lung inhomogeneities [1] . Methods Thirteen female piglets (21 ± 2 kg) were ventilated with a strain (V T /FRC) greater than 2.5 (V T ~ 38 ± 3 ml/kg) for 54 hours or until massive lung edema developed. Piglets were divided into fi ve groups characterized by diff erent energy loads obtained varying the respiratory rate: 15 breaths/minute (n = 3), 12 (n = 3), 9 (n = 3), 6 (n = 2) and 3 (n = 2). Every 6 hours two CT scans were performed (end-expiration and endinspiration) and a static pressure-volume curve was obtained. A total of 51 CT scan couples and 51 corresponding pressure-volume curves was analyzed. The energy dissipated in the lung parenchyma at each breath was determined as the hysteresis area of the pressure-volume curve. CT scans were quantitatively analyzed with dedicated software. Data are presented as median (interquartile range). Results Piglets ventilated with higher energy loads (RR 15 and 12) developed ventilator-induced lung injury (VILI), while piglets ventilated with lower energy loads (RR 9, 6 and 3) did not. The energy dissipated in the lung parenchyma at each breath was 0.56 J (0.52 to 0.62). Dissipated energy increased with time in piglets that developed VILI, while it remained near-constant in all the other piglets. Recruitability was 6% (3 to 8) of lung parenchyma, strain was 3.1 (2.6 to 3.5) and lung inhomogeneity extent (that is, the percentage of lung parenchyma that is inhomogeneous) was 10% (9 to 11). The energy dissipated in the lung parenchyma was well related to lung recruitability (r 2 = 0.50, P <0.0001), strain (r 2 = 0.57, P <0.0001) and lung inhomogeneity extent (r 2 = 0.42, P <0.0001). Multiple linear regression showed that dissipated energy was independently related to all of the three determinants of energy dissipation: lung opening and closing (P = 0.025), strain (P <0.0001) and lung inhomogeneities (P <0.01). Conclusion Energy dissipation was largely dependent on strain, obtained with very high tidal volumes, but also lung inhomogeneities and lung opening and closing played a signifi cant role. Reference aim of this preclinical study was to study both time-dependent and dose-dependent eff ects of supplemental oxygen during prolonged ventilatory support on pulmonary infl ammation in a well-established murine model of ventilation comparing low and high tidal volumes. Methods Healthy male C57Bl/6J mice, aged 9 to 10 weeks, were randomly assigned to experimental groups (n = 8), in which the applied fractions of oxygen (FiO 2 ) were 30%, 50% or 90% and tidal volumes were either 7.5 or 15 ml/kg. Anesthetized mice were tracheotomized and ventilated for 8 or 12 hours. Infl ammatory cells and mediators were measured in bronchoalveolar lavage fl uid (BALf ). Results Mice exposed to higher FiO 2 had signifi cantly higher PaO 2 levels at the end of the experiment. The total number of infl ammatory cell in the BALf was not signifi cantly diff erent between the experimental groups (P = 0.28), yet an increasing trend in the percentage of neutrophils was observed with increasing FiO 2 (P = 0.03). Cytokine and chemokine levels did not diff er between FiO 2 groups at 8 hours of ventilation. In mice ventilated for 12 hours, a signifi cantly increasing trend in IFNγ, IL-1β, IL-10, MCP-1 and TNFα ( Fig. 1, P <0 .01) was measured with increasing FiO 2 , whereas IL-6, KC, MIP-2, GM-CSF and VEGF remained virtually unchanged. Diff erences between the tidal volume groups were small and did not markedly infl uence the eff ects of hyperoxia. See Figure 1 . Conclusion Hyperoxia induced a time-dependent and diff erentiated immune response that was independent of tidal volumes in a model of mechanically ventilated mice. The presence of cytokines and chemokines in the pulmonary compartment was more pronounced with prolonged and severe hyperoxia. Introduction Electrical impedance tomography (EIT) is a functional imaging technology allowing one to regionally monitor aeration of the lungs. We used EIT with increased signal quality and spatial resolution to describe and quantify the regional changes in aeration caused by body position, both during spontaneous breathing and mechanical ventilation in pulmonary healthy patients undergoing laparoscopic prostatectomy. Methods In 40 patients we performed EIT measurements at fi ve points of time (Table 1) with the Swisstom BB2 prototype. Thirty-two electrodes were used to apply weak alternating currents to the thorax and to measure the resulting voltages, from which tomographic images of the changes in regional impedance caused by ventilation were created. We describe the ventilation distribution using a novel EIT lung function parameter called Silent Spaces that provides information about areas that do not receive much or any air during tidal breathing and are divided into nondependent (NSS) and dependent Silent Spaces (DSS) using a reference line that runs perpendicular to the gravity vector right through the centre of ventilation. NSS and DSS are expressed as a percentage of the total lung area. Results Perioperative changes of NSS and DSS are shown in Table 1 as mean ± SD. Statistically signifi cant diff erences marked by § when compared with 1 or by * when compared with 3 (P <0.05). Conclusion We describe for the fi rst time the mapping of Silent Spaces during spontaneous breathing and changing ventilation conditions and body positions in patients with healthy lungs using EIT. This mapping of Silent Spaces might prove useful for developing perioperative protective ventilation strategies. 
The still unclear mechanisms causing ventilatorinduced diaphragm dysfunction (VIDD) are considered intrinsic to the diaphragm muscle fi bers. VIDD delays and complicates weaning from mechanical ventilation (MV) and accordingly contributes to prolonged ICU stay by 50%, with older patients being more aff ected than the young. The main aim of this study was to measure the eff ects of aging and 5 days of MV on rat diaphragm muscle fi ber structure and function. We also aimed to investigate the biological age of the old rats to obtain data useful to design future experimental studies focusing on the eff ects of age in an ICU setting. Methods We used a unique ICU rat model, which allows us to maintain the vital parameters stable under deep sedation and MV for long durations (several weeks). Diaphragm fi ber cross-sectional area (CSA) and force-generating capacity (specifi c force = absolute force / CSA) were measured in young (6 months) and old (28 to 32 months) F344/ BN hybrid rats in response to 5 days of deep sedation and volumecontrolled MV. To investigate the biological age of the old rats, we performed a second set of experiments, comparing muscle fi ber CSA and specifi c force in fast and slow-twitch distal hind limb muscles in three diff erent age groups: young adults (6 months), middle aged (18 months) and old rats (28 months). Results This study demonstrated an unexpected increase in CSA (P <0.001) of the diaphragm fi bers in response to 5 days of MV in both young and old animals. Maximum force decreased 39.8 to 45.2% (P <0.001) in both young and old animals compared with controls, resulting in a dramatic loss of specifi c force. This increase in CSA and the concomitant decrease in specifi c force observed in both young and old diaphragm fi bers are compatible with an ineff ective compensatory hypertrophy in response to the MV. The comparison of the limb muscles Results Twenty-eight patients were included in G1 and 22 patients in G2. There was no signifi cant diff erence between the patients' admission parameters and daily NIV usage times. PaCO 2 decreased >5 mmHg in 93% of G1 patients and in 60% of G2 patients in the fi rst 6 hours (P = 0.044). A 10 mmHg reduction in PaCO 2 occurred in more patients (93% vs. 60%, P = 0.004) and in a shorter time (1.8 ± 1.2 vs. 3 ± 3 days, P = 0.044) in G1. At the time of discharge, PaCO 2 levels were <50 mmHg in 79% of G1 and 41% of G2 patients (P = 0.006). Both groups showed similar and signifi cant improvements in PaO 2 , PaCO 2 and HCO 3 levels within the fi rst 4 days but only in G1 patients were HCO 3 levels decreased more rapidly than G2 patients (P = 0.007). Duration of NIV (6 ± 2 vs. 8 ± 3 days, P = 0.002) and the number of mode and pressures changes (0.3 ± 1.8 vs. 2 ± 2 times, P >0.0001) were signifi cantly less in G1. While mean IPAP was similar in both groups, maximum and minimum EPAP titrated automatically in G1 were signifi cantly diff erent from G2. Mean tidal volume and amount of leakage were also signifi cantly higher in G1. Conclusion These results suggest that the AVAPS-AE mode may provide some advantages in hypercapnic ICU patients such as rapid PaCO 2 reduction, less NIV duration and workload. Methods All 89 inpatients on the CCU who received mechanical ventilation continuously for 7 days or more between October 2012 and December 2013 were initially included. Forty patients who were intubated prior to arrival at RCHT, had incomplete notes, or were extubated during end-of-life care were excluded. Patients were divided into groups by fi rst airway intervention; 31 TOE, 18 TR. Results A total 52% (16/31) of patients had TOE, required no other airway intervention and survived to discharge from hospital, compared with 72% (13/18) of TR patients. Four patients from each group failed the fi rst intervention and died prior to a second intervention. In total, 8/11 patients who had a second intervention after failed TOE survived to discharge from hospital. One patient had a second TR but died before discharge. This gave an in-hospital mortality rate of 19% for the TOE group and 28% for the TR group. TOE was performed earlier, all 31 on days 7 to 15. TR was performed later; 14/18 on days 7 to 15, and 4/18 on days 17 to 23. Early TR was more successful; 11/11 survived to discharge without a second intervention who had TR on days 7 to 12, compared with 29% (2/7) after day 12. TOE was more successful when performed later; 64% (7/11) survived to discharge without a second airway intervention when TOE was after day 10, 45% (9/20) between days 7 and 10. After fi rst failed TOE, four patients had a successful second TOE; all four survived to discharge resulting in a median CCU stay of 29 days and median hospital stay of 39 days (excluding prior to CCU admission). Seven patients had TR after the fi rst failed TOE, fi ve survived to discharge from the CCU and four to discharge from the hospital. This group had shorter median stays in both the CCU (27 days) and hospital (32 days). Overall, the median duration of time ventilated, in the CCU, and in hospital was shorter for the TOE group; 13, 17 and 24 days respectively, compared with 22, 27.5 and 34 days for the TR group. Conclusion TOE is more common and is associated with shorter time spent ventilated, in the CCU and in hospital than TR. It is also associated with a lower in-hospital mortality rate. TOE is more successful when performed after day 10; TR is more successful when performed before day 13. After failed TOE, a second TOE is associated with longer time in hospital but a better mortality rate than secondary tracheostomy. Introduction Bilevel non-invasive ventilation (NIV) is an established therapy in chronic obstructive pulmonary disease (COPD) but confl icting evidence exists for its use in patients with pneumonia. Initial arterial pH <7.25 is used as a marker of severity and need for admission to critical care (CC) [1] . We examined the impact of pH and condition on outcome in patients with acute respiratory failure (ARF) of mixed aetiology treated with NIV. Methods Data were collected retrospectively for a 5-year period from 2008 to 2013 using the Metavision electronic patient record. We identifi ed all patients admitted with ARF treated with bilevel NIV. Patients who received continuous positive airway pressure or had a primary surgical problem were excluded. We recorded primary cause of respiratory failure, arterial blood gas values and mortality. Results A total of 145 patients were identifi ed. Mean age was 64 and 51% were male. The primary diagnosis was pneumonia in 69 patients and exacerbation of COPD in 57. The overall mortality was 19% on CC and 39% at 1 year. In patients with COPD, infective exacerbations had a higher CC mortality (17%) compared with non-infective (0%). However, by 1 year the mortality was 28% in infective and 29% in non-infective. Patients with pneumonia had a higher mortality on CC (25%) and at 1 year (48%). Patients with an initial pH <7.25 were less likely to survive. The mortality at discharge from CC was 16% (pH ≥7.25) and 26% (pH <7.25) but narrowed to 38% and 39% by 1 year. When subdivided, it was found that patients with infective COPD and pH <7.25 had the lowest 1-year mortality (17%) while those with pneumonia and pH <7.25 had the highest mortality (67%). Conclusion NIV is used in our unit with comparable success rates to published series [2, 3] . COPD patients responded well to NIV, while Critical Care 2015, Volume 19 Suppl 1 http://ccforum.com/supplements/19/S1 S88 patients with pneumonia treated with NIV have the highest mortality. A low presenting pH is associated with a higher mortality in patients with pneumonia treated with NIV. However, in COPD patients, pH <7.25 is not associated with higher mortality in CC or at 1 year. Further work defi ning the precise role of pH as a prognostic indicator is warranted. References Introduction A large meta-analysis suggests that use of low tidal volumes benefi ts patients without ARDS [1] but most studies in this meta-analysis included patients receiving ventilation during general anesthesia for surgery. The aim of the present meta-analysis is to determine the association between tidal volume size and development of pulmonary complications in ICU patients. Methods An individual patient data meta-analysis of studies of ventilation in ICU patients without ARDS. Corresponding authors of retrieved studies provided individual patient data. The primary outcome, pulmonary complications, was a composite of development of ARDS or pneumonia during hospital stay. Secondary outcomes included ICU and hospital length of stay, and in-hospital mortality. Patients were assigned to three groups based on tidal volume size (≤7 ml/kg predicted body weight (PBW), 7 to 10 ml/kg PBW, or ≥10 ml/ kg PBW). Results Seven investigations (2,184 patients) were meta-analyzed. Pulmonary complications occurred in 23%, 28% and 31% respectively in the ≤7 ml/kg PBW, 7 to 10 ml/kg PBW and ≥10 ml/kg PBW group (adjusted RR, 0.72; 95% CI, 0.52 to 0.98; P = 0.042). Occurrence of pulmonary complications was associated with a lower number of ICUfree days and alive at day 28, a lower number of hospital-free days and alive at day 28 and increased in-hospital mortality. Conclusion Ventilation with low tidal volumes is associated with a lower risk of development of pulmonary complications. Occurrence of pulmonary complications is associated with an increased ICU and hospital length of stay and in-hospital mortality in ICU patients without ARDS. Reference 
The combination of a global demographic shift and increased survival following critical illness has led to an increasing number of patients requiring prolonged mechanical ventilation (PMV) and longer critical care stay. This is a prospective observational study evaluating the characteristics and speciality-based outcome of critically ill patients undergoing prolonged mechanical ventilation in the North of England Critical Care Network (NoECCN). Methods A weekly survey was conducted over a 1-year period screening patients older than 16 years of age requiring PMV in all 18 adult critical care units within the NoECCN. Patient data collected included patient demographics, admission diagnosis and speciality, hospital length of stay (LOS) pre and post critical care admission, severity of illness scores, critical care LOS and status at hospital discharge. Results During the study period 134 patients met the criteria for PMV representing 1% of annual admissions and 6.9% NoECCN beddays. The majority of patients receiving PMV were medical (50.7%), followed by emergency surgery (20.1%), elective surgery (16.4%) and specialist services such as spinal cord injury (8.2%) and cardiothoracic transplant (4.5%). The commonest admission diagnosis in the medical population was pulmonary infection followed by acute neurological disorders, while 89.4% of surgical patients were admitted to critical care during the perioperative period. At the end of the study period the highest hospital mortality was observed in the nonspecialist surgical population (26.5%). In contrast, the medical population had one of the lowest hospital mortality rates (11.8%), lower than predicted using the intensive care national audit research network illness severity score. Comparable rates of hospital discharge were found in both medical (85%) and nonspecialist surgical patients (88.9%).
Conclusion The results of this study highlight an expanding proportion of NoECCN critical care bed-days occupied by stable patients undergoing PMV. In keeping with published UK data, elevated hospital mortality was observed in the nonspecialist surgical subpopulation. Although the literature suggests the medical cohort of patients has poorer prognosis, within our region all were liberated from mechanical ventilation and over 80% were discharged from hospital. Reference Introduction Estimating respiratory mechanics of mechanically venti lated patients is unreliable when patients exhibit spontaneous breathing (SB) eff orts on top of ventilator support. This reverse triggering eff ect [1] results in an M-wave shaped pressure wave. A model-based method to reconstruct the aff ected airway pressure curve is presented to enable estimation of the true underlying respiratory mechanics of these patients. Methods Airway pressure and fl ow data from 72 breaths of a pneumonia patient were used for proof of concept. A pressure wave reconstruction method fi lls parts of the missing area caused by SB eff orts and reverse triggering by connecting the peak pressure and end-inspiration slope (Figure 1) . A time-varying elastance model [2] was then used to identify underlying respiratory elastance (AUCE drs ). Introduction Model-based respiratory mechanics can be used to guide mechanical ventilation therapy. However, identifi ed mechanical properties vary breath to breath, leading to potential treatment errors when using model-based care that requires accuracy. This study investigates and quantifi es this variability to improve its application in guiding clinical interventions. Methods Retrospective data from 12 acute respiratory distress syndrome (ARDS) patients were used [1] . Each patient was sedated to prevent spontaneous breathing eff ort, and ventilated using the volume control mode with a square fl ow profi le. Varied PEEP levels were maintained for 30 minutes before 1 minute of data were collected for analysis. This dataset provides a wide range of respiratory mechanics values, and the clinical protocol detail is in [1] . A clinically proven, single compartment model respiratory system elastance (Ers) is identifi ed from data for every breathing cycle at each PEEP level using a linear regression method. The dynamic elastance (Edynamic = (peak airway pressure -positive end-expiratory pressure) / tidal volume) of the corresponding breathing cycle is calculated for comparison.
The coeffi cient of variation (CV) of identifi ed Ers across all patients was low (<0.005), as expected, as the 30-minute period allows time-dependent alveolar recruitment to fully occur and stabilise. However, even with substantial stabilisation periods, there remains a diff erence between the minimum and maximum estimated Ers within each 1-minute period of analysed data. Introduction Maintaining the appropriate tidal volume (VT) is important for success of lung protective ventilation. However, in neonates, the presence of airway leaks may increase the errors in the delivery of tiny VT, which raises a concern of ventilator-induced lung injury. This study is to investigate the accuracy of VT delivery during non-invasive ventilation (NIV) with modern neonatal ventilators. Methods Using a lung simulator for a patient body weight of 3 kg, we measured the actual delivered VT in the lung and compared it with the value displayed on the ventilator in six ventilators. We tested 18 conditions with various combinations of respiratory mechanics (normal, restrictive, obstructive), leak levels (0, 1.0, 1.5 l/minute), and PEEP settings (5, 10 cmH 2 O). All conditions were tested in NIV mode. The pressure level was set to achieve VT to the lung at 6 to 7 ml/kg. All other settings were: F I O 2 0.21, I time 0.6 seconds, f 25/minute, and default rise time. We calculated the mean errors of the ventilator-displayed VT at various levels of airway leak.
The VT mean error values are presented in Table 1 . When no leak existed, the mean error was less than 5% in all ventilators except one (C3) which showed a mean error of 26%. As the leak level increased, three ventilators (C3, G5, and VN500) showed marked diff erences between the delivered and displayed VT. In particular, the VN500 could not operate in the large leak condition. The other three ventilators (PB840, PB980, Servo i) showed acceptable VT accuracy across all conditions tested. Conclusion Tidal volume accuracy during neonatal NIV varies greatly among diff erent ventilators and leak conditions. This must be considered in neonatal ventilation management to avoid overventilation or underventilation. Methods 'Hats and caps' was devised on our ICU [3] and used for the training: this teaches that capnography traces on the left signify the airway is functional, in contrast to the traces on the right which indicate immediate attention is required (Figure 1 ). This was presented to staff working on the ICU in individual bedside teaching sessions with feedback obtained and evaluated. Introduction Weaning from mechanical ventilation is an important concern in ICU clinical practice. Surface electromyography (sEMG) [1] is a non-invasive tool to assess activity of diff erent muscles. We describe sEMG patterns of respiratory muscles during a CPAP trial [2] in patients undergoing pressure support ventilation.
Methods Twenty-one adult and clinically stable patients undergoing assisted mechanical ventilation for more than 48 hours were investigated during pressure support (baseline) and during a 2-hour CPAP trial. sEMG of diaphragm (costmar), intercostal and sternocleidal (accessory muscles) was recorded with a dedicated device (Dipha16; Inbiolab, Groningen, the Netherlands) simultaneously with airway waveforms and expressed as the ratio of the signal during baseline. Diaphragmatic electrical activity from a nasogastric tube (EAdi) of 14 of those patients was also measured.
The rapid shallow breathing index was lower than 105 in all patients and only one patient failed the trial. We observed that the mean inspiratory value of costmar increased immediately after switch to CPAP but did not signifi cantly vary during the CPAP trial (ANOVA, P = 0.7). On the other hand, the activation of accessory muscles increased signifi cantly during the same period (P = 0.01) and was strongly correlated with respiratory rate (r = 0.41, P <0.001) and inversely with Table 1 . Introduction The aim of this study was to determine the types, incidence, and risk factors for early postoperative pulmonary complications in heart transplantation recipients. Methods We retrospectively collected data from the records of consecutive heart transplantations from January 2003 to December 2013. A total of 83 patients underwent heart transplantation. Those patients younger than 10 years (n = 9) and the patients who died intraoperatively (n = 1) or during the fi rst postoperative day (n = 1) were not included in the analyses. The data collected for each case were demographic features, duration of mechanical ventilation, respiratory problems that developed during the ICU stay, and early postoperative mortality (<30 days). The respiratory complications that we sought were pleural eff usion, pneumonia, pulmonary atelectasis, pulmonary edema, pneumothorax, and acute respiratory failure. Methods We included 10 patients receiving iLA Activve® due to hypercapnic respiratory failure as bridge-to-transplant or obstructive lung disease. Sweep gas fl ow was increased in steps from 1 to 14 l/ minute at constant blood fl ow (phase 1). Similarly, blood fl ow was gradually increased at constant sweep gas fl ow (phase 2). At each step, gas transfer via the membrane as well as arterial blood gas samples were obtained. (VT) reduction from 6 to 4 ml/kg attenuates overdistension but is associated with hypercarbia [2] . We thought to combine extracorporeal CO 2 removal (ECCO 2 R) with continuous renal replacement therapy (CRRT) through the insertion of an oxygenator membrane within the hemofi ltration circuit in patients presenting both ARDS and acute kidney injury (AKI). Methods A fi rst set of measurement was performed at 6 ml/kg before and after ECCO 2 R. Twenty minutes later, VT was reduced to 4 ml/kg for the remainder of the study period (72 hours). Ventilator settings were those of the ARMA trial. The CRRT mode was hemofi ltration with 33% of predilution. Ultrafi ltration was adjusted to achieve a fi ltration fraction of 15%. Sweep gas fl ow was constant at 8 l/minute. The primary endpoint was a 20% reduction of PaCO 2 at 20 minutes after initiation of ECCO 2 R. Results Eight patients were studied. Age was 69 ± 11 years, SAPS II was 68 ± 9 and SOFA score was 13 ± 4 at inclusion. Blood fl ow, at the inlet of the oxygenator membrane, was 400 ± 4 ml/minute. CO 2 removal rate was 84 ± 4 ml/minute. Initiating ECCO 2 R, at 6 ml/kg, induced a mean PaCO 2 reduction of 17% (41 ± 5.5 to 33.9 ± 5.6 mmHg, P <0.001 [2] . In our experience, there were no complications during the transfer of ECMO patients, even for longer trips. A wide and thorough clinical evaluation and multidisciplinary ECMO team allowed the optimization of clinical parameters before transport and a safely transfer. The start of ECMO treatment at peripheral hospitals and the transfer of patients in ECMO may be a viable option compared with conventional ventilation. Our data suggest that ECMO can be set up safely in peripheral hospitals by a multidisciplinary highly specialized ECMO team [3] .
Introduction Cerebral microhemorrhages (MH) are diminutive focal bleedings which can be detected best by MRI using susceptibilityweighted imaging sequences (SWI). They can be found in a variety of neurologic diseases. The pattern of distribution can lead to the underlying pathomechanism [1] . Survivors of high-altitude cerebral edema (HACE) showed multiple MH, predominantly in the splenium of the corpus callosum. Mountaineers with a lack of acclimatization to high altitudes tend to suff er from HACE. Hypoxemia in great heights is discussed to be the main trigger of HACE [2] . Acute respiratory distress syndrome (ADRS) is characterized by oxygenation failure in mechanically ventilated patients. The severity is classifi ed by the ratio of arterial oxygen tension to fraction of inspired oxygen [3] . In some patients suff ering from severe ARDS, refractory to conventional therapy, venovenous extracorporeal membrane oxygenation therapy is the therapeutic option to ensure oxygenation. Methods Retrospectively, we examined 20 patients with cerebral MRI (including SWI) who had suff ered from severe ARDS and received ECMO therapy. The MRI slides were anonymized and analyzed by two experienced neuroradiologists. Based on the distribution pattern and characteristic, a modifi ed HACE score (mHCS) was surveyed [2] . Results Six of 20 patients (30%) showed multiple MH with emphasis in the splenium of the corpus callosum. Eight patients had sporadic MH in the parenchyma of the brain but not in the corpus callosum. The remaining six patients had no intracerebral alterations. The distribution of MH with involvement of the splenium resembled that seen in HACE survivors.
Conclusion Based on these results, we postulate that hypoxemia is one of the main players in the development of splenium-associated MH, not only in HACE but also in severe ARDS and other diseases accompanied with severe hypoxemia. Further investigations have to examine potential triggers and special circumstances concerning ARDS treatment which lead to MH in this distinctive pattern.
Diff erential venous oxygen return: a key factor of diff erential hypoxia in venoarterial extracorporeal membrane oxygenation X Hou 1 , X less diff erences of venous oxygen return and attenuated diff erential hypoxia were also observed in IVC-CA and FA-IJV. See Figure 1 .
Conclusion Diff erential venous oxygen return is a key factor of diff erential hypoxia in VA ECMO. We can take advantage of the notion of diff erential venous oxygen return to choose better cannula confi guration in clinical practice.
The HELP-ECMO pilot study (Heparin low dose protocol versus standard care in critically ill patients undergoing ECMO; ACTRN12613001324707) is a randomised controlled phase II study evaluating two levels of heparin anticoagulation in patients with no requirement for full anticoagulation. This work is a substudy of the HELP-ECMO trial and describes the consent process of the parent study. At our site, consent for research is often obtained by the research coordinator with little involvement from investigators. However, the nature of the ECMO population required a modifi ed consent approach to be implemented given that ECMO is often inserted emergently. It required a model that would be successful out of hours and could be delivered by any member of the treating team. 
The HELP-ECMO pilot study is enrolling patients admitted to a large metropolitan ICU who require ECMO. Education on eligibility criteria and study processes was given to all ICU senior medical staff . Consent must be performed prior to the commencement of anticoagulation, often only a short time after ECMO cannulation. To facilitate recruitment at all hours, a HELP-ECMO study box is located in the ICU. Within the box is an instruction page outlining the screening, consent and randomisation process, as well as administrative tasks. Plain language statements, randomisation envelopes and consent documentation stickers are provided. A consent script is provided to ensure consistency across consenting personnel. The process is reviewed by the research coordinator the following day to confi rm local governance compliance.
Results From April to December 2014, 30 patients were screened. Fourteen met the eligibility criteria and were approached for consent. Twelve patients were enrolled and randomised to receive either standard anticoagulation or low-dose heparin as per the study protocol. Consent was provided by the person responsible for 10 patients. One patient was competent to consent for themselves and one was enrolled under legislation allowing enrolment into research in the absence of a person responsible. There were two refusals. Seventyone per cent of participants were approached out of hours. Eighty-six per cent were consented by clinicians. Twenty-one per cent of patients were consented by a non-investigator. Conclusion The model of consent described has proven to be successful in this challenging patient population. The ability of all staff to perform consent for the study has been a signifi cant factor in the success of the pilot. The review of study processes by research coordinators has supported this model. Conclusion VA-ECMO is an eff ective treatment tool for refractory CS in patients with acute life-threatening heart failure. Patients aff ected by acute decompensation of CCM had poorer outcomes characterized by multiple organ dysfunction, as already known in the literature.
Rhabdomyolysis is a condition that results in the release of mainly creatine kinase (CK) and myoglobin from the breakdown of myocytes. Myoglobin has been known to cause renal failure (RF) and the CK level is routinely used as an indicator. A CK level >5,000 U/l was found to be associated with the risk of RF [1] . However, data are lacking on the level of CK to predict RF, especially in general trauma patients. The purpose of this study was to determine the initial CK level that predicts markedly elevated CK and the characteristics of trauma patients with elevated CK. 
The clinical diagnosis of rhabdomyolysis is confi rmed by creatine kinase (CK) levels >1,000 IU/l [1] . A local therapeutic protocol triggers aggressive renoprotective treatment in all patients with CK >2,000 IU/l. To evaluate local practice and refi ne CK thresholds for the instigation of renoprotective treatment, we studied the correlation between CK time trends and adverse outcomes such as acute kidney injury (AKI), the need for emergency renal replacement therapy (RRT) and mortality. We also evaluated the McMahon Score, a risk prediction model based on demographic, clinical, and laboratory variables available on admission [2] . Conclusion Severe AKI with RRT need is highly associated with previous HTN. The number of previous medications is related to severe AKI too. HTN has been described as a risk factor for developing AKI in critically ill patients [1] . ACEI and ARB use has been associated with AKI development in septic patients [2] . To our knowledge, this is the fi rst study that investigates risk factors associated with AKI in surgical septic patients with CIAI.
Introduction Acute kidney injury (AKI) is associated with increased morbidity and mortality in critically ill patients [1] . Early detection and treatment may improve outcome.
Methods A retrospective analysis of prospectively collected data from 2,158 patients without end-stage renal disease from the EPaNIC trial [2] . For early detection of AKI, defi ned according to the creatinine-based KDIGO guidelines [3] , three multivariate logistic regression models (LR) were developed using data available at baseline (LR_B), upon ICU admission (LR_BA), and at the end of the fi rst day in the ICU (LR_BAD1). In a subpopulation (n = 580) where plasma neutrophil gelatinaseassociated lipocalin (pNGAL), an early biomarker of AKI, was measured at ICU admission, the value of adding pNGAL to LR_BA and LR_BAD1 was assessed. The models were evaluated via bootstrapping, by comparing receiver operator characteristic (ROC) and decision curves.
Critical Care 2015, Volume 19 Suppl 1 http://ccforum.com/supplements/19/S1
Results Table 1 presents the performance of the models and admission pNGAL. Performance improved when predictions were made at a later time point, and was highest for LR_BAD1. Similar results were obtained in subgroups of septic and cardiac surgery patients. As an independent predictor, pNGAL alone did not perform better than a model using routine clinical data available upon admission. However, when combining pNGAL with LR_BA, predictive performance improved. The performance of LR_BAD1 was not improved by including pNGAL. Conclusion This study shows the potential of data-driven models based on routinely collected patient information for early detection of AKI during the fi rst week of ICU stay. Although adding admission pNGAL to admission data improved early detection of AKI, this added value is lost upon inclusion of data from the fi rst day of ICU.
Introduction This study tested the hypothesis that the cellular response in the kidney to sepsis is characterized by early activation of AMP activated protein kinase (AMPK), and that such activation is temporally associated with downregulation of the epithelial sodium channel (B-ENaC). Methods Fifteen C57BL/6 wildtype (WT) mice were subjected to cecal ligation and puncture (CLP), and sacrifi ced at 2, 6, 18, 24, or 48 hours. In addition, we pretreated three WT and three AMPK Beta 1 knockout mice with the AMPK activator AICAR (100 mg/kg intraperitoneal, 24 hours before CLP), and sacrifi ced 24 hours after CLP. Blood and tissue samples were collected for all animals. AMPK activation (pThr172), B-ENaC, and mitophagy (LC3 II/I) were examined by western blot of kidney lysates. Plasma creatinine (Scr) was assessed using ELISA.
The acute response to sepsis was characterized by early activation of AMPK which increased from 6 to 18 hours, peaked at 24 hours, and decreased by 48 hours ( Figure 1A ). This activation was associated with a consistent decrease in B-ENaC expression. In AICAR pretreated animals, AMPK was only activated in WT mice, which was associated with a decrease in the expression of B-ENaC as compared with AMPK KO mice ( Figure 1B) . Conclusion AMPK was activated early after induction of sepsis, and was associated with a consistent decrease in Beta-ENaC expression in the apical membrane of tubular epithelial cells. In addition, absence of AMPK activation in KO animals was associated with increased expression of Beta-ENaC at 24 hours after CLP. These data support the hypothesis that early activation of AMPK decreases energy consumption through ion channel downregulation. Figure 1 .
Conclusion TIMP-2 and IGFBP-7 can predict AKI in both septic and nonseptic critically ill patients. Further pragmatic randomised controlled trials are needed to prove their role on clinical basis. Reference
Introduction The gold standard for routine evaluation of kidney function is measurement of serum creatinine concentration (Scr). In ICU patients, muscle loss and dilution leads to decreased Scr. Scr is distributed in total body water, resulting in delay of Scr changes when the glomerular fi ltration rate (GFR) changes (lag time). Cystatin C (CysC) is a protein produced by all cells with a nucleus and therefore less aff ected by muscle mass. Also, the lag time may be shorter as the volume of distribution is only extracellular fl uid. The objective of this study was to evaluate whether single point measurement of CysC is more adequate than Scr for monitoring of kidney function in ICU patients.
Methods Data were collected in two prospective single-center studies on a convenience sample of ICU patients. During the 24-hour study period, we measured CysC, Scr, and urinary inulin clearance (Cinu) as a Introduction There is currently no accurate method of measuring the glomerular fi ltration rate (GFR) during acute kidney injury (AKI). Fourhour creatinine clearance (4-CrCl) is often used. We have previously validated a method of measuring the GFR using a continuous infusion of low-dose iohexol (CILDI) in patients with stable renal function (GFR from normal to <30 ml/minute/1.73 m 2 ). Steady state was achieved in <10 hours in all subjects and we calculate that variations >10.3% suggest an AKI. In this study we compare GFR measured by CILDI with 4-CrCl and 1-hour creatinine clearance (1-CrCl).
Methods Critically ill patients with evolving AKI and patients following nephrectomy were recruited. Demographics were compared using the t test. CIDLI was connected for up to 72 hours. Plasma and renal iohexol and creatinine concentrations were measured by tandem mass spectrometry four times daily. Iohexol renal clearance (IRC) and
1-CrCl and 4-CrCl were calculated and compared using Bland-Altman analysis. Results Baseline estimated GFR was similar in the postnephrectomy (88 ± 28) to the evolving AKI group (92 ± 23), P = 0.70. The evolving AKI group had a higher APACHE score (17.8 ± 5.1 vs. 10.6 ± 3.9; P <0.001). When 1-CrCl was compared with IRC, a bias of 0.8% (SD 26%, limits of agreement -52 to 50%; Pearson's r = 0.90) was observed in the evolving AKI group, whereas bias was -3.3% (SD 16, limits of agreement -35 to 29%; Pearson's r = 0.95) in the postnephrectomy group. When 4-CrCl was compared with IRC, bias was 5.1% (SD 54, limits of agreement -102 to 112%, Pearson's r = 0.45) in the established AKI group and bias was -4.5% (SD 38, limits of agreement -79 to 70%; Pearson's r = 0.78) in the postnephrectomy group. Conclusion Our data suggest that 4-CrCl is not as accurate and precise as 1-CrCl in patients with AKI and following nephrectomy. IRC appears to be more accurate and precise in patients with a predicted AKI risk and outcome (post nephrectomy) than in patients with evolving AKI. We hypothesise that IRC will be useful alternative to creatinine-based measures of AKI.
A Introduction Acute kidney injury (AKI) following coronary artery bypass grafting (CABG) results in signifi cant morbidity, mortality and prolonged stay on the cardiothoracic ICU. We sought to determine the incidence of AKI in our cardiothoracic centre, hypothesizing a higher occurrence in patients undergoing CABG using cardiopulmonary bypass (ONCAB) compared with off -pump (OPCAB) surgery [1] . Methods Retrospective data from all nondialysed patients undergoing isolated CABG at our institution were collected for the year 2013. Propensity scoring using the software platform MatchIt® was used to match subjects from ONCAB and OPCAB groups with regard to preoperative variables: logistic EuroSCORE, creatinine clearance (CrCl), gender and operative urgency. Postoperative AKI was defi ned as a rise of 50% or more in baseline serum creatinine [2] . Chi-square analysis was used to determine statistically signifi cant diff erences. Results From 500 cases (369 OPCAB, 131 ONCAB), 262 subjects were included in the fi nal analysis, with 131 in each group. There was a higher incidence of AKI and renal replacement therapy (RRT) in the ONCAB group, although this was not signifi cantly greater than in the OPCAB group ( Table 1 ). The mortality rate was identical with three deaths in each group. The average length of ICU stay for the OPCAB group was 1.96 days versus 2.49 days for the ONCAB group. Increasing total FOKIS score was associated with increasing mortality and increasing OI (Table 1) . FOKIS, controlled for PRISM, was an independent predictor of OI (P = 0.03). Conclusion In PICU patients, admission or day 3 AKI alone did not predict maxFO. A composite score that includes both AKI and FO parameters correlated with OI and discriminated survivors from nonsurvivors. FO seems to result from combination of increased fl uid exposure with underlying AKI but cannot fully be explained by oliguria in pediatric patients.
Introduction Tissue inhibitor of metalloproteinase-2 (TIMP-2) and insulin-like growth factor binding protein 7 (IGFBP7) are specifi c urinary biomarkers which can predict acute kidney injury (AKI) in critically ill patients within 12 hours [1] . A [TIMP-2]·[IGFBP7] result >0.3 (ng/ml) 2 /1,000 is associated with seven times the risk of AKI compared with a test result ≤0.3 [2] . The aim of our study was to explore the use of potentially nephrotoxic medications within the window between a positive biomarker test and the diagnosis of AKI stage 2 or 3. Methods We identifi ed all patients enrolled into the Sapphire study [1] who received at least one potentially nephrotoxic drug on the day of AKI (defi ned by stage 2 or 3 as per KDIGO classifi cation Little is known about these losses in RRT and whether they diff er between types of RRT. This study aims to quantify micronutrient losses during RRT in patients with AKI and to compare them in three diff erent RRT modalities: continuous venovenous haemofi ltration (CVVH), intermittent haemodialysis (IHD) and sustained low-effi ciency diafi ltration (SLEDf ). Methods A prospective observational study is being conducted at NUH. Thirty-three adult patients with AKI requiring RRT (13 IHD, 10 SLEDf, 10 CVVH) have been recruited. Samples of blood and RRT effl uent were obtained at baseline, mid and end-session from each participant during their fi rst RRT treatment. Samples were processed and stored at -80°C for subsequent analysis of amino acids by highperformance liquid chromatography and trace elements by inductively coupled mass spectrometry after derivatization from physiological fl uids. Micronutrient losses were calculated by multiplying masscorrected concentrations by total volume of RRT effl uent, adjusted for baseline plasma concentrations and RRT dose. Data were analysed by restricted maximum likelihood estimating equations.
The total baseline plasma concentration of all standard amino acids was similar between IHD versus SLEDf groups (1,812 ± 517 vs. 2,675 ± 527 μmol/l, respectively) but were higher in the CVVH group (3, 194 ± 564 μmol/l). RRT reduced the plasma concentration of amino acids in the SLEDf group (to 1,732 ± 529 μmol/l; P = 0.02), but had no eff ect in the IHD or CVVH groups (IHD; 1,853 ± 523, CVVH; 2,845 ± 512 μmol/l). Introduction Venovenous extracorporeal membrane oxygenation (VV-ECMO) is a novel therapy for severe respiratory failure (SRF). Its introduction has reduced mortality; however, patients require substantial blood product support and between 10 and 20% of cases develop a life-threatening haemorrhage. Methods We contacted 336 practitioners at 135 centres, examining their haematological management. Results In total 25% of practitioners contacted responded; 85% were attending physicians, predominantly based in North America and Europe, 41 and 32% respectively. Ninety-six per cent of units used a polymethylpentene membrane oxygenator and all used a centrifugal pump. Thirty-four per cent of responders managed <10 cases a year and 60% worked in units handling <20 annually, 6% saw >50 patients. One centre did not use unfractionated heparin. Monitoring of anticoagulation varied; 52% used the APTT, 43% the ACT and 5% the APTTr. Sixty per cent did not routinely measure antithrombin. Scenario 1 was based on a patient with H1N1. Practitioners targeted a haemoglobin (Hb) of 80 to 100 g/l; however, 20% targeted a Hb outside this range; 38% favouring a transfusion threshold of <80 g/l when the patient was improving compared with 32% when the patient had just started on ECMO. Seventeen per cent of practitioners transfused platelets when the count was <30 × 10 9 /l whilst 21% maintained the platelet count >100 × 10 9 /l. Scenario 2 described a patient with SRF secondary to a hospital-acquired pneumonia. The patient developed a haemothorax, with persistent blood loss of 200 ml/hour. Practitioners targeted a higher haemoglobin concentration of 100 g/l and targeted a higher platelet count of >100 × 10 9 /l when compared with the patient in scenario 1, neither of these diff erences was statistically signifi cant. Seventy-one per cent stated they would manage the patient off anticoagulation. There was no agreement as to the length of time off anticoagulation; 26% restarted anticoagulation in <12 hours, compared with 22% who advised no anticoagulation for >5 days. Scenario 3 examined the management of an incidental intracranial haemorrhage. There was a lack of consensus regarding the duration off anticoagulation; 14% of responders held anticoagulation for less than 12 hours whilst 37% held anticoagulation for >5 days and tranexamic was considered useful by 25%. Conclusion There was wide variation in the use of blood products and the intensity of anticoagulation. This is not surprising given the current lack of evidence. Further work is required to provide a standardised approach. . Five patients were discharged from the ICU but did not survive until H-discharge. At discharge from the ICU, these patients were similar to H-survivors in SOFA score, heart rate, mean arterial pressure and lactate, but showed lower StO 2 downslope (-13 (-16.5, -11.7)%/minute vs. -8.6 (-11.7, -6.5)%/minute, P = 0.01).
In the literature, duration of mechanical ventilation (DMV) is often considered an important risk factor (RF) [1] for VAP [2] in critical patients; generally the whole duration of MV is taken into Methods The team studied 52 VRU crashes that occurred in an urban area. The clinical data included an injury assessment using total body CT scan, Injury Severity Score (ISS), Abbreviated Injury Score (AIS), ICU and hospital length of stay and outcome score. Engineers collect data onsite with the partnership of the police, and assess the dynamics of the vehicles with the most advanced reconstruction techniques. Medical and engineering data were cross-matched during the correlation process. Injuries suff ered by each person were related to specifi c impact objects.
The average ISS is 21.5 (SD 10.9). Cars are the most involved in serious urban VRU crashes. Car-to-pedestrian crashes are the most frequent (50%). The impact speed is always over 40 km/hour ( Table 1) . The head and face are the most frequently injured part (48% of the 571 injuries collected), followed by lower extremities (15%). In terms of maximum AIS (MAIS), the head is the most severely injured region with 42% of MAIS 3+ (Figure 1 ). 
The head is still the most frequently and severely injured region. The severity of injuries increases in the most rigid part of the car. Improving VRUs' safety devices (active and passive) to reduce impact speed and severity of the primary impact has to be the next step. Introduction Various DAMPS, alarmins are released after trauma. The soluble receptor for advanced glycation endproducts (sRAGE) was reported to be associated with acute renal failure and duration of ventilation [1] . Cell-free DNA (cfDNA) has been associated with prognosis in trauma patients [2] . We studied the kinetics of these two biomarkers over the fi rst 5 days, in a cohort of severely ill trauma patients. Methods Patients who had sustained serious traumatic injury, within 24 hours of trauma, were recruited in a level I trauma center. We collected ISS, baseline demographic characteristics, resuscitation information and daily organ dysfunction (MOD) scores, over 10 days. Blood samples were collected within 24 hours of trauma (day 0) and on days 1, 3 and 5. sRAGE was measured by ELISA and cfDNA was measured by UV absorbance after plasma isolation. We collect data about patient characteristics, injury profi les, neurological fi ndings, results of radiological examinations, and medical courses. We then analyze the sensitivity and specifi city of MR-CDR for detecting intramedullary lesions on MRI and conduct further analysis. Conclusion Using a curved dilator and the TWG technique for the thoracic drainage procedure we found statistically signifi cant advantage to the TWG technique in comparison with the CS technique regarding precise chest tube placement within the pleural cavity.
Introduction The outcome of multiple trauma patients is related to a number of diagnostic and therapeutic interventions during hospitalization. ICU patients with severe trauma are susceptible to sepsis leading to poor outcome. Factors associated with the occurrence of sepsis and the outcome of these patients were investigated. Methods We studied retrospectively all trauma patients admitted to the A' ICU of KAT General Hospital in Athens during the last 3 years and were treated for more than 5 days. Age, gender, the type of injury, the severity of injury (Injury Severity Score), the length of ICU stay, severe sepsis, coexisting diseases, the outcome and the cause of death were recorded. Logistic regression and chi-square tests were used for statistical analysis. 
The normobaric oxygen paradox (NOP) postulates that a period of normobaric hyperoxia followed by a rapid return to normoxia will create a condition of relative hypoxia, which acts in turn as a stimulus for erythropoietin (EPO) production [1] . Variations in GSH and oxygen free radical (ROS) levels may be involved in this process. We tested the NOP in critically ill patients.
Methods A prospective observational study on 38 mechanically ventilated (FiO 2 <50%) patients with no active bleeding, no blood transfusion needed, and no kidney failure. Eighteen patients underwent a 2-hour period of normobaric hyperoxia (FiO 2 = 100%), 20 patients were evaluated as controls (no FiO 2 variations). EPO was assayed at baseline (t0), 24 hours (D1) and 48 hours (D2). Serum GSH and ROS were assayed at t0 (baseline), t1 (2-hour FiO 2 100%) and t2 (2hour return to normoxia) in 12 patients in the hyperoxia group.
Results EPO tended to increase in the hyperoxia group over time (P = 0.05), while it remained stable in the control group (P = 0.53) (Figure 1 ). ROS levels increased at t1 and decreased at t2, GSH tended to decrease at t1 and increased at t2 in the hyperoxia group. ). Therefore we used a rounded up cutoff value of 0.5 mg/l to examine the diagnostic accuracy of PATHFAST D-dimer to exclude PE. The correlation between PATHFAST and VIDAS results was particularly close for concentrations at or around the critical cutoff value of 0.5 mg/l. The correlation between PATHFAST and STRATUS results was particularly close in the patient group with VTE (r = 0.9694), whereas slightly lower results were obtained with STRATUS in the control group. With the widely used cutoff value 0.5 mg/l, PATHFAST demonstrated suitable sensitivity but not STRATUS. ROC analysis indicated that optimal cutoff values could be set at either 0.5 or 0.6 mg/l and at 0.3 or 0.4 mg/l for PATHFAST and STRATUS, respectively. Conclusion By use of the PATHFAST D-dimer assay only six of diagnoses were missed at the time of fi rst presentation compared with 10 diagnoses missed by the VIDAS D-dimer Exclusion assay, yielding higher sensitivity of the PATHFAST D-dimer assay compared with the VIDAS assay (90% vs. 83%). The STRATUS assays showed comparable performance and appeared to be suitable for the exclusion of VTE in the emergency room setting, whereas PATHFAST demonstrated superior sensitivity. Moreover, the PATHFAST analyzer allows simultaneous determination of D-dimer and cardiac troponin I within 16 minutes from whole blood samples. Therefore, this method might be useful at the point of care for early diagnostic assessment of patients with symptoms of PE or chest pain admitted to the ER or to the chest pain unit. Introduction Despite preventive anticoagulation therapy measures, venous thromboembolic disease is a major cause of morbidity and mortality among patients hospitalized in ICUs. In fact, pulmonary embolism is not only the most serious manifestation of the disease, but also one of the primary causes of sudden death. The aim of this study is to investigate the frequency of thromboembolism and pulmonary embolism in ICU hospitalized trauma and neurosurgical patients. Methods One hundred ICU patients, 51 postoperative neurosurgical and 49 trauma, were included in the study. Patients' demographic data as well as medical history, temperature, white blood cells and platelets counts were recorded on admission, the day of thrombosis diagnosis and the fi nal outcome of their treatment. Statistics were performed with SPSS-19. P <0.05 was considered signifi cant.
Results Thirty-eight out of 100 patients presented thrombosis, 14 trauma and 24 neurosurgical. We examined the correlation of thrombosis development during hospitalization with diagnosis, treatment allocated time and overall patient outcome. It was found that neurosurgical patients developed thrombosis more frequently than trauma patients (P <0.05). In relation to diagnosis, thrombosis was prevalent among patients with brain lesions (P = 0.018). Regarding the type of thrombosis, pulmonary embolism was also commonly apparent among individuals with brain lesion (P = 0.020). In addition, there was a statistically signifi cant correlation in thrombosis occurrence between hospitalization day (P <0.01) and patients' outcome on discharge (P <0.001). The type of thrombosis was directly associated with poor outcome, especially one that resulted from central catheters (P <0.001) and pulmonary embolism (P <0.001). However, no correlations were found with temperature, white blood cells and platelet counts on admission (P > 0.05). Conclusion Thrombosis aff ects the ICU patient's fi nal outcome. The type of thrombosis contributes to a poor outcome and mainly the occurrence of pulmonary embolism signifi cantly increases the mortality rate. The DVT was diagnosed using compression ultrasonography with color Doppler. Images were interpreted by two independent investigators trained in ultrasonography. All patients received intermittent pneumatic compression and unfractionated heparin twice daily during their IUC stay. Once the DVT was detected, therapeutic anticoagulation was initiated. Contrast-enhanced CT was performed when the patients were suspected to have pulmonary embolism. The primary outcome was the incidence of DVT during the ICU stay. Patients were followed until their hospital discharge. Results A total of 51 patients were included. The median age and BMI were 73 years and 23 kg/m 2 , respectively; 31% were female and 69% were surgical critical care patients. The median APACHE II and SOFA scores were 20 and 8, respectively. Risk factors associated with DVT were presence of central venous catheter 63%, malignancy 9% and hemodialysis 14%. The rate of DVT was 18.6% and the rate of CRT was 13.7%. All of these were asymptomatic and seen in neck and upper extremities. There was no DVT-associated adverse event (pulmonary embolism, bleeding) during hospital stay. The 28-day all-cause mortality rate was 3.4%. Conclusion While incidence of asymptomatic DVT is relatively high in adult critically ill patients, they were found only in the neck and upper extremities without any adverse event. Further research is needed to evaluate the clinical signifi cance of this type of DVT.
Introduction Computed tomographic pulmonary angiography (CTPA) has been used as a standard tool for diagnosing an acute pulmonary embolism (APE). The right ventricular (RV) strain signs may be used to predict adverse outcomes. However, the results are still controversial.
The primary objective of our study was to evaluate the relationship between the RV strain signs and respiratory failure requiring mechanical ventilation or death in APE. The secondary objective was to identify clinical factors which related to those outcomes. ) is an alternative parameter, its changes are unable to detect regional hypoxia. Our aim was to evaluate the quotient of the central venous-toarterial carbon dioxide gradient (δPCO 2 ) and the arteriovenous oxygen content diff erence (Ca-cvO 2 ) as a valid transfusion trigger parameter in hemodynamically stable anemic patients to reduce the amount of potentially counterproductive erythrocyte transfusions [1] . Methods Forty-fi ve postoperative patients admitted to our cardiac ICU were enrolled between January 2013 and September 2014. Three groups were defi ned according to the trend of blood loss over the surgical drains in the fi rst 24 postoperative hours. Mild blood loss was defi ned as 500 to 1,000 ml/24 hours, moderate (1,000 to 1,500/24 hours) and severe (>1,500 ml/24 hours). In addition to the δPCO 2 the following parameters were monitored: CI, CO, SVR, serum lactate, ScvO 2 and hemoglobin. Ca-cvO 2 was calculated and the δPCO 2 / Ca-cvO 2 quotient was assessed for a total of 400 paired blood samples. All enrolled patients were hemodynamically stable. A retrospective analysis of this data was performed.
Results δPCO 2 /Ca-cvO 2 showed signifi cant correlation with the moderate and severe blood loss groups (P <0.01), while no signifi cant correlation was detected in the mild blood loss group. The abnormality of the δPCO 2 /Ca-cvO 2 was easy detectable and refl ected intracapillary hemoglobin capacity decline and signifi cantly improved after erythrocyte transfusions (P <0.005). Conclusion Blood transfusions carry risks of adverse eff ects and should be carried out responsibly. Our fi ndings suggest an additive and easy detectable transfusion trigger parameter (δPCO 2 /Ca-cvO 2 ) providing physiological information on anemia-related altered oxygen extraction conditions and hence the indication for erythrocyte transfusions. However, additional studies are warranted to confi rm these fi ndings. Reference
Introduction We aimed to evaluate the frequency of red blood cell (RBC) transfusion in patients with traumatic brain injury (TBI) as well as determinants and outcomes associated with RBC transfusion in this population.
Methods We conducted a systematic review of cohort studies and trials of patients with TBI. We searched Medline, Embase, The Cochrane Library and BIOSIS databases from their inception up to 30 June 2014. We selected cohort studies and RCTs of adult patients with TBI reporting data on RBC transfusions. We extracted data related to demographics, baseline characteristics, blood product use and any relevant clinical patient-oriented outcome. Cumulative incidences of transfusion were pooled through random eff ect models with a DerSimonian approach, after a Freeman-Tukey transformation to stabilize variances. To evaluate the association between RBC transfusion and potential determinants as well as outcomes, we pooled risk ratios or mean diff erences with random eff ect models and the Mantel-Haenszel method. Sensitivity and subgroup analysis were planned a priori. 
The aim of this study is to explore the association between red blood cell transfusion (RBCT) and mortality in Thai critically ill surgical patients. Methods This study was a part of the multicenter, prospective, observational study performed in nine surgical intensive care units (SICUs) across the nation between April 2011 and November 2012 [1] . This study included adult patients admitted to the SICUs after surgery. Patients were categorized into transfusion and no transfusion groups according to whether or not they received RBCT at any time during SICU stay. Demographic data, clinical outcomes as well as SICU and hospital length of stay (LOS) and SICU and hospital mortality were collected. Patients were followed for up to 28 days or until discharge from the SICUs. The primary endpoint was hospital mortality. Data were compared between groups and logistic regression analysis was performed to determine whether RBCT was an independent risk factor of hospital mortality. In addition, patients were matched between groups based on the propensity score of the requirement of RBCT and were then compared. Introduction Anaemia in patients with severe traumatic brain injury (TBI) may worsen neurologic outcome. The aim of this study was to determine the association of haemoglobin level (Hb) with neurologic outcome and to determine a transfusion threshold which may be used in future transfusion trials aimed at improving neurologic outcome in TBI patients. Methods A substudy of the prospective multicentre Activation of Coagulation and Infl ammation in Trauma (ACIT) II study was performed on subjects recruited between January 2008 and December 2014. All adult trauma patients admitted to a level 1 trauma centre with severe traumatic brain injury (AIS head ≥3), ICU admission and available Hb levels on admission were selected for analysis. The primary outcome was the cognitive functioning of patients as determined by an estimated Glasgow Outcome Scale (GOS) on discharge. Anaemia was defi ned as a Hb level of ≤9 g/dl (severe anaemia) or ≤10 g/dl (moderate anaemia) within the fi rst 24 hours post injury. Multivariate logistic regression models were used to determine the association between anaemia and neurologic outcome. The receiver operating characteristic curve and the Youden Index were used to determine an optimal transfusion threshold. Introduction Sub-Saharan Africa suff ers from more acute lifethreatening indications for blood transfusion compared with highincome countries [1] . The commonest 'systems failure' contributing to perioperative death in low-resource settings is the timely availability of correctly cross-matched blood products [2] . Often this is not the result of an absolute shortage of blood products, but failure in the chain of supply and distribution. We audited an early step in this chain, the quality of blood requests, at the University Teaching Hospital (UTH) in Lusaka, Zambia. UTH does not have a formal blood request form, and only the cancer diseases hospital (CDH) has a blood request form developed by the blood bank.
We performed a 1-day retrospective review in June of blood request forms submitted to the cross-match laboratory, followed by a 14-day prospective review in September 2014. Group and save requests were excluded. Each form was audited against the American Association of Blood Banks (AABB) minimum standards for content of a blood request form. Analysis was performed with Fisher's exact test for nominal data and t test for continuous data. Results A total of 1,163 blood requests were reviewed, 51 from CDH and 1,112 from other wards. Eighteen forms from CDH (35%) and 22 from other wards (2%) met all minimum AABB standards (P <0.0001). The mean number of standards met on the requests from CDH and the rest were 11.25 (SD 0.93) and 8.87 (SD 1.75) respectively (P <0.0001).
Considering all blood requests, the standards met in order from least to most were: signature of requesting doctor (36%), urgency of request (43%), hospital number (59%), indication for transfusion (62%), type of product requested (72%), requesting doctor's name (78%), age or date of birth of patient (84%), gender of patient (89%), quantity of products requested (90%), date form was completed (90%), patient's ward (95%), and patient's full name (100%). Conclusion The audit revealed an important system failure impacting on effi cacy and safety of transfusion practice at UTH. Full patient identifi ers, as well as vital information such as the indication and urgency, were rarely fi lled in, which are crucial for the blood bank to prioritise the release of blood products. The audit shows that practice may be signifi cantly improved by a cheap intervention such as a standardised blood request form meeting international standards.
Introduction In Zambia, supply of blood is insuffi cient to meet clinical need, on a national level. Paradoxically, blood is also more often transfused unnecessarily in this setting. The Zambian National Blood Transfusion Service is currently scaling up voluntary blood donation and supply systems, and requires hospitals to improve blood transfusion safety. At a rural district hospital in Zambia, we audited practice and surveyed knowledge amongst staff using standards established in national guidelines. Methods A retrospective audit over 2 months of all blood transfusion forms at St Francis Hospital, Eastern Province, Zambia. Respective patients' notes were reviewed for: record of observations during transfusion; patient demographics; and length of stay. We surveyed nurses' attitudes, confi dence and knowledge in relation to blood transfusion standards. Results In May and June 2014, 457 requests were made for blood, of which 157 (34%) received blood transfusion, of which 108 (69%) had records of observations available. The audit demonstrated that requests were mostly complete (90%), but urgency was indicated in only 32%. The matching of blood to patient by more than one nurse was recorded amongst 66% of cases. Only 2% of transfusions met minimal requirements for transfusion reaction monitoring. Of nurses surveyed (n = 20), most were experienced in their post (mean 7.3 years, range 2 weeks to 20 years). Nurses rated themselves as highly confi dent in handling blood transfusions and identifying and dealing with transfusion reactions. However, 90% believed they could identify all transfusion reactions by measuring temperature alone, and 25% would measure temperature only as a parameter to monitor the transfusion, even in ideal settings. Most knew to check observations before, 15 minutes after the start of transfusion and then hourly thereafter (88%); but only 10% would check at the start, at completion and 4 hours after completing transfusion. The most frequently reported reason for not doing observations was time pressure on the ward (85%).
In this setting, current practice is evidently inadequate to identify and prevent blood transfusion reactions. The survey revealed high confi dence but patchy knowledge amongst nurses of the requirements for safe blood transfusion. Better timing to transfuse at times when nursing staff numbers are higher, alongside compulsory training, may together represent potential low-cost interventions to improve blood transfusion safety. Introduction During early sepsis, activation of the infl ammatory response and coagulation occurs. Extracorporeal therapies are used to adsorb mediators, but the coagulation of fi lters is a drawback [1, 2] . The aim of this study is to evaluate whether thromboelastography (TEG) may detect hypercoagulation and may improve anticoagulation during extracorporeal treatments. Methods Twenty-four patients with early severe sepsis had a TEG monitoring at basal time (T0) and during three diff erent extracorporeal treatments (T1): coupled plasma fi ltration (CPFA) with heparin infusion (Group A), CPFA with citrate infusion (Group B) and RRT with oXiris fi lter -heparin coated -and no heparin infusion (Group C). ANOVA test was used for the statistical analysis.
Results Table 1 presents the TEG values in early septic patients at T0. At T1, angle and MA decreased and r increased in Group A at diff erence with Group B and Group C (P <0.01). In group C, LY 30 was higher than in Group A and B (P <0.01). Results We initially kept 2,870 references. In total, 453 articles had mortality in their keywords. After reading the abstracts, 37 papers were analysed, and three articles evaluating mortality in two groups of patients (using and not using a VHT) were identifi ed. Among these three studies, only one had raw data available. We did not succeed in getting this information from the two other authors. We asked the main authors of the 37 selected papers, and renowned authors in the fi eld, if they had studies with new data that could be included in our review. The answers were negative. The three studies were: Johansson and Stensballe, total 832 cases, 121 traumas, raw data unavailable for trauma [1] ; Messenger and colleagues, prospective study, 50 cases, mortality similar, raw data unavailable [2] ; and Kashuk and colleagues, prospective study, 68 cases, mortality 59% control group, 28% VHT group [3] .
Conclusion With the studies available as of the end of 2014, it is impossible to conclude whether the use of a VHT in the emergency department decreases mortality. Other studies are needed.
The aim of this analysis is to explore and evaluate the role of thromboelastography (TEG) in aiding the management of patients admitted with haemorrhage to the ICU. TEG has been shown to rationalise and reduce transfusion requirements [1] but the precise role of TEG in the assessment and management of haemostasis in the bleeding patient is uncertain and has not been previously demonstrated.
Methods A prospective audit of TEG analyses performed on patients in the general medical and surgical ICU was recorded over a 2-month period. Operators documented the reason for admission, demographic data, indication for TEG, laboratory results, blood gas analysis, TEG results, diagnosis and subsequent action from the TEG result. Only patients who had been admitted with haemorrhage were included in this analysis. Results Seventy-eight audit sheets were completed, of which 31 identifi ed haemorrhage as the reason for admission. The mean age was 59.3 (range 21 to 90) and the mean APACHE II score was 18.23 (range 11 to 37). The main indications for TEG analysis included coagulopathy (64%) and ongoing haemorrhage (45%). As a result of performing TEG analysis, 23 (74%) patients had a documented change in their management. Ten patients did not require any further administration of blood products, which they would have received based on conventional laboratory results. The information gained from TEG also resulted in the omission of anticoagulation in three patients, and with a further two patients anticoagulation increased. Conclusion TEG aids prompt rationalisation of blood products and titration of anticoagulation in the bleeding patient. TEG identifi es a number of patients who required administration of platelets and other procoagulants which would not have been identifi ed by conventional methods. Several patients would have also received inappropriate transfusions which has both cost and resource implications, alongside the potential adverse eff ects on patients. We recognise that further research is needed to clarify the overall effi cacy of TEG in the bleeding patient.
Introduction This review aims to assess the value of thromboelastography (TEG) on the general ICU, which has not been previously demonstrated. TEG is a near-patient assessment of whole blood coagulation and fi brinolysis, which reduces transfusion requirements during cardiothoracic surgery and liver transplantation. Methods A prospective audit of TEG tests performed on patients being treated on a general ICU was conducted over 2 months. Results A total of 332 TEG tests were performed with a failure rate of 29.8%. Seventy-eight audit sheets were collected. Mean patient age was 68.9 years and mean APACHE II score was 18.1. Admissions included trauma (33.0%), perioperative (43.6%), haemorrhage (42.3%) and sepsis (21%). Standard tests of coagulation demonstrated 22 defi cits in coagulation which were not identifi ed as functionally signifi cant with TEG. Of these, 20 had abnormal clotting factor activity as measured by the INR/APTTr and 13 patients were thrombocytopenic. In total, 52.6% documented that the TEG result changed the management of the patient. In 46.8% of these cases no further blood products were required. In 41% there was no documentation. See Table 1 and Figure 1 . 
Our aim was to assess hemostasis, using ROTEM, in patients with end-stage liver disease (ESLD) undergoing liver transplantation (LT) and to develop a predictive model for patients prone to high intraoperative blood loss. Methods We retrospectively analyzed 122 patients who underwent LT between January and December 2013 in a single national center. Patients with acute liver failure or incomplete data were excluded. Demographic data, severity of liver disease assessed by MELD score (model for ESLD), presence of portal vein thrombosis, and laboratory data were recorded preoperatively. We performed concomitant ROTEM assay and standard coagulation tests (prothrombin time (PT), International Normalized Ratio (INR), fi brinogen) 1 hour before surgery and 15 minutes after the neohepatic phase. Intraoperative blood loss was recorded. High blood loss was defi ned as loss of one blood volume during surgery. Correlation between recorded data standard ROTEM parameters and derived thrombodynamic ROTEM parameters (potential index (TPI), maximum velocity of clot formation (MaxV), time to MaxV (MaxVt), AUC) were analyzed using SPSS 19.0. Results After applying exclusion criteria, 72 patients were analyzed with mean age of 54.5 years (SD 11.6) and a median MELD score of 17.4 (7 to 34). Preoperative MCE correlated with age (P = 0.044, 95% CI (-7.50, -0.12)) and MELD score (P = 0.009, 95% CI (-37.21, -6.69)), but not with PT (P = 0.557) or INR (P = 0.623). MaxV correlated with fi brinogen level (P = 0.005, 95% CI (0.01, 0.05)) and AUC correlated with age (P = 0.034, 95% CI (-257.74, -11.91)) and MELD score (P = 0.01, 95% CI (-1,233.14, -215.33)). Patients with portal vein thrombosis had an increase in InTEM CFT (P = 0.002, 95% CI (77.98, 317.97)) and MaxVt (P = 0.03, 95% CI (5.53, 105.63)). No correlation was found between preoperative ROTEM parameters and intraoperative blood loss. We calculated ΔMaxV, ΔMaxVt and ΔAUC as the mathematical diff erence between preoperative and intraoperative MaxV, MaxVt and AUC. High blood loss correlated with ΔAUC (P = 0.005, 95% CI (15.69, 61.03)), ΔMaxV (P = 9=0.002, 95% CI (-20,413, 6,392)) and ΔMaxVt (P = 0.008, 95% CI (15.69, 61.07)). Conclusion MELD score correlated with a decrease in MaxV and AUC on preoperative ROTEM but not with INR. Patients with portal vein thrombosis have increased InTEM CFT and MaxVt. High blood loss was associated with a decrease in thrombodynamic parameters, but no correlations were found between blood loss and standard ROTEM parameters. Figure 1 (abstract P344) . Change in management due to TEG results.
Introduction FDA-approved dosing of four-factor prothrombin complex concentrate (4F-PCC) in the USA is based on an INR and weight; however, there are data suggesting that a fi xed dose of 4F-PCC may be suffi cient for INR reversal and hemostasis [1, 2] . The objective of this study was to assess the effi cacy and safety of a fi xed dose of 1,500 units of 4F-PCC. Historically, warfarin reversal included a combination of factor IX complex, vitamin K, and fresh frozen plasma (FFP Introduction The study was conducted as a head-to-head comparison of a four-factor prothrombin complex concentrate (4F-PCC) and two diff erent three-factor PCCs (3F-PCC) for eff ective reversal of vitamin K antagonist (VKA)-induced anticoagulation using an established rat model of acute bleeding [1] . The 4F-PCC (containing the human coagulation factors II, VII, IX and X) is indicated for the urgent reversal of acquired coagulation factor defi ciency induced by VKA therapy in adult patients with acute major bleeding. In contrast, the 3F-PCCs (containing factors II, IX, X and only minimal VII) are indicated for the prevention and control of hemorrhagic episodes in hemophilia B patients. Nevertheless, the use of 3F-PCC for correcting hemostasis following warfarin overdose has been discussed but the lack of factor VII in these 3F-PCC products has raised questions about effi cacy. Methods Rats received an oral dose of 2.5 mg/kg phenprocoumon. At 15.75 hours post dosing, animals were treated with a single intravenous dose of saline, 4F-PCC (Beriplex® P/N, Kcentra®; CSL Behring) or 3F-PCC (Bebulin® VH; Baxter and Profi lnine® SD; Grifols). Study endpoints included bleeding following tail clip, activated partial thromboplastin time (aPTT), and prothrombin time (PT). In addition, the plasma levels of vitamin K-dependent coagulation factors were determined. Results Acute coumarin anticoagulation of rats induced a rise in median bleeding time by ≥2-fold from an average of 823 to 1,800 seconds (maximum observation period) compared with untreated animals. In parallel, PT and aPTT were prolonged from 8.9 to 29.9 seconds and from 14.5 to 25.5 seconds, respectively. Treatment with 4F-PCC was able to fully and statistically signifi cantly reverse bleeding, achieving average bleeding times of 676 seconds. In parallel, the elevation in PT was reduced to 15.1 seconds. In contrast, the two 3F-PCC products were not or only partially able to reduce coumarin-induced bleeding with average bleeding times of 1,398 and 1,708 seconds post treatment, respectively. This also correlated with inferior reductions in PT which achieved minimum levels of 23.8 and 29.5 seconds, respectively. There was no reduction in aPTT seen for any treatment option. 
The anticoagulant eff ect of dabigatran can be reversed with idarucizumab or PCCs in porcine blood in vitro [1] . However, the impact on clinical parameters such as blood loss is not known. Thus, this study assessed the effi cacy of idarucizumab in comparison with PCC and aPCC in dabigatran-anticoagulated swine following polytrauma on clinically relevant endpoints. Methods After ethical approval, 28 male pigs were administered dabigatran etexilate (30 mg/kg twice daily p.o.) for 3 days. Dabigatran was administered intravenously in anaesthetised animals on day 4 to achieve consistent high concentrations. Animals were randomised to receive idarucizumab (60 mg/kg, n = 7), PCC (50 U/kg; n = 7), aPCC (50 U/kg; n = 7) or placebo (n = 7). Intervention started 12 minutes after bilateral femur fractures and a standardised blunt liver injury. The primary endpoint was blood loss (observation period 300 minutes). Further, histopathology, haemodynamics and several coagulation variables were also assessed. Data were analysed by repeated-measures ANOVA (mean ± SD). Results Dabigatran levels were comparable between groups (571 ± 174 ng/ml) and resulted in altered coagulation variables. Blood loss was comparable 12 minutes post trauma between groups (801 ± 49 ml) and increased to 3,816 ± 236 ml in anticoagulated control animals post injury. Idarucizumab treatment reduced total blood loss to 1,086 ± 55 ml (P <0.005 vs. all), aPCC to 1,639 ± 104 ml (P <0.05 vs. control) and PCC to 1,797 ± 80 ml (P <0.05 vs. control) after 5 hours. All animals in the intervention groups survived, whereas control animals died within the observation period (mean survival: 89 minutes, range: 62 to 145 minutes). In histopathology no signs of thromboembolic events were present. Altered coagulation variables returned to baseline levels after idarucizumab application and were also signifi cantly, although inconsistently and to a lesser extent, ameliorated following PCCs. Conclusion All medical interventions were associated with reduced blood loss and increased survival. However, idarucizumab, a specifi c antidote to dabigatran, reduced total blood loss more prominently and normalised coagulation parameters to a greater degree as compared with either PCC or aPCC. Reference Results Twenty-three patients for each k-TEG and CSA group were compared. The time to available results was shorter using the CSA protocol in comparison with k-TEG ( Table 1 ). The diff erences were both statistically (P <0.00001) and clinically (mean reduction time 21 minutes) signifi cant. SCT needed the longest time to obtain the fi nal results.
Conclusion The implementation of a CSA, including r-TEG and ff -TEG, could shorten the time to a targeted treatment in critically bleeding patients.
Introduction Assessing the cost-eff ectiveness of therapeutic interventions is increasingly crucial for health decision-making. Spontaneous bacterial peritonitis (SBP) is one of the major complications of liver cirrhosis. The use of albumin in conjunction with antibiotics has been shown to be eff ective through clinical trials [1] . Methods A decision tree (TreeAge®) (Figure 1 ) was populated from published sources for clinical, cost and epidemiologic variables. The perspective taken was that of the US payer. The robustness of the model was checked using one-way and probabilistic sensitivity analyses. The clinical course was followed for 3 months or until death. Total medical costs and quality-adjusted life years (QALYs) [2] were calculated. Results Total costs were decreased when using albumin, and the improved survival resulted in an additional QALY for patients on albumin, decreasing the cost per QALY. See Table 1 and Figure 2 .
Conclusion The use of albumin in the treatment of SPB is cost-eff ective.
Introduction The use of albumin in therapeutics is controversial in several areas and requires assessment based on evidence for eff ective resource allocation. Supported indications include sepsis, areas of hepatic diseases and coronary artery bypass grafts (CABG). Latent therapeutic demand (LTD) [1] is the underlying evidence-based demand ensuring ample supplies of drugs are available and aff ordable. Estimating the LTD would assist decision-making and resource Critical Care 2015, Volume 19 Suppl 1 http://ccforum.com/supplements/19/S1
allocation, but many of the clinical and epidemiologic variables are subject to uncertainty. Decision analysis [2] may assist in generating an assessment of the demand for albumin.
Methods A decision analysis model was constructed using Excel. The model is based on the relationships of the epidemiological and clinical factors shown in the infl uence diagram (exemplifi ed in Figure 1 for sepsis). Data for the individual factors were obtained from the literature. One-way sensitivity analysis was used to generate Tornado diagrams (exemplifi ed in Figure 2 for albumin use in sepsis) to determine the relative contribution of diff erent factors to the LTD. Probabilistic sensitivity analysis was used to generate a probability distribution and calculate a mean level for the LTD of each indication.
Results On average, albumin use was calculated as 104 g per 1,000 inhabitants in severe sepsis, 157 g per 1,000 inhabitants in liver diseases and 61 g per 1,000 inhabitants in CABG. This shows a total LTD of 322 g per 1,000 use of albumin in the US annually. Conclusion Albumin consumption in the USA currently averages 479 g per 1,000 population [3] . Hence, the LTD of these three evidence-based indications represents 67% of current usage. Further work is needed to assess the LTD for albumin in other, less well-defi ned areas. References 
The induction of ECMO may result in metabolic acidosis [1] due to circuit priming with chloride-rich fl uids, and the sudden decrease in plasma strong ion diff erence (SID). This eff ect can be attenuated using balanced solutions with a SID equal to the patient's plasma bicarbonate concentration (HCO 3 -) [2] . We aimed to compare the eff ects of a novel balanced solution (SID equal to patients' HCO 3 -) with those of commonly employed crystalloids for circuit priming in patients undergoing venovenous ECMO. Methods We randomly assigned patients with acute respiratory failure in need of ECMO to receive either NaCl 0.9% (NS, SID = 0), Ringer lactate (RL, SID = 28), or a novel balanced solution (Solution X, SID equal to the patient's HCO 3 -) for circuit priming solution. Arterial blood gases and laboratory parameters were collected at 0, 5, 30, 60, 90, and Results We enrolled 20 patients (23 priming procedures -RL, n = 8; NS, n = 8; Solution X, n = 7). ECMO was initiated for ARDS (45%), bridge to lung transplant (25%), acute graft failure after transplant (15%), and acute on chronic respiratory failure (15%). Average priming volume was 10 ± 5 ml/kg; patients' baseline HCO 3 was 28 ± 6 mEq/l. During the fi rst 2 hours after ECMO initiation, arterial pH raised similarly in all groups (P = 0.39) due to CO 2 removal. In contrast, BE decreased starting after 5 minutes in both the NS and RL groups (BE variation, -2.2 ± 1.7 and -1.9 ± 1.3 mEq/l, P <0.001 vs. baseline; P = 0.04 for interaction, two-way ANOVA, 2-hour period). No BE changes were observed in the Solution X group (0.3 ± 0.8 mEq/l). In the NS group, BE reduction was associated with a reduction in SID (from 39 ± 8 to 34 ± 6 mEq/l at 5 minutes, P = 0.008), entirely due to an increase in Cl (103 ± 7 vs. 108 ± 6 mEq/l, P = 0.001). In the RL group, BE and SID reductions (40 ± 8 vs. 36 ± 8 mEq/l, P = 0.008) were associated with an increase in both Cl (105 ± 7 vs. 107 ± 7 mEq/l, P = 0.01) and lactate (1.4 ± 0.6 vs. 2.2 ± 1.0 mEq/l, P = 0.008). No changes were observed in other electrolyte concentrations. Dilution did not diff er between groups (P = 0.25 for Atot variation). The acidifying eff ect of NS and RL was amplifi ed in patients with higher baseline HCO 3 -.
Conclusion As compared with NS and RL, the use of a novel balanced solution with a SID equal to the patient HCO 3 level for ECMO priming uniquely avoids the addition of metabolic acidosis to patients with uncompensated hypercapnia. 
The aim of our study is to investigate the eff ect of intraoperative use of gelatin in living donor liver transplantation on postoperative acute kidney injury (AKI). It has been demonstrated that ischemia and chloride-liberal fl uid management cause AKI in liver transplantation [1] . Gelatin has minimal side eff ects on renal functions [2] ; however, it might be a reason for postoperative AKI. Methods A total of 154 liver transplantation patients were retrospectively evaluated between September 2011 and September 2013, and among these, 128 patients were included in the study. The patients who were under 18 years old, transplanted from cadaveric donors and needed preoperative renal replacement therapy were excluded. The patients were divided into two groups as GI (without gelatin administration) and GII (with gelatin administration). The patient's age, gender, actual body weight, diagnoses, MELD score, APACHE II score, duration of operation, total clamping time, noradrenalin infusion rate, amount of erythrocyte suspension, fresh frozen plasma (FFP) and thrombocyte suspension used, intraoperative fl uid balance, intraoperative and total clamping diuresis, serum creatinine levels on the postoperative 1st, 2nd, 4th and 7th days, duration of mechanical ventilation, length of ICU and hospital stay, hospital and 1-year mortality rate were recorded. The changes in creatinine levels on the 1st, 2nd, 4th and 7th days were evaluated according to the KDIGO guideline for AKI [3] . Results In total, 128 patients were categorized as GI (58, 45%) or GII (70, 55%). Total clamping time, intraoperative diuresis, intraoperative crystalloid use, intraoperative fl uid balance, operation bleeding, erythrocyte suspension, FFP and thrombocyte suspension use and postoperative lactate levels of GII were statistically signifi cantly higher than GI (P <0.001 for each). According to the KDIGO guideline, AKI in GII on the 1st, 2nd, 4th and 7th days (11.4%; 20%; 24.3%; 17.1%) was statistically signifi cantly higher than GI (P <0.001 for each). Conclusion In patients who received gelatin, kidney dysfunction in the postoperative period was observed more frequently. Also in this group, total clamping time was longer and amount of blood products used during surgery was more than the other group. Which of these factors is associated with AKI has to be revealed with further studies.
The purpose was to study the incidence of acute kidney injury (AKI) according to RIFLE and AKIN criteria in critically ill burn patients resuscitated with Ringer's solution and supplements of lower molecular weight hydroxyethyl starch (HES)130/0.4/6%, and to determine the relationship between RRT indication and mortality. Methods We studied 165 consecutive patients admitted to the critical care burn unit. Resuscitation was performed using lactated Ringer's solution and HES at a low dose to achieve urine output, lactate levels, and transpulmonary thermodilution parameters. The contributions of colloids and crystalloids were measured, and renal function was evaluated. Statistical analysis was performed using the Spearman test. Results The average total body surface area (TBSA) burned was 30 ± 15%, and the median of the total volume needed in the fi rst 24 hours was 4.01 ml/kg/% TBSA burned. According to the RIFLE criteria, 10 (6.1%) patients presented with risk, 11 (6.7%) presented with injury, and 11 (6.7%) presented with failure. According to the AKIN criteria: 9.7% presented stage I, 3% stage II and 10.3% stage III. Replacement therapy (RRT) was performed in 15 patients (9.1%). In six of these patients RRT was employed in the fi nal stages of multiorgan failure. In the remaining nine patients, for various reasons only one survived. Conclusion During the resuscitation phase of the burn patients, the use of HES (130/0.4/6%) at low doses does not seem to cause more risk or injury according to RIFLE or AKIN criteria than those reported by studies in burn patients resuscitated without HES. However, the need for RRT is associated with a high mortality, although in many cases the display is terminal.
Introduction Graft failure is a major cause of morbidity in patients with burns, resulting in increased length of hospital stay and increased number of operations. At our regional burns unit we collated the data from anaesthetic charts of patients admitted to our burns ICU who required skin grafting. The aim was to analyse whether any anaesthetic variables contribute to graft failure. Methods Thirty-fi ve patients were included in the analysis with a total of 191 operations. These were a combination of debridement, split skin grafts (SSG) and change of dressings. All patients were admitted to our burns ICU between January 2009 and October 2013. Exclusion criteria were death prior to discharge and initial surgery at a diff erent hospital. Sixteen patients had good graft viability (Group A) and 19 patients had poor graft viability (Group B). Logistical regression was performed using SPSS (Version 22.0). Hosmer and Lemeshow testing was used to confi rm goodness of fi t. Independent variables were age, sex, preoperative haemoglobin, intraoperative fl uid resuscitation, blood products, inotropes, volatile agents and temperature. Poor graft viability was defi ned as requiring at least one additional skin graft. Analysis was performed on all operations and then by subtype of operation (that is, SSG and debridement, SSG only).
There was no signifi cant diff erence in age, %total burn surface area or Belgian Outcome Burns Injury score between the groups. For all operation data, use of colloids was found to signifi cantly contribute towards poor graft viability (P = 0.035, 95% CI). When analysis was performed on only SSG and debridement operations, colloids remained signifi cant (P = 0.034, 95% CI) and metarminol use was found to signifi cantly contribute (P = 0.028, 95% CI) to poor graft viability. Overall use of inotropes was not signifi cant between the two groups. Other variables including minimum and maximum temperature, preoperative haemoglobin and blood transfusion were not found to be signifi cant. Conclusion Our results suggest that the use of colloids is a contributor to poor graft viability in burns. This was found to be independent of temperature and overall inotrope use; however, the use of metarminol may be a contributing factor. Introduction For a long time, many investigators have tried to demonstrate increased mortality associated with acid-base distur bances. In this study, we sought to determine the association of hyperchloremia measured at ICU admission and whether this electrolyte disturbance is associated with an increase in morbidity and mortality. Methods Data were retrospectively collected for consecutive adult patients admitted to Agustin O'Horan Hospital ICU, between January 2011 and July 2014, who underwent inpatient medical treatment using electronic fi les. Results The dataset consisted of 936 medical fi les and serum chloride concentration values on admission, 853 being eligible. Hyperchloremia (serum chloride >110 mmol/l) is quite common, with an incidence of 47.71%. Patients were propensity matched based on their association with death and hyperchloremia. Of the 853 patients collected, patients with hyperchloremia after admission (n = 446, 52.3%), patients were matched to patients who had normal serum chloride levels after admission. These two groups were well balanced with respect to all variables collected. The hyperchloremic group was at increased risk of mortality at ICU discharge, relative risk ratio = 1.81; 95% confi dence interval, 1.41 to 2.51 risk increase of 25.31%. Admission hyperchloremia was associated with increased morbidity, mortality and higher scores in severity scales; this association was statistically important. See Figure 1 . Conclusion This retrospective cohort trial demonstrates an association between hyperchloremia and poor ICU admission outcome (death). Additional studies are required to demonstrate a causal relationship between these variables.
Kidneys play a crucial role in the regulation of electrolytes and acid-base homeostasis. Impaired renal function is associated with greater urinary strong ion diff erence (SIDu) in patients with metabolic acidosis [1] . In critically ill patients, several factors, such as infused fl uids and acid endogenous production, would lead to changes in plasma SID and acid-base homeostasis without renal regulation of urinary electrolytes and SIDu [2] . Hence, AKI can be highlighted as an inability to address acid-base metabolic disturbances, which may be detected before major increases in creatinine or decreases in urine output. We evaluated the eff ects of renal function on urinary strong ion excretion using the Stewart approach to acid-base in critically ill patients with AKI. Methods A retrospective study was conducted. Patients with a diagnosis of AKI according to KDIGO creatinine criteria and available urinary chemistry at one point during their ICU stay were evaluated. Day 0 was defi ned as the day when SIDu was calculated from urinary spot analysis (SIDu = Na+U + K+U -Cl-U). Patients were followed and staged for AKI in the next 3 days. AKI reversibility was defi ned according to the lack of criteria for AKI. Results In total, 143 critically ill patients with a diagnosis of AKI were included. SIDu at day 0 did not diff er between diff erent AKI stages at day 0. SIDu at day 0 was statistically diff erent between diff erent AKI stages at days 1, 2, 3 ( Table 1 ). SIDu at day 0 was statistically diff erent between reversible and not reversible AKI at days 1, 2, 3 ( Table 2) . A conventional receiver-operating curve was generated to assess the accuracy of SIDu to predict AKI reversibility at day 1. AUC for SIDu was 0.82 (P <0.0001; 95% CI: 0.75 to 0.88). Conclusion SIDu identifi ed patients with reversible AKI with good accuracy. SIDu can be a promising, simple and cost-eff ective tool in AKI patient evaluation. Further research is needed to assess SIDu capability to early detect patients with renal dysfunction before increases in creatinine or decreases in urine output. Figure 1 (abstract P360) . Group mortality, high and low chlorine.
Introduction Care for ICU patients has benefi ted from medical devices. Bisphenol A (BPA) and phthalates can leach from the plastic matrix. We hypothesized that ICU patients are exposed to BPA and phthalates through medical devices. Methods Serum (n = 118) and urinary (n = 102) samples of adult (n = 35) ICU patients were analyzed for total BPA and di(2-ethylhexyl)phthalate (DEHP) and other phthalate metabolites (PMs). We also enrolled patients preoperatively before scheduled thoracic surgery and repeat samples were taken on days 1 to 4 during the ICU stay. Control data came from 44 healthy controls or from referenced literature.
Our results show that adult ICU patients are continuously exposed to phthalates (that is, DEHP) as well as to BPA, albeit to a lesser extent, resulting in detectable serum and urinary levels in almost every patient and at every studied time point. Moreover, these levels were signifi cantly higher than in controls or compared with referenced literature. The chronology of exposure was demonstrated: the preoperative urine and serum levels of the DEHP metabolites were often below the detection limit. Medical devices are the source of these chemicals: patients on hemofi ltration, extracorporeal membrane oxygenation or both showed serum levels 100-fold or 1,000-fold higher than the general population or workers in plastic industry. The serum and some of the urinary levels of the DEHP metabolites are the highest ever reported in humans; some at biologically highly relevant concentrations of even ≥10 to 50 μM. Conclusion Adult ICU patients are exposed to plastic softeners, in particular PMs. Despite the continuously tightening regulations, BPA and DEHP are still present in medical devices. Because patient safety is a concern in the ICU, further research into the (possibly toxic and clinical) eff ects of chemicals released from medical devices should be undertaken. Introduction Vitamin D defi ciency may frequently occur in critically ill patients and may be associated with sepsis and increased mortality. We therefore evaluated the prevalence of 25-hydroxyvitamin D defi ciency in a Dutch ICU, and its relationship with sepsis, morbidity and mortality. Methods We conducted a prospective observational study in a 10-bed mixed ICU. A total of 1,372 patients were admitted between July 2011 and June 2013 including 198 readmissions, of which 940 patients were studied. 25-Hydroxyvitamin D levels were determined within 24 hours after admission. 25-Hydroxyvitamin D levels were judged as suffi ciency (>50 nmol/l), insuffi ciency (30 to 50 nmol/l) and defi ciency (<30 nmol/l). Results The prevalence of defi ciency and insuffi ciency was 36% and 38%, respectively. Only 26% of the patients had suffi cient vitamin D levels. Vitamin D defi ciency is associated with sepsis (P <0.001) at ICU admission. Patients with defi cient levels had higher mean APACHE IV scores, 64 versus 52 (P <0.001), and longer length of hospital stay, 12 versus 9 days (P <0.001), respectively, as compared with patients with suffi cient levels. Patients with defi cient vitamin D levels had an odds ratio for in-hospital mortality of 1.4 (95% confi dence interval of 0.84 to 2.29, P = 0.2) relative to patients with suffi cient vitamin D levels. Conclusion 25-Hydroxyvitamin D defi ciency frequently occurs in Dutch critically ill patients. Although relating to sepsis, disease severity and morbidity, vitamin D defi ciency is not an independent predictor of mortality in these patients, which was otherwise relatively low. Introduction Stress hyperglycaemia (SH) is commonly observed during hospitalisation in the ICU and adversely infl uences outcome [1] . When SH occurs in previously nondiabetic patients, this might refl ect a latent disturbance of glucose metabolism and predict future risk of diabetes. We wanted to assess the incidence of disturbed glucose metabolism (DGM) and identify predictors for future diabetes risk. This could support timely diagnosis, prevention, and early treatment of impending diabetes mellitus (DM). Methods In this prospective observational study, we enrolled 338 patients without known DM, who were admitted for at least 36 hours to the ICU of the Antwerp University Hospital between September 2011 and March 2013. A 75 g oral glucose tolerance test was performed 6 to 9 months post ICU admission to screen for disturbed glucose metabolism. Furthermore, we examined whether post-discharge glucose disturbances could be predicted by the FINDRISC questionnaire [2] , patient demographics, comorbidities, HbA1c at ICU admission, and by parameters related to ICU stay (glucose parameters, insulin need, caloric intake, disease severity). Results In total, 246 patients (73%) experienced SH during their ICU stay. Eight months post ICU admission, glucose metabolism was disturbed in 119 (35%) subjects. Of these, 27 (8%) had impaired fasting glucose, 43 (13%) had impaired glucose tolerance, 25 (7%) had impaired fasting glucose and impaired glucose tolerance, and 24 (7%) were diagnosed with DM. A disturbed glucose metabolism tended to be more prevalent in subjects who experienced SH during ICU stay as compared with those without SH (38% vs. 28%, P = 0.065). HbA1c on admission correlated with the degree of SH (r = 0.308, P <0.001). The FINDRISC score (9.5 vs. 11, P = 0.001), SAPS 3 score (median of 42 in both groups, P = 0.003) and daily caloric intake during ICU stay (222 vs. 197, P = 0.011) were associated with a DGM. Conclusion Stress hyperglycaemia is frequent in nondiabetic patients and has a tendency towards future disturbances in glucose metabolism and DM. Glucose metabolism was disturbed in 35% of subjects 8 months post ICU admission, of whom 7% was diagnosed with diabetes mellitus. Predictors of elevated risk included a high FINDRISC score, high SAPS 3 score, and a lower daily caloric intake during ICU stay. References Introduction It is conjectured that transition of hypoglycemia to hyperglycemia may be more harmful than hypoglycemia itself. We investigated the association between the degree of correction of hypoglycemia and ICU mortality in patients under moderately strict to strict glycemic control. Methods This is a retrospective analysis from a pooled cohort from seven ICUs in the Netherlands over 6 years. ICU patients who developed hypoglycemia (<70 mg/dl) were included. We excluded patients who were readmitted, and patients with hypoglycemia in whom no followup blood glucose measurement was performed within 8 hours. We determined the association between three measures of correction of hypoglycemia within 8 hours after hypoglycemia and ICU mortality: predefi ned ranges of the 'highest blood glucose level' (<80 mg/dl; 80 to 110 mg/dl; 110 to 150 mg/dl (reference category); 150 to 180 mg/ dl; and >180 mg/dl); quartiles of the 'delta glucose' , defi ned as the diff erence between minimum and maximum blood glucose level with the third quartile as reference category; and quartiles of the 'standard deviation' of the blood glucose level with the third quartile as reference category. Results In total, 4,516 ICU patients developed at least one episode of hypoglycemia. In three separate multivariate analyses for each of the three measures we adjusted for the respective confounders. The category 80 to 110 mg/dl of the 'highest blood glucose level' was associated with increased mortality compared with the reference category (odds ratio (OR) = 1.31, 95% confi dence interval (CI) = 1.06 to 1.61). The lowest quartile of the 'delta glucose' (OR = 1.32, 95% CI = 1.03 to 1.69) and the lowest quartile of the 'standard deviation' (OR = 1.55, 95% CI = 1.23 to 1.96) were associated with higher ICU mortality than their reference categories.
Not the transition to hyperglycemia, but insuffi cient recovery from hypoglycemia is associated with an increased ICU mortality in patients under moderately strict or strict glucose control with insulin. Introduction The purpose of this study was to compare a computer protocol against a paper protocol in managing three domains of glucose control. Hyperglycemia is common in critically ill patients, and their risk of death is associated with hyperglycemia, hypoglycemia, and glucose variability. A safe and eff ective insulin protocol must minimize hyperglycemia and glucose variability while also avoiding hypoglycemia. Computer-based insulin protocols promise better performance by adjusting to each individual's sensitivity to insulin. Methods This is a historical cohort study with 759 patients admitted to three ICUs (medical/cardiac, trauma, and neuroscience) at an academic tertiary care hospital. All adult patients from January 2012 to October 2013 on one of two continuous insulin protocols for at least 8 hours were included. At the start of the study period the paper protocol in use (Adult ICU) had a target glucose of 140 to 180 mg/dl and was used for any patient with a glucose higher than 180 mg/dl. In June 2013 this was replaced by a computer-based insulin protocol (EndoTool) that had the same criteria for initiation and had a target glucose of 150 mg/dl. The primary exposure was the insulin protocol, and the primary outcome was performance in maintaining glucose control. Results The median glucose in the EndoTool group (141.5 mg/dl) was lower than in the Adult ICU group (159.9 mg/dl) (P <0.0001). The standard deviation of glucose in the EndoTool group (32.3 mg/ dl) was lower than the Adult ICU group (39.5 mg/dl) (P = 0.0001). The proportion of patients in each group with 10% or higher of measurements at a severe hyperglycemia level (≥200 mg/dl) in the EndoTool group (35.2%) was lower than the Adult ICU group (64.1%) (P <0.0001). The proportion of patients who had at least one moderate hypoglycemic measurement (<70 mg/dl) was not signifi cantly diff erent between the EndoTool group versus the Adult ICU group (11.73% vs. 9.3%, respectively; P = 0.34). However, there was a higher overall incidence of hypoglycemia in the EndoTool group (5.65 hypoglycemic measurements/100 person-protocol days) compared with the Adult ICU group (3.43/100 person-protocol days) (RR = 1.65, 95% CI = 1.09 to 2.45, P = 0.014). Severe hypoglycemia (<40 mg/dl) was rare, only occurring in 1/179 (0.56%) in the EndoTool group and 4/580 (0.69%) in the Adult ICU group. Conclusion Patients on the computer protocol had a lower median glucose, less variability, and less hyperglycemia than patients on the paper protocol. There was a higher risk of moderate but not severe hypoglycemia in the computer group. glucose data prove in such routine use?' Using actual case data, we have shown how comparing the mean absolute relative diff erence (MARD) and integration of the area under the curve (AUC) from the continuous glucose monitoring and intermittent measurement can be used to measure patient risk. Methods The analysis used aggregated case data generated from our recent clinical trials, where a GlySure sterile, single-use sensor and dedicated monitoring system was used to measure the blood glucose concentration in patients continuously and in real time. The measurement of risk was compared using the MARD, an accepted error calculation tool, and the AUC was calculated using an AUC analysis software program. Results When MARD from the GlySure sensor and intermittent measurement using the hospital's existing protocol was compared, the measure of risk to the patient (that is, the uncertainty regarding the patient's absolute blood glucose status) for the GlySure sensor was 50.5% lower than the intermittent measurement. The results also showed that as the variability of the BG data increases, the benefi t of continuous monitoring increases by signifi cantly reducing patient risk. The continuous monitoring reduces the patient's risk by 88%, 73%, and 69% respectively in high, medium and low variability situations. Conclusion It is more and more evident that continuous glucose technology will be instrumental in driving safe and eff ective glucose management protocols that will support more consistent glycemic management standards within ICUs and across institutions. References Introduction There is a need for continuous glucose monitoring in critically ill patients. The objective of this trial was to determine the point accuracy and reliability of a device designed for continuous monitoring of interstitial glucose levels in ICU patients (Sentrino; Medtronic MiniMed, Northridge, CA, USA). Methods Critically ill patients with an anticipated life expectancy >96 hours were eligible for participation, if the platelet count was >30 × 10 12 ml. Device readings were compared with glucose measurements in arterial blood using blood gas analyzers (RapidLab Siemens Healthcare Diagnostics, The Hague, the Netherlands). We used a linear mixed model to determine which factors aff ect point accuracy. In addition, we determined the reliability, including duration of device start-up and calibration, skips in data acquisition, and premature disconnections of sensors. Results We included 50 patients, aged 65 (56 to 72) years with an APACHE II score of 23 (17 to 26). Admission types were medical (62%), elective surgery (22%) and emergency surgery (16%), and 22% had diabetes. For the accuracy analyses we had 929 comparative samples from 100 sensors in 45 patients (11 (7 to 28) samples per patient) during 4,639 hours (46 (27 to 134) hours per patient and 46 (21 to 69) hours per sensor). The Bland-Altman plot showed a bias of -0.6 mg/dl with limit of agreement between -57.2 and 56 mg/dl. Glucose prediction error analysis showed 60% of the glucose values <75 mg/dl within ±15 mg/dl and 75.8% of the glucose values ≥75 mg/dl within 20% of the comparative RapidLab results. Clarke error grid analysis showed 75.3% in zones A and 23.5% of the paired measurements in zones B, 0.3% of the paired measurements in zones C and 0.9% of the paired measurements in zones D. Point accuracy did not meet the ISO14971 standard for dosing accuracy, but improved with increasing numbers of calibrations, and was better in patients who did not have diabetes mellitus. Sixty out of 105 sensors were removed prematurely for a variety of reasons. The device start-up time was 49 (43 to 58) minutes.
The number of skips in data acquisition was low, resulting in availability of real-time data during 95 (89 to 98)% of the connection time per sensor. Conclusion The point accuracy of the device was relatively low in critically ill patients. The device reliability was relatively good, although sensors were removed prematurely for a variety of reasons. Introduction Hyperglycemia is common and often multifactorial in critically ill patients. The association of hyperglycemia with adverse outcome has repeatedly been established in a variety of settings. The objective of this study was to investigate whether hyperglycemia on admission to the ICU impacts presentation and outcome of sepsis patients and whether this eff ect is diff erent for patients with a history of diabetes mellitus. Methods A two-center, prospective observational cohort study was conducted including all consecutive critically ill patients admitted to the ICU between January 2011 and July 2013. Sepsis patients were identifi ed using strict clinical and diagnostic criteria. The fi rst glucose measurement within a time window of 4 hours before up to 4 hours after ICU admission was categorized into euglycemia (71 to 140 mg/dl), mild hyperglycemia (141 to 200 mg/dl) or severe hyperglycemia (>200 mg/ dl), patients with hypoglycemia were excluded. A multivariable Cox proportional hazard model was used to determine the eff ect of admission hyperglycemia on mortality corrected for covariates. Results Of the 1,059 patients admitted with sepsis, 526 (55.8%) had admission glucose levels within the normal range, 270 (25.5%) had mild hyperglycemia and 202 (19.1%) severe hyperglycemia. Patients with severe hyperglycemia were older, had higher APACHE IV scores and were more often diabetics compared with euglycemic patients. Shock on admission was more common in patients admitted with euglycemia. Crude mortality increased with increased admission glucose and a Cox regression analysis showed increased risk for 30day (HR = 1.67, CI = 1.24 to 2.23), 60-day (HR = 1.42, CI = 1.08 to 1.87) and 90-day (1.31, CI = 1.02 to 1.70) mortality in patients admitted with severe hyperglycemia compared with euglycemia. The association between mortality and severe hyperglycemia on admission was only present in patients without known diabetes but not in patients with a history of diabetes (30-day mortality HR = 1.67, CI = 1.15 to 2.43 vs. 1.84, CI = 0.97 to 3.49). Severe hyperglycemia was associated with a blunted proinfl ammatory cytokine response (IL-6 and IL-8) on admission in patients without, but not in patients with diabetes. Conclusion Severe hyperglycemia on admission is associated with increased 30-day, 60-day and 90-day mortality in sepsis patients without a history of diabetes mellitus. microdialysis-based continuous glucose monitoring (CGM) (EIRUS®; Maquet Critical Care AB, Solna, Sweden). Methods Patients with an expected stay in the ICU of >48 hours needing an arterial catheter and a central venous catheter (CVC) were eligible. For a maximum of 3 days, during 8 hours per day, 125 μl blood was drawn from the arterial line every 15 minutes. Point accuracy was expressed using Clarke error grids, Bland-Altman plots and glucose prediction error analysis [4, 5] . Trend accuracy was expressed using continuous glucose error grid analysis [6] . Results Three-hundred and fi fty-four paired samples were obtained from seven patients (66 (59 to 79) years old, APACHE II score 23 (20 to 28), 51 (19 to 77) samples per patient). Point accuracy: 91% of paired values were in zone A, with the remaining 9% of the values in zone B in the Clarke error grid. In the Bland-Altman, bias was 5.4 mg/dl with an upper limit of agreement of 32.5 mg/dl and a lower level of agreement of -21.8 mg/dl. Glucose prediction error analysis showed that 91% of the values ≥75 mg/dl within 20% of the values measured by the blood gas analyzer were within range. Trend accuracy: in the rate error grid of the continuous glucose error grid analysis, 96% of the paired values were in zone A, 3.7% were in zone B and 0.3% were in zone C. Conclusion Point and trend accuracy of the tested microdialysis-based CGM are good in critically ill patients.
Introduction Hyperglycemia occurs in 50 to 85% of patients admitted to a medical ICU (MICU) and has been associated with poor prognosis [1, 2] . Whether applying intensive insulin therapy to achieve tight glycemic control in critically ill patients is benefi cial remains controversial [2] . Another important observation is a link between glycemic variability and mortality [3] . We performed a pilot study hypothesizing that when implementing intensive insulin therapy, real-time continuous glucose monitoring (RT-CGM) may help to safely achieve tight glucose control, while avoiding hypoglycemia and reducing glycemic variability in MICU patients. Methods This two-center randomized controlled pilot study was performed during a 3-year period. To be included, patients had to be severely ill (APACHE II score ≥20) and CGM monitoring had to be started within 48 hours after admission in the MICU. Thirty-fi ve patients (age 66 ± 10 years; nondiabetic/diabetic patients 27/8; APACHE II score 28 ± 6) were randomly assigned to RT-CGM (n = 16) or to blinded CGM. In both groups a microdialysis-based glucose sensor (GlucoDay®S) was used during a 96-hour period of glucose monitoring. Insulin infusion was performed using a modifi ed Yale protocol. Outcome measures were percentage of time in normoglycemia and in hypoglycemia, glycemic variability, and CGM accuracy. Results In the RT-CGM group the percentage of time at the target glycemia (80 to 110 mg/dl) was 37 ± 12% versus 34 ± 10% in the control group (NS) and glycemia averaged 119 ± 17 mg/dl versus 122 ± 11 mg/ dl respectively (NS). Time spent in hypoglycemia (<60 mg/dl) was not statistically diff erent between the group assigned to RT-CGM (0.6 ± 1.6% of the time) versus those with blinded CGM registration (2.4 ± 4.3% of the time). Parameters of glucose variability (standard deviation of mean glucose value, coeffi cient of variation, mean amplitude of glucose excursions) did not diff er between the groups. The GlucoDay®S values and arterial glycemia correlated well with 98.6% of data falling in regions A and B of error grid analysis. Conclusion In our study, the use of RT-CGM neither improved glucose control and variability, nor did it reduce hypoglycemic events. On the other hand we can state that our insulin infusion protocol already led to overall tight glucose control without a signifi cant hypoglycemia risk, leaving little space for improvement.
Introduction Faecal peritonitis (FP) is a common cause of sepsis and admission to the ICU [1] . We report a review of all patients admitted to our ICU over 5 years with FP. The aim was to defi ne the clinical characteristics, outcomes and risk factors for mortality in ICU patients with FP. Methods Data were extracted retrospectively from electronic case fi les. The primary outcome was ICU mortality. Secondary outcomes were hospital, 28-day, 90-day and 1-year mortality. Logistic regression analysis was used to identify independent risk factors for mortality. Results Ninety-nine FP patients were admitted between April 2008 and January 2014. Median age was 73 (IQR 61 to 79), with a female preponderance (53.5%). The median ICU length of stay (LOS) was 5 days (IQR 2 to 16). On admission to critical care, clinical data included (all medians): temperature 36.6°C (IQR 36 to 37.2), systolic blood pressure (BP) 113 mmHg (IQR 104 to 136), diastolic BP 56 mmHg (IQR 49 to 67), lactate 2.3 mmol/l (IQR 1.5 to 3.7), bilirubin 12 μmol/l (IQR 9 to 20), haemoglobin 104 g/l (IQR 93 to 116), haematocrit 31 (IQR 28 to 36), creatinine 88 μmol/l (IQR 66 to 152), prothrombin time 13.1 seconds (IQR 11.9 to 14.4). In 86 patients the initial operation was an emergency laparotomy, with primary perforation in 53 cases. Subsequent anastomotic dehiscence and need for relaparotomy happened in 24 and 33 cases respectively. Forty per cent of patients underwent more than one surgical abdominal intervention. The most common antibiotic used was tazobactam and fl uconazole was the commonest antifungal. The percentages of patients receiving mechanical ventilation, renal replacement therapy and inotropic/vasopressor support during ICU stay were 72.7%, 25.3% and 84.8% respectively. The ICU and hospital mortality rates were 23.5% and 26.1%, respectively, increasing to 26.7% at 28 days, 28.4% at 90 days and 32.2% at 1 year. None of the surgical factors or diabetes infl uenced survival. The strongest independent risk factors associated for ICU mortality were systolic BP on ICU admission (OR = 1.05, 95% CI = 1.01 to 1.09, P = 0.015), acute kidney injury (AKI) within the fi rst 24 hours of ICU admission (OR = 0.15, 95% CI = 0.03 to 0.9, P = 0.026) and lactate on ICU admission (OR = 0.62, 95% CI = 0.39 to 1, P = 0.05). Conclusion In this cohort of critically ill FP patients the ICU and 12-month mortality rates were 23.5% and 32.2%, respectively. The most consistent predictors of mortality across all time points were AKI within 24 hours of ICU admission and admission lactate.
Methods In this single-centre observational study we prospectively enrolled all patients undergoing elective cardiac surgery. The primary output was the time to faeces (TTFE) as representing the postoperative ileus. Secondary outputs were the occurrence of ischaemic colitis and pneumonia. Quantitative variables were compared by ANOVA or Wilcoxon tests, qualitative variables by chi-square tests. Multivariate analyses were performed by logistic regression, P <0.1 for inputs P <0.05 for outputs. Results We included 349 patients: age 67.5 ± 10.8 years, M/F sex ratio 252/97, preoperative left ventricle ejection fraction 58.8 ± 10.6%, bypass/valve ratio 234/154, number of grafts 2.7 ± 0.9, mammal arteries 1.8 ± 0.5. In univariate analyses, bypasses received more anaesthetic drugs (P <0.01), had shorter extracorporeal circulation duration, 67 ± 27 versus 75 ± 24 minutes (P <0.01), and received less blood products (P <0.0001). Bypasses had lower postoperative levels of troponin (3.9 ± 7.6 vs. 8.1 ± 21 pg/ml, P <0.01) and LDH (330 ± 162 vs. 420 ± 175 pg/ ml). In contrast, the intra-abdominal pressure (IAP) was higher and related to the number of grafts at day 0 ( Figure 1 ) and day 1 (P = 0.01 and 0.02 respectively), and to the number of mammal grafts at day 0 and day 1 (P = 0.01 and 0.04 respectively). The TTFE was longer but did not reach signifi cance (P = 0.13) as well as the occurrence of abdominal ischaemia (P = 0.22). The occurrence of pneumonia was higher (P = 0.01). In multivariate analysis, the IAP at day 0 and day 1 was related to propofol quantities only. The predictors of pneumonia were: duration of mechanical ventilation, peak lactate in the postoperative 24 hours, and coronary bypass: OR = 163, 2.6, and 4.2 respectively. Conclusion The number of coronary grafts and of mammal artery used in cardiac surgery is associated with higher IAP and higher risk of pneumonia. However, whether this is due to direct bowel ischaemia or longer anaesthesia remains to be studied in larger trials. Results Eighty-seven patients (69.9%) were females. Mean age was 48.9 years. Primary cancer was colorectal in 42 patients (32.5%), ovarian in 39 (30%), appendiceal in 29 (22%), others in 15.5%. Average operative time was 11 ± 2.1 hours. Average intraoperative crystalloids given were 12,217 ± 4,359 ml, packed RBCs were 2 ± 2.3 units, colloids 1,083 ± 898 ml, average blood loss was 1,108 ± 785 ml. All patients were admitted to the ICU post procedure. The average fl uid balance during the OR was 9,481 ± 4,694 ml. Patients stayed in the ICU for an average of 6 ± 5.3 days. All patients survived the ICU stay. The duration of mechanical ventilation was 57 ± 83 hours, total fl uid balance while in the ICU was 1,467 ± 3,399 ml. Hypomagnesemia was the most frequent electrolyte abnormalities in 79 (61%). Pleural eff usions in 48 (37%), of which three patients only required drainage, Seven patients (5.6%) developed pneumonia, no patient required renal replacement therapy. Average hospital LOS was 33.7 ± 29 days. Only two patients died in the hospital. When the fi rst 65 patients were compared with the last 64 patients, the duration of MV, ICU LOS and hospital LOS were all signifi cantly shorter in the latter group (72 vs. 43 hours, 6.8 vs. 5.0 and 40 vs. 27 days respectively; P <0.01 for all). Conclusion With proper selection of patients, CRS with HIPEC can be done safely with no major complications. There is a signifi cant reduction in ICU utilization and shorter hospital LOS with more experience in such procedure, suggesting a learning curve as well as better utilization of resources by referring such patients to a high-volume center. 
The disseminated intravascular coagulation (DIC) score is a predictor of outcome in critically ill patients [1, 2] . Yet disturbances of coagulation and hemostasis, as refl ected by the DIC score, are a common fi nding in patients with liver cirrhosis. Thus, it is unclear whether the DIC score has prognostic value in critically ill patients with liver cirrhosis. The aim of this study was to assess the applicability and prognostic impact of the DIC score in critically ill patients with liver cirrhosis. Methods Patients with liver cirrhosis admitted to the medical ICU were analyzed for this study. Detailed laboratory analyses including platelet count, D-dimer, fi brinogen and prothrombin index were performed on admission and the DIC score was calculated. Survival was assessed on site or by contacting the patients or the attending physician. Results In total, 150 admissions to the ICU with liver cirrhosis were analyzed. Thirty-nine percent were female. Median age was 56 (IQR 49 to 63) years. The median SOFA score on admission was 9 (6 to 13), median MELD score 26 (IQR 18 to 36). Twenty-eight-day mortality was 59%. Median DIC score on admission was 5 (IQR 4 to 6). Overt DIC (DIC score ≥5) was found in 65%. DIC score was signifi cantly higher in nonsurvivors compared with survivors (5 (IQR 4 to 7) vs. 4 (IQR 3 to 6); P <0.01). AUROC for the DIC score in prediction of 28-day mortality was 0.68 (95% CI = 0.59 to 0.77). Overt DIC on admission was signifi cantly associated with 28-day mortality (OR = 3.4 (95% CI = 1.69 to 6.84), P <0.01). The 28-day mortality rate in admissions with cirrhosis and overt DIC was 70% compared with 40% in those with a DIC score <5. Conclusion Disturbances in coagulation and hemostasis are found in the majority of cirrhotic patients admitted to the ICU. The DIC score is a suitable predictor of 28-day mortality in critically ill patients with liver cirrhosis. References Introduction Factors associated with initial poor graft function (IPGF) after liver transplantation are still under debate. Although the initial insult to the graft begins during the cold ischemia time (CIT), recent studies showed that most injuries occur during rewarming. Ischemicreperfusion (I/R) injuries are present in all grafts and may be responsible for postoperative graft dysfunction. Along with other factors, I/R injury may also play a role in the development of postreperfusion syndrome (PRS) after revascularization of the liver graft. The aim of this study was to assess whether longer warm ischemia time (WIT) is associated with PRS or with IPGF after liver transplant. The secondary aim was to investigate whether patients with intraoperative PRS have a higher risk for postoperative IPGF. Methods This retrospective observational study included 60 liver transplant patients. We excluded from the study group patients with retransplant procedures, and the recipients of divided grafts and of grafts from extended criteria donors. We recorded: demographic data, intraoperative PRS, CIT, WIT, ALT, AST levels and standard coagulation tests on postoperative days (POD) 1 to 5. Statistical analysis was performed using SPSS Statistics v.19.1 with signifi cant P value under 0.05.
We used the criteria of Nanashima and colleagues for the diagnosis of IPGF (ALT and/or AST level above 1,500 IU/l within 72 hours after OLT). The study group included 33 men (55%) and 27 women. Mean (±SD) age was 50.56 (±13.26) years. WIT longer than 60 minutes correlated signifi cantly with ALT and AST levels in POD 1 to 3 (P <0.0001 for ALT in POD 1 to 3, P = 0.001 for AST in POD 1, P = 0.007 and 0.013 for AST in POD 2 and 3) and with prothrombin time (P = 0.008 in POD 1, P = 0.03 in POD 2 and P = 0.015 in POD 3). We could not fi nd a correlation between PRS and WIT (P = 0.566), CIT (P = 0.439) or transaminase levels on POD 1 to 3. The correlation between WIT >60 minutes and IPGF was confi rmed using the Pearson chi-square test (P <0.0001). The same test was used to correlate IPGF with PRS with nonsignifi cant results (P = 0.876).
Conclusion Our study showed that PRS is not a risk factor for IPGF after liver transplantation. WIT over 60 minutes does not infl uence the development of PRS, but is associated with IPGF after liver transplantation. Close monitoring of liver tests in the early postoperative period is very important especially in recipients of grafts with WIT over 60 minutes. Further eff orts to decrease WIT may prove useful for the reduction of IPGF in liver transplant patients. The results of 36 patients could not be reached, so 426 patients were evaluated in the study. Among 426 patients, 169 (39.7%) were female and 257 (60.3%) were male. The mean age was 63.7 ± 18.7. HBsAg was positive in nine (2.1%) patients; all of these nine were male. Anti-HCV was positive in four (0.9%) patients; among these, three were male and one was female. Only one patient was positive for anti-HIV.
In the present study, it was observed that the seroprevalences of HBsAg, anti-HCV and anti-HIV were not higher than in our city population. However, taking the safety precautions of the healthcare workers during surgical or invasive procedures such as catheterization, intubation or tracheostomy without any information about the serological test results of the patients will reduce the contamination of these agents.
Patients with decompensated cirrhosis admitted to the ICU have historically had a very high mortality rate [1] . It has been suggested that improving patient selection can improve ICU outcomes in patients with cirrhosis [2] . The aim of this study was to determine the mortality and evaluate the risk factors that may infl uence the outcome of this group of patients in a large UK district general hospital with a view to introducing selection criteria for future ICU admission of patients with decompensated liver disease. Methods A retrospective analysis was performed of all adult patients with decompensated chronic liver disease admitted to a general (nontransplant) critical care unit between January 2012 and December 2013. Data were collected regarding demographics, ICU mortality, hospital mortality, aetiology of chronic liver disease, severity scores, acute diagnoses, and organ support requirements. Results Thirty-seven patients were identifi ed, with a median age of 57 years, predominantly male (62%). Seventy-six per cent had alcohol-related cirrhosis. Overall ICU mortality was 29.7% and hospital mortality was 48.6% -these values were higher in the alcoholic group (39.3% and 57.1% respectively). All ICU deaths were in those with alcoholic liver disease. Median scores were: APACHE III 93, SOFA (day 1) 9, Child-Pugh 11, MELD 21. Seventy per cent were treated for sepsis, 22% had a GI bleed, 57% had encephalopathy, 24% had suspected/ confi rmed spontaneous bacterial peritonitis, and 70% had an acute kidney injury. Organ support requirements were: 35% respiratory (non-invasive or invasive ventilation), 38% vasoactive agent support, 24% renal replacement therapy (RRT). Alcoholic liver disease patients requiring respiratory or cardiovascular support had an ICU mortality of 64%, and those requiring RRT had a mortality of 75%. Alcoholic liver disease patients requiring combined respiratory, cardiovascular, and RRT support had 100% mortality. Conclusion Those with decompensated chronic liver disease admitted to the ICU have a signifi cant ICU/hospital mortality, which is increased in alcoholic liver disease. Sepsis and AKI were the most common acute diagnoses in this cohort. Alcoholic liver disease patients requiring organ support have a very high mortality, and the outlook for multiorgan failure requiring RRT in this group is dismal. References a supportive therapy as a bridge to transplantation or recovery in adults with liver failure. The system off ers specifi c challenges when applied in children due to the large extracorporeal volume (700 to 750 ml). We therefore developed an adapted protocol for the application in children.
Methods Priming of the blood circuit is performed using 2 l isotonic saline, whereas the plasma circuit, containing both adsorption devices, is fi lled with 2 U fresh frozen plasma or 400 ml stabilized solution of human plasma proteins. Next, for children with body weight (BW) <25 kg, a solution of 60 to 65% packed cells (PC) is infused in the inlet blood line at 40 ml/minute. The volume of PC needed is calculated based on the circuit priming volume and the maximum allowed extracorporeal blood volume of the child (= 8 ml/kg × BW). After the priming phase, blood and plasma fl ow are increased to at least 100 ml/ minute and 200 ml/minute, respectively, and dialysate fl ow is set at 300 ml/minute. Regional citrate anticoagulation is done with a calciumfree dialysate, while, eventually, heparin is added to the priming solution. Post treatment, the circuit volume is either not reinfused (BW <25 kg) or reinfused using isotonic saline (BW >25 kg), with a volume depending on the hydration status and the originally infused volume of PC. Reduction ratios (RRs, %) of urea, creatinine (Crea), bilirubin (bili), and ammonia (NH 3 ) were calculated from pretreatment and posttreatment serum concentrations. Primary and secondary patient outcome was evaluated.
Results Eight children (fi ve male/three female), 8.6 ± 5.9 years old (range 2 to 15.6 years), BW 32 ± 21 kg, GFR 71 ± 20 ml/minute/1.73 m 2 , with an uncuff ed double lumen dialysis catheter (8 to 14 Fr Femoralis (n = 6) and 9 Fr Jugularis (n = 2)) were treated according to this protocol. In total, 19 sessions were executed using FX40 (n = 13), FX50 (n = 3), and FX60 (n = 3) dialysers during 6.5 ± 0.9 hours. Blood fl ow was 149 ± 45 ml/minute, albumin fl ow 226 ± 49 ml/minute, and ultrafi ltration fl ow 432 ± 517 ml. RRs were 70 ± 15% (urea), 34 ± 14% (Crea), 44 ± 16% (bili), and 36 ± 10% (NH 3 ). Primary survival rate was 100%. Four patients were transplanted (bridge to transplant) of which, however, one died within 30 days after discharge from the ICU. The fi fth patient died due to primary disease 9 months after treatment, and the remaining three patients fully recovered (bridge to recovery). Conclusion This adapted Prometheus® protocol is promising for the treatment of children with liver failure. 
The molecular adsorbent recirculating system (MARS) is used to remove circulating albumin-bound toxins in patients with liver failure. However, the application of MARS has not demonstrated improved survival in randomized clinical trials and the clinical utility has not been fi nally established. In our department, the use of MARS is now restricted to the most critically ill patients with acute or acute on chronic liver failure. We aimed to explore MARS effi cacy in removing toxicity parameters and the safety of the system. Methods Since 2005, we have treated 69 patients (30 males/39 females with median age of 39 years ranging from 1 month to 70 years) listed for liver transplantation with MARS. The median Model of End Stage Liver Disease (MELD) score in patients older than 12 years of age (n = 56) was 33 (interquartile range 26 to 39). The fl ow rate was 35 to 40 ml/ kg/hour and treatment kits were changed every 8 to 12 hours. The patients were treated for a median of 31 hours (range 1 to 240 hours). Results Fifty-fi ve patients (79%) were successfully bridged to transplantation. Nine died before they could be transplanted, and fi ve patients recovered without liver transplantation. Forty-four (81%) of the transplanted patients were alive 30 days after transplantation. Ammonium decreased modestly from a median of 148 to 124 μM (P = 0.03) during MARS treatment. We detected worsening of coagulopathy with signifi cant decreases in platelet count and fi brinogen concentrations, and increase in International Normalized Ratio. Phosphate and magnesium decreased signifi cantly during MARS treatment.
Conclusion Close observation and treatment of coagulopathy and electrolyte disturbances is essential when treating patients with MARS. MARS can reduce and stabilize ammonium and other biomedical markers in patients listed for urgent liver transplantation with high MELD score and liver encephalopathy. It seems that, in some cases and with our settings, the detoxifi cation properties of MARS may be insuffi cient.
First clinical experience with a new type of albumin dialysis: Introduction Liver failure (LF) is associated with prolonged hospital stay, increased cost and substantial mortality. With regard to a limited number of donor organs, extracorporeal liver support is an appealing concept to bridge to transplant or to avoid transplant in case of recovery. A new type of albumin dialysis, the HepaWash® system, was recently introduced. The HepaWash® system provides rapid regeneration of toxin-binding albumin by secondary circuits altering binding capacities of albumin by biochemical (changing pH) and physical (changing temperature) modulation of the dialysate. Methods We evaluated the fi rst 14 patients treated with the HepaWash® system with regard to safety and effi cacy. Seven patients were treated in the context of the run-in phase of the studies (HEPATICUS 1 and HEPATICUS 2) and seven patients were treated since the HepaWash® system received the CE certifi cate in July 2013. Patients treated suff ered under acute on chronic LF (n = 9) or secondary LF which resulted from nonhepatic diseases such as sepsis (n = 5 Conclusion So far the HepaWash® system has proven to be a safe and feasible procedure to eff ectively eliminate water and protein-bound toxins in humans with LF.
Introduction Hospital admission and mortality rates for patients with cirrhosis in the UK are rising [1] . Cirrhotic patients are physiologically challenged and at increased risk of sepsis and death [2] . Mortality rates for cirrhosis in nontransplant ICUs are up to 37% [3] . Increased availability of medical therapies and public expectation places pressure on limited intensive care resources. There is a lack of research into factors used to decide which patients to admit or refer to the ICU. Methods A prospective survey was sent to all consultant gastroenterologists and consultant intensivists in Scotland. Each recipient rated the signifi cance of 18 physiological and social criteria on their decision to refer or admit a patient to intensive care from 1 to 5, with 1 being no infl uence and 5 denoting signifi cant impact on the decision. Recipients listed additional criteria used in their own practice and asked whether they would admit or refer individual grades of Child-Pugh cirrhosis with either a gastrointestinal bleed or sepsis. Results Thirty-fi ve consultant gastroenterologists and 65 intensive care consultants responded, representing a response rate of 34% and 45% respectively. The only criterion given an average rating of 5 by both gastroenterologists and intensivists was Child-Pugh score when stable. Presence on the transplant list, referral secondary to bleeding varices, recent discharge from the ICU, abstinence from alcohol, nutritional status, age under 30 and more than one additional organ failure all scored 4 or 5 from both groups. Sex, employment, smoking or drug use, deprivation and positive virology status did not infl uence the decision to refer or admit patients. Clinicians reported compliance with medication and outpatient appointments plus an obvious precipitant factor as important features in their decision. The majority of respondents would refer or admit all grades of Child-Pugh cirrhosis with gastrointestinal bleeding. Most would refer or admit Child-Pugh A or B with sepsis. A total 76.5% of gastroenterologists would refer Child-Pugh C cirrhosis with sepsis but only 33.3% of intensivists would accept. Conclusion Referral and admission decisions for patients with cirrhosis are multifactorial. Child-Pugh status when stable appears to be of greatest signifi cance. The diff erence in opinion of admission of patients with Child-Pugh C with sepsis requires further evaluation.
Introduction Cirrhotic patients admitted to the ICU are usually regarded as having a particularly poor prognosis when compared with other groups of critically ill patients. The aim of our study was to evaluate the prevalence, case mix and outcomes of patients with cirrhosis admitted to the general ICU of a nontransplant center. Methods Data were collected from a running ICU database. We studied cirrhotic patients admitted to the ICU between January 2013 and November 2014. Results A total of 30 patients with cirrhosis were admitted, accounting for 3% of total ICU admissions. Mean age was 54.5 years, with a male preponderance (76.7%). The main cause for cirrhosis was alcohol (53.3%), followed by alcohol plus chronic hepatitis C virus (HCV) infection (20%) and HCV virus infection alone (13.3%). The most common causes for admission were sepsis/septic shock (26.7%), surgical (23.4%), gastrointestinal bleeding and hepatic encephalopathy (16.7% each). At admission, these patients presented an average Model for End-Stage Liver Disease score of 23.5 ± 10.4 with 70% classifi ed as grade C in the Child-Pugh score; mean Acute Physiology and Chronic Health Evaluation (APACHE II) 29.2 ± 8.7 and new Simplifi ed Acute Physiology Score (SAPS II) 62.7 ± 29. Regarding organ failure at admission, the mean Sequential Organ Failure Assessment score was 12.8 ± 4.5. The ICU mortality of these patients was 43.3% and hospital mortality was 53.3%. The variables at admission that related signifi cantly with ICU mortality were: all scores described except for Child-Pugh score, bilirubin, the International Normalized Ratio, creatinine, bicarbonate, lactate, pH and the use of renal replacement therapy during the ICU stay (P <0.05). The mortality rate of cirrhotic patients was superior to the general ICU mortality (43% vs. 26%). However, patients with cirrhosis presented signifi cantly higher severity scoring systems (APACHE II; SAPS II) at admission compared with noncirrhotics, with high prevalence of organ dysfunction as assessed by SOFA score.
Conclusion The high severity of disease in conjunction with the high mortality rate observed in this group of patients should make us consider the possible benefi ts of earlier referring/admission to the ICU, ideally before multiorgan failure arises. On the other hand, in nontransplant centers where cirrhotic patients constitute a small percentage of total ICU admissions, the complexity and peculiarities of the management of these patients should prompt their early transfer to a specialized center.
The aim was to study the evolution and incidence of intraabdominal hypertension in critical burn patients using a slightly restrictive fl uid therapy protocol based on monitoring transpulmonary thermodilution and lactic acid. Methods A prospective study of 132 consecutive patients admitted to the Critical Burn Unit between October 2008 and October 2011. In all of them resuscitation was performed by objectives: blood pressure (>65 mmHg), hourly diuresis (0.5 to 1 cm 3 /kg), lactic acid clearance and thermodilution transpulmonary parameters (CI >2.5 l/minute/m 2 , ITBI: 600 ml/m 2 ). We performed measurements of IAP with a bladder catheter every 8 hours in the fi rst 72 hours. Results Ninety-eight men and 34 women were studied. Mean age 48 ± 18 years and a TBSA of 35 ± 22%. The fl uid provided by %TBSA in the fi rst 8 hours was less than predicted by Parkland (4.05 ml/kg), although the total contribution in the fi rst 24 hours was similar. The evolution of the intra-abdominal pressure was: admission 9.7 mmHg, Conclusion IAH incidence when a slightly restrictive fl uid protocol used is less than expected. Introduction Intra-abdominal hypertension (IAH) is frequent in the ICU and has been associated with adverse outcomes and worse prognosis. The purpose of our study was to assess risk factors for IAH and prognosis of major injured patients during burn resuscitation. Methods Adult burned patients with a burn injury exceeding 20% of total body surface area, from 1 April to 30 November 2013, were included. IAP was measured when IAH was suspected, according to the Kron method via the Foley catheter. Monitoring of IAP was performed every 6 hours during 5 days until normalization.
Results Twenty patients were enrolled in the study. The mean age was 36 ± 13 years. There were 14 males and six females. The average TBSA was 44 ± 17%. Screening and monitoring of IAP were applied by: oliguria (42%), abdominal distension (31.5%) and gastrointestinal trouble (21%). IAH occurred between day 2 and day 3 after early burn resuscitation, respectively in 52% and 63%. IAH was observed in 69% of cases in patients admitted to the ICU with a delay of 1.6 days post burn injury. IAH was noted in 13 patients; of these, fi ve patients developed an abdominal compartment syndrome. The mean IAP was 16 ± 7 mmHg. Patients were assigned into two groups: G1 (IAH+; n = 13) and G2 (IAH-; n = 7). Comparative study of the two groups shows that HIA increases signifi cantly body weight gain within the fi rst 5 days after injury: 8 kg for G1 versus 2 kg for G2 (P = 0.04), occurrence of ARDS (70% for G1 vs. 16.7% for G2, P = 0.02), respiratory failure (77% for G1 vs. 28.5% for G2, P = 0.06), shock (70% for G1 vs. 16.7% for G2, P 0.02) and mortality (61.5% vs. 50%).
Conclusion IAH was frequent in early burn resuscitation of major injured patients. It seems to be associated with fl uid overload in burns and contributes to organ damage.
The aim of our study was to investigate the eff ects of sepsis on respiratory mechanics in a porcine model of intra-abdominal hypertension (IAH). Methods Sixteen pigs were divided into two groups of eight (G-A/ G-B). All animals received general anesthesia and were mechanically ventilated. Parameters recorded included respiratory system, chest wall and lung compliance (CRS, CCW, CL) and respiratory system and chest wall inspiratory and expiratory resistances (RRSisp, RRSexp, RCWisp, RCWexp). After baseline measurements (0 minutes), intraabdominal pressure IAP was raised by helium insuffl ation to 25 mmHg in both groups and remained at that level for the whole study. In G-B, sepsis was induced 60 minutes after IAP increase, by i.v. administration of Escherichia coli endotoxin. Parameters were recorded every 20 minutes. The last measurement was made at 180 minutes, right after deinsuffl ation, and IAP return to baseline levels.
Results CRS decreased statistically signifi cantly in both groups after IAP increase and increased after deinsuffl ation only in G-A. Similarly, CCW decreased in both groups but returned to baseline values in both groups after deinsuffl ation. CL decreased more signifi cantly in G-B and returned to baseline values only in G-A. RRSisp increased only in G-B and did not decrease after deinsuffl ation, whereas RRSexp increased in both groups, in a more signifi cant manner in G-B, and decreased only in G-A after deinsuffl ation. RCWisp and RCSesp did not show any alterations during the study period. Results are depicted as mean values ± SD in Tables 1 and 2 . Conclusion Both sepsis and IAH have negative eff ects on respiratory mechanics. However, their combination has even more detrimental eff ects, which do not ameliorate after deinsuffl ation. Introduction CD73/ecto-5'-nucleotidase is an enzyme that generates adenosine, which dampens infl ammation and improves vascular barrier function in several disease models. CD73 also circulates in a soluble form in the blood [1] . We studied whether levels of soluble form of CD73 and cytokines/chemokines predict the development of organ failure in acute pancreatitis [2, 3] . Methods Altogether, 161 patients with acute pancreatitis (107 were subclassifi ed according to the revised Atlanta criteria into mild, 29 into moderately severe and 25 into severe forms) were studied. Serum and blood cell samples were collected at admission. Protein levels of soluble form of CD73 in serum were determined using a novel enzymelinked immunosorbent assay, activity of soluble form of CD73 using radioactive enzyme assays, and CD73 messenger RNA levels from leukocytes using quantitative PCR. Serum levels of 48 cytokines and growth factors were determined using Bio-Plex Pro Human Cytokine Assay 21-plex and 27-plex magnetic bead suspension panels. Results Activity and protein concentration of soluble form of CD73 and messenger RNA level of CD73 all decreased along with the disease severity (P ≤0.01 for all). The activity of soluble form of CD73 at admission predicted the development of severe pancreatitis in diff erent groups of the patients. Especially, activity of soluble form of CD73 was better than C-reactive protein or creatinine in predicting the severity of pancreatitis in the group of patients without any signs of organ failure at admission. In subgroup analyses of patients with severe pancreatitis and without organ dysfunction upon admission, IL-8, hepatocyte growth factor and granulocyte colony-stimulating factor (G-CSF) levels predicted the development of severe pancreatitis, with G-CSF being the most accurate cytokine. Conclusion Activity of soluble form of CD73 and levels of certain cytokines at admission to the hospital have prognostic value in predicting the development of the severe form of acute pancreatitis. The possibility that combining them with other prognostic markers might improve prognostic accuracy requires further studies.
Introduction Heterogeneous published results led us to conduct a clinical trial to assess the effi cacy of a new formulation of four probiotics (P) as prophylaxis for complications after colorectal surgery. Methods A double-blind, placebo-controlled randomized study was conducted enrolling patients undergoing colorectal cancer surgery. Placebo or a formulation of L. acidophilus, L. plantarum, B. lactis and S. boulardii was administered starting 1 day before operation and continuing for 15 days post operation. Patients were followed-up for 30 days with the development of postoperative complications as the primary outcome. PAXGene tubes and serum were collected on postoperative day 4 for measurement of gene expression and serum cytokines (ClinicalTrials.gov NCT02313519). Results Administration of P signifi cantly decreased the rate of all postoperative major complications (28.6% vs. 48.8% of placebo, P = 0.010, odds ratio: 0.42). Major benefi t was found in the reduction of the postoperative pneumonia rate (2.4% vs. 11.3%, P = 0.029), of wound infections (7.1% vs. 20.0%, P = 0.020), of anastomotic leakage (1.2% vs. 8.8%, P = 0.031) and of the need for mechanical ventilation (20.2% vs. 35.0%, P = 0.037). The time until hospital discharge was shortened as Critical Care 2015, Volume 19 Suppl 1 http://ccforum.com/supplements/19/S1 S137 well. Gene expression of SOCS3 was positively related with circulating IL-6 in the P group but not in the placebo group (Figure 1 ). Conclusion The studied P formulation signifi cantly decreased the risk of postoperative complications, namely mechanical ventilation, infections and anastomotic leakage. Modulation of the gene expression of SOCS3 is one suggested mechanism.
Introduction In this study, we aimed to determine the success rate of nasoenteric tube (NET) insertion into the postpyloric area by ultrasonography (USG) and compare it with the commonly used method, direct abdominal radiography.
Methods Patients admitted to an adult ICU between April and July 2014 with an indication for NET insertion for enteral feeding were included in the study after informed consent was given from patients' relatives. Nasoenteric feeding tubes were placed using the blind bedside method by a single anesthesiologist. Any motility stimulant agent was not used. The outside of the polyurethrane 8 F with unweighted NET (Bexen, Spain) and its guiding wire were lubricated with gel. The NET was inserted into the nostril after determination of the mouthposterior ear-xiphoid distance and pushed on at least such a distance. Followed by auscultation of the gastric area and air infusion of 30 to 50 ml into the tube, the patient was positioned on their right side and the tube was advanced 20 to 30 cm more. Then the guiding wire inside the NET was removed. The patient was then brought to the supine position and NET was visualized by two radiologists simultaneously by M5 portable USG (Mindray, PRC), with a 3.5 MHz convex probe whether it passes through the postpyloric area or not. Localization of the tube was confi rmed with abdominal radiography in all patients. During the fi rst insertion of the NET, the ratios for inaccurate localization and correct placements through the postpyloric area were recorded and results were compared with abdominal radiography. Results In this study, the bedside blind method was used for NET insertion into 34 patients. Eleven of the tubes were detected passing through the postpyloric area by USG. In one case the NET could not be seen in the postpyloric area by USG, but it was detected in the postpyloric area by control abdominal radiography. In 22 patients, NETs were detected in the stomach with control abdominal radiography. Success for NET placement with the bedside blind method and USG imaging was 35% versus 91.6%, respectively. Conclusion The success rate of the bedside blind method in the NET placement was low. It is clear that if any other placement techniques with high success rate will be applied, USG will be useful in a higher number of patients reducing the need for abdominal radiography.
The objective was to determine whether the administration of thiamine mitigates elevated lactate levels in patients with septic shock. Thiamine is essential for aerobic metabolism and we have found that thiamine levels are low and inversely correlated with lactate levels in patients with sepsis. Methods We performed a randomized, double-blind, placebocontrolled, two-center trial from January 2010 to October 2014. We enrolled patients with septic shock, elevated lactate (≥3 mmol/l) and no obvious competing cause of lactate elevation. Patients received thiamine 200 mg or placebo i.v. twice/day for 7 days. The primary outcome was lactate levels at 24 hours. Secondary outcomes included the SOFA score at 24 hours and mortality. Lactate levels at 24 hours were compared between groups using the Wilcoxon rank-sum test and categorical variables were compared using the Fisher's exact test. Lactate values at 24 hours, for those who died before 24 hours, were imputed according to a predefi ned plan. We performed a preplanned analysis in those with baseline thiamine defi ciency (≤7 nmol/l). Results We enrolled 88 patients; 43 received thiamine and 45 placebo. Baseline characteristics were similar between groups. We found no overall statistical signifi cant diff erence in 24-hour lactate levels between thiamine and placebo groups (2.5 (IQR: 1.5 to 3.4) vs. 2.6 (IQR: 1.6 to 5.1), P = 0.40). Fewer patients in the thiamine group had lactate levels >4 mmol/l at 24 hours (21% vs. 38%, P = 0.10) and this was statistically signifi cant if only evaluating survivors at 24 hours (7% vs. 33%, P = 0.03), although our preplanned analysis was to impute data. We found no diff erence in 24-hour SOFA score or mortality. A total of 28 (35%) patients were thiamine defi cient. Of the defi cient patients, those receiving thiamine had statistically signifi cant lower lactate levels at 24 hours (2.1 (IQR: 1.4 to 2.5) vs. 3.1 (IQR: 1.9 to 8.3), P = 0.03) and more patients in the placebo group had a lactate >4 mmol/l (38% vs. 7%, P = 0.07). Mortality in the thiamine and placebo groups was 13% and 46%, respectively (P = 0.10).
Conclusion Thiamine defi ciency is prevalent in septic shock. Thiamine did not decrease overall median lactate levels at 24 hours. In the patients with thiamine defi ciency, there were statistically signifi cant lower lactate levels at 24 hours in the thiamine group and a large, although nonsignifi cant, diff erence in mortality.
Introduction Malnutrition is common in hospitalized medical patients and is associated with poor clinical outcomes. Whether malnutrition has a direct link to adverse outcomes or is rather a mirror of the severe patient condition remains debated. Our aim was to study the association of acute and chronic malnutrition status with blood biomarkers from diff erent pathophysiological concepts to better understand the underlying mechanisms of malnutrition. Methods We prospectively followed consecutive adult medical inpatients hospitalized between February 2013 and October 2013 in a tertiary care Swiss hospital. Nutritional risk was assessed using the Nutritional Risk Screening (NRS 2002) score, which incorporates acute and chronic measures of malnutrition. Multiadjusted regression models were used to investigate the associations between acute and chronic nutritional risk and biomarkers mirroring infl ammation (CRP, PCT, proADM, leucocytes), stress (copeptin), renal dysfunction (creatinine, urea), nutritional status (vitamin D25, albumin, calcium, glucose), and hematological function (platelets, INR, Hb, RDW). Biomarker levels were transformed into deciles due to skewed distributions. Results A total of 529 patients (mean age 72 years, 57.1% male) were included. Overall, there was a signifi cant association of NRS and most biomarkers of infl ammation, stress, renal function, nutrition and the hematological system (coeffi cient and 95% CI): CRP 0.021, P = 0.0021, PCT 0.28, P = 0.003, proADM 0.4, P <0.001, copeptin 0.44, P <0.001, urea 0.28, P = 0.002, vitamin D25 -0.23, P = 0.012, albumin -0.6, P <0.001, hemoglobin -0.5, P <0.001, RDW 0.46, P <0.001. These associations remained robust after adjustment for sociodemographics (model 1), comorbidities (model 2) and main medical diagnosis (model 3). Subgroup analysis suggested that mainly the acute part of malnutrition and not chronic malnutrition was associated with an increase in biomarker levels.
Conclusion Acute malnutrition was associated with a pronounced infl ammatory response and an increase in biomarkers from diff erent pathophysiological systems which may partly explain the link between malnutrition and adverse medical outcomes. However, interventional trials are needed to prove causal relationships.
The provision of nutrition in the critical care unit (CCU) has shifted from nutrition support to nutrition therapy, and the potential benefi ts derived from this in the recovery of the critically ill is being explored [1] . We audited the management of nutrition in the CCU in a South African Hospital against the American Society of Parenteral and Enteral Nutrition Guidelines. Furthermore, we reviewed the knowledge and confi dence of healthcare providers in the management of nutrition in the CCU. Methods Retrospective data collection of patients admitted to a fourbed CCU over a 4-month period in 2013. A survey was distributed to diff erent disciplines involved in patient nutrition in the CCU.
Results Seventy-two patients were admitted to the CCU during this time period, and notes were able for 44. Three paediatric patients were excluded. Twenty-nine patients stayed for 2 or more days (the audit population). The median age of the audit population was 38, 19 were female. Sixteen were postoperative admissions. The median APACHE II score of the patients with suffi cient available data (n = 16) was 14 (range 6 to 34). The audit found that 21 of the patients had nutrition started in the CCU, with 15 having nutrition started within 48 hours. Only eight patients had a nutritional assessment done. A total of 45 responded to the survey: eight anaesthetists, 25 from surgical disciplines, seven CCU nurses, and fi ve dieticians. All agreed that nutrition should be started in the fi rst 48 hours, except from the surgeons only 14 (56%) agreed. The average self-rating of knowledge of nutrition management in the CCU (1 = lowest, 5 = highest) was 2.1 with the dieticians and CCU nurses showing the highest confi dence with 3.4 and 2.6, respectively. The anaesthetists rated their knowledge at 1.9 and the surgeons rated themselves at 1.8.
We found that there is poor management of nutrition in the CCU. This is paired with limited knowledge and low confi dence in management amongst the attending staff . Evidence would suggest that the development and dissemination of clear hospital guidelines could improve rates of correct management [2] . However, the lack of uniform guidance based on strong evidence from the leading global authorities on nutrition suggests that, in order to improve implementation of adequate nutrition, more research is urgently required.
Introduction Optimizing enteral nutrition early has been shown to be benefi cial in critically ill patients. However, underfeeding is still a common problem. The critically ill surgical patient often presents with additional challenges to optimal enteral feeding. The objective of this study was to improve enteral feeding practices in a surgical ICU. Methods The Clinical Practice Improvement Programme is a local quality improvement initiative involving a multidisciplinary team aimed at identifying and improving defi ciencies in the process of delivery of care. A team led by an intensivist, consisting of doctors, surgeons, nurses and a pharmacist, was formed to improve enteral feeding practices in a surgical ICU. The quality improvement methodology was employed.
An audit was carried out to determine the problem of underfeeding in the unit. Root cause analyses were conducted and team members identifi ed key barriers to optimal feeding and areas for improvement. Protocols were developed to standardize and encourage early enteral feeding as well as to reduce the time feeds are interrupted for patients who were going for surgeries or for various other reasons. Educational interventions were conducted with lectures to physicians and nurses. Visual aids in the form of screensavers at each bedside computer served as reminders to the team to optimize feeding. A subsequent audit was then conducted to determine the improvement in achieving the desired outcomes, namely the amount of calories and proteins received as well as the proportion of patients who achieved >70% of their target calories and proteins. We considered target calories to be 25 kcal/kg/day and target proteins to be 1.5 g/kg/day. Results Patients received more calories (78.3% vs. 59.1%) and more proteins (70.2% vs. 54.6%) post implementation. The mean percentage of patients in the post group who achieved >70% of required calories was 80.1% versus 30.9% in the pre group. The mean percentage of patients who achieved >70% of required proteins was 58.3% versus 32.1% in the pre group.
Conclusion The multipronged approach of the quality improvement methodology helped to increase the provision of calories and proteins in our population of critically ill surgical patients. However, there is still room for improvement in terms of achieving optimal enteral nutrition targets early in our population. There is also a need to look into sustaining such results. 
The NUTRIC score is a tool designed to quantify the risk of critically ill patients developing adverse events that may be modifi ed by aggressive nutrition therapy, in the general population of an ICU. Cancer patients are more prone to be at nutritional risk due to the disease and treatment complications. Our aim was to characterize NUTRIC score behavior in the population of patients admitted to an oncologic ICU. Methods Between January and June 2014 we applied the NUTRIC score to all patients, age >18 years, without cerebral death criteria and with a length of stay (LOS) >72 hours. Data were collected and analyzed using SPSS v20.0. To evaluate the impact on mortality we used logistic regression.
Results Sixty-nine patients were included, 23 women (33.3%) and 46 men (66.7%). Most patients were aged between 50 and 75 years (72.5%) and had normal range weight 58% (n = 40). The mean LOS was 11.56 (minimum: 3 to maximum: 69). The most common motive for admission was sepsis (7.7%, n = 26). APACHE II score was above 15 in 77% of the patients (n = 53) and SOFA score was superior to 6 in 56.5% (n = 30). The NUTRIC score was low risk in 42% (n = 29) of the patients and high in 58% (n = 40). Twenty-eight-day mortality was 26.1% (n = 18). A high NUTRIC score corresponded to a 22-fold increased odds of dying in the fi rst 28 days (P <0.001). Both APACHE II and SOFA were mortality predictors alone, with an increase of 1 point in APACHE score corresponding to an increase of 14% (P = 0.002) and an increase of 1 point in SOFA corresponding to an increase in the odds of being dead at 28 days (P = 0.002). Body mass index, age, number of comorbidities, and days in the ICU did not correlate with mortality. Conclusion The NUTRIC score is a good tool in cancer patients to predict 28-day mortality. Nevertheless, the only compounds of the NUTRIC score that correlated independently with mortality were APACHE II and SOFA scores. Further investigation towards the inclusion of other categories such as tumor staging and the type of tumor could be useful to develop a specifi c prognostic tool for this population.
Introduction Critically ill patients experience hypermetabolism increas ing substrate utilization, especially glucose oxidation. Glycogen is the main source of glucose in the body, being 85% and 15% stored in skeletal muscle and liver respectively. Since glycogen stores are limited we evaluated the hypothesis that critical illness could be associated with glycogen depletion leading to skeletal muscle catabolism for gluconeogenesis and eventually resulting in cachexia, an important determinant of future ICU survival and ICU-acquired weakness.
Methods Nine critically ill patients (58.75 ± 25 to 75 years old) with an ICU stay from 1 day to 5 weeks were evaluated for skeletal muscle glycogen content using a rapid, non-invasive high-frequency ultrasound methodology (MuscleSound®, Denver, CO, USA). Scans were obtained from the rectus femoris and vastus lateralis muscles. Glycogen content was measured with a score from 0 to 100 according to the MuscleSound® scale. Patients had a variety of primary diagnoses including septic shock (n = 3), hemorrhagic shock/abdominal hypertension (n = 1), hypovolemic shock/post major oncologic surgery (n = 1), trauma (n = 3), and burn injury (n = 1). Results Six out of nine patients had no glycogen present in the muscle (score = 0). The other three patients had glycogen scores between 5 and 15 which are well below scores of healthy individuals (reference 50 to 70). As a comparison we collected post-competition levels in competitive athletes, which decrease their glycogen stores (score 15 to 25) but are well above those of most critically ill patients we have studied.
Conclusion This is the fi rst time that muscle glycogen stores have been evaluated in critical illness. Our data show severe glycogen depletion in ICU patients which probably leads to muscle catabolism necessary for gluconeogenesis, eventually resulting in cachexia. This fi nding poses severe metabolic challenges for ICU patients in which interfering with recovery can contribute to poor survival. In light of our fi ndings, re-evaluation of nutritional protocols and potential anabolic/anticatabolic therapy to decrease muscle catabolism may improve survival. Diff erent therapeutics that may prevent hypermetabolism (such as beta-blockers) should be re-evaluated along with anabolic agents (that is, oxandrolone) which could counteract the severe catabolic response in critical illness. Monitoring of muscle glycogen repletion could signal the transition from the catabolic to anabolic phase. Introduction Low plasma glutamine concentrations are associated with unfavourable outcome at acute ICU admission. We questioned whether there is a diff erence in plasma glutamine level after acute or elective ICU admission. Methods We performed a single-centre prospective observational study in a 22-bed mixed ICU. Exclusion criteria were age <18 years and total parental nutrition at admission. Patients were divided into two groups: elective surgery and acute admissions. Blood samples were taken at ICU admission and daily at 6.00 a.m. Glutamine levels were measured using the Bioprofi le 100 plus analyser (Nova Biomedical UK, Cheshire, UK). A Mann-Whitney U test was used to detect diff erences between groups and a Bonferroni method to correct for multiple comparisons.
We included 88 patients after elective surgery (76 cardiac and 12 general surgery) and 90 patients after acute admission (27 sepsis, 17 acute surgery, two trauma and 44 medical). Baseline characteristics are presented in Table 1 . Plasma glutamine levels at admission were signifi cantly lower in acute patients compared with elective surgery, 0.25 mmol/l (IQR 0.09 to 0.37) versus 0.43 mol/l (IQR 0.33 to 0.55) Figure 1 (abstract P399) . Indirect calorimetry measured signifi cantly higher resting energy expenditures.
(P <0.001). There appeared to be a signifi cant correlation between the APACHE IV score and glutamine levels (R = 0.52, P <0.001). Moreover, in a backward linear regression analysis this correlation was independently associated with APACHE IV scores and the presence of infection, but not with the type of admission. Conclusion Plasma glutamine levels were signifi cantly lower after acute admission compared with elective surgery. In both groups a considerable amount of patients had decreased glutamine levels, but this was not independently associated with the type of admission.
In contrast to previous studies we found that glutamine levels were determined by severity of illness and the presence of an infection.
Introduction Intravenous fi sh oil (FO) lipid emulsions (LEs) are rich in ω-3 polyunsaturated fatty acids, which exhibit anti-infl ammatory and immunomodulatory eff ects. We previously demonstrated that FOcontaining emulsions may be able to decrease mortality and ventilation days in the critically ill. Over the last year, several additional randomized controlled trials (RCTs) of FO-based LEs have been published. Therefore, the purpose of this meta-analysis was to update our systematic review aimed to elucidate the effi cacy of FO-based LEs on clinical outcomes in the critically ill. Methods We searched computerized databases from 1980 to 2014. Overall mortality was the primary outcome and secondary outcomes were infections, ICU and hospital length of stay (LOS), and mechanical ventilation (MV) days. We included RCTs conducted in critically ill adult patients that evaluated FO-based LEs in parenteral nutrition (PN) or enterally fed patients. We analyzed data using RevMan 5. Introduction This study aimed to describe the patient characteristics and prehospital factors associated with an ICU admission from the ED.
There is a paucity of information about the early recognition of critical illness by paramedics; especially in the Australian prehospital setting. Methods A retrospective cohort study, July 2012 to June 2014, conducted in the Perth metropolitan area, which is served by a single ambulance service. Adult patients were included if transported to a public hospital ED that used the ED information system (EDIS) (seven of eight EDs) and were admitted to the ICU from the ED (ED-ICU group). Patients aged <16 years, those from rural areas or transfers were excluded. We used existing ambulance clinical data linked to EDIS data. Prehospital cohort characteristics are described using univariate statistical techniques. Logistic regression was conducted with admission to the ICU from the ED (critical illness surrogate) as the outcome variable. Variables included in regression models were age, sex, paramedic-identifi ed urgency, that is the time patients should be seen by a doctor based on the Australasian Triage Scale, paramedicidentifi ed patient problem and the time taken from the ambulance service receiving the call to hospital arrival. Physiological variables: systolic blood pressure (SBP), heart rate (HR), respiratory rate (RR), temperature, oxygen saturation, and GCS were included in the logistic models. Introduction We aimed to identify the adequacy of assistance provided and to assess correct anaesthetic equipment and drug availability at emergency calls made in the ICU and in remote areas of the hospital. Emergency calls often involve managing critically ill patients with the highest mortality results. The importance of a clinical team with the necessary competencies and the right level of resources are paramount. Methods We undertook a prospective survey of all adult patients with emergency calls put out to the anaesthetic team in a London district general hospital over a 6-week period. We performed a snapshot audit of equipment in resuscitation trolleys across each ward and in the radiology department. We compared the data collected on available equipment with the standard set by the Resuscitation Council (UK) Recommended Minimum Equipment Checklist [1] . The survey addressed the availability, clinical competency and appropriate duration of stay of the anaesthetic assistant at the emergency calls.
Further qualitative data were collected on the availability of required emergency drugs.
Results During the study period 44 emergency calls were attended. Twenty-three (52%) of these calls were in the accident and emergency department, and four (9%) in the ICU. Survey results demonstrated two cases where no anaesthetic assistant arrived at the emergency call put out to them. In cases where timely assistance was available, the assistant did not have the adequate clinical and anaesthetic skills required by the attending physician. In 6% of cases where skilled assistance was required (n = 2), it was felt that the assistant did not stay for the clinically required length of time. Emergency drugs required were found to not be available in 11% of cases (n = 5) and in 17% of cases (n = 6) the necessary emergency equipment was not available. Data were collected on equipment from 17 resuscitation trolleys. The inadequacies identifi ed were the oxygen cylinders were fi lled less than 75% full in 41% cases (n = 7) and end-tidal capnography was identifi ed to be absent. Conclusion Emergency calls require standards to be met involving the competency of responding team members and adequate resources. This leads us to question whether guidelines should exist regarding the clinical competency and timeliness of the assistant available to the physician at emergency calls. Reference Introduction There is widespread concern regarding excess mortality for patients admitted to hospital out of hours. We introduced an electronic track and trigger system (Patientrack) with automated alerts in a large university teaching hospital between 2010 and 2012. The system computes the patient's early warning score and alerts medical staff via a pager. It is operational 24 hours a day, 7 days a week and could be an eff ective tool to reduce variations in mortality throughout the working week. Based on analysis of responses, a focused three-pronged intervention was formulated and implemented hospital wide. The arms of the intervention were: identifi cation of the name and role of each staff member using highly visible labels; role allocation according to policy written through a multidisciplinary working group; and a time out during the response allowing a structured synopsis of the patient's current status to be communicated to the team. The intervention was preceded by extensive staff education, and 175 staff (50%) completed the survey 6 months later to assess its success.
The intervention signifi cantly increased satisfaction amongst staff regarding: identifi cation of the team leader and other key staff members at the response; and time out eff ectiveness in reducing repetition and improving staff understanding of the patient's status and medical issues. We found no signifi cant change in staff perceptions regarding the clarity of the ongoing treatment plan at the end of the MET call response. Conclusion Utilising a low-cost intervention in a regional setting, we were successful in improving staff perceptions of role allocation and communication within our MET call responses. The intervention also led to signifi cantly increased overall satisfaction with the MET call system. Through our surveys we have identifi ed other facets of the MET call response that also require attention. Given our encouraging results we are designing a follow-up intervention incorporating structured multidisciplinary training in MET call scenarios. Introduction A medical emergency team (MET) was introduced in our institution in January 2012 to provide timely response to the needs of acutely ill inpatients and cardiac arrest calls. The MET assesses the patient and prescribes a management plan for the responsible team to follow; promptly stabilising and transferring patients to a place of safety where required. We aimed at evaluating the eff ects of introducing the MET on clinically relevant processes and outcomes. Methods Prospective data were analysed using STATA 10.1. The primary outcome measure was immediate mortality (defi ned as mortality at Cardiovascular illness was the predominant baseline morbidity but none of the baseline illness showed increased risk of mortality in this group. Among true cardiac arrest events, 92.6% was due to pulseless electrical activity/asystole and 7.7% was due to ventricular fi brillation (VF)/pulseless ventricular tachycardia (VT); both of these did not have any diff erence on the initial outcome. But having an initial rhythm of VF/pulseless VT had 90% more chance for discharge from the hospital, with P = 0.04. Although arrival time of the CBT did not have any infl uence on the fi nal outcome, duration of resuscitation ≤20 minutes had an odds ratio of 10.6 with P <0.001 favoring return of spontaneous circulation over death after controlling for age. Of the 203 patients who had true cardiac arrest events, 43 (21.2%) were discharged from the hospital. Good neurological outcome at discharge was seen among 22 (10.8%) of the patients based on Cerebral Performance Category Score. Conclusion Our experience shows that out of every 1,000 patients admitted to our hospital, about fi ve sustained cardiac arrest, of whom only 11.3% survived to hospital discharge with good neurological recovery. Variation in the eff ectiveness of the cardiopulmonary resuscitation quality in comparison with world data could be due to the inherent diff erence in the severity of the primary illness in the patients and diversity in the reported data. References practices relating to CPR and AED training in London secondary schools.
Methods We conducted a registered audit of CPR and AED training in London schools. Secondary schools were identifi ed via web links for each of the London Borough Councils. Telephone interviews with school staff familiar with CPR and AED training practices were conducted prospectively using a standardized web-based survey. All survey response data were captured electronically. We defi ned universal training as any programme which delivers CPR and AED training to all students in the school. We used simple descriptive statistics to summarise the results. Results A total of 51 schools completed the survey covering an estimated student population of 54,037. There were four (8%) schools that provide universal training programmes and an additional 23 (45%) off er optional training programmes for students. There were 16 (31%) schools which have an AED available on the school premises. The most common reasons for not having a universal CPR training programme is the requirement for additional class time (15/51; 29%) and that funding is unavailable for such a programme (12/51; 24%). There were three students who died from sudden cardiac arrest over the period of the past 10 years. anesthetized domestic pigs. Before induction, the animals were randomized to receive either 1,000 ml (34 ± 3 ml/kg, group A, n = 7) or 500 ml (16 ± 2 ml/kg, group B, n = 7) of balanced crystalloid solution or to undergo no fl uid loading during CPR (group C, n = 5). After spontaneous circulation (ROSC) was restored, the animals were observed for 90 minutes.
Results In all groups, signifi cant increase of intracranial pressure followed by its decrease after ROSC was observed. While in groups B (from 12 ± 2 to 18 ± 2 mmHg, P <0.05) and C (from 13 ± 1 to 18 ± 3 mmHg, P <0.05) it was comparable (P >0.05), the rise of intracranial pressure in group A was signifi cantly higher (from 12 ± 3 to 23 ± 3 mmHg, P <0.05). Whereas coronary perfusion pressure was lower in group A than in control group C during volume loading, fl uid infusion induced its mild increase in group B (group A: 12. Conclusion ICU survivors were more likely to have had a shockable rhythm, been to the cathlab, received PPCI and TH and been less sick than nonsurvivors, but these may simply refl ect selection and other biases. Any benefi t these factors did confer to cardiac patients may have been off set by our liberal ICU admission policy to OHCAs with nonshockable rhythms. However, access to 24-7 PPCI may determine survival and we suggest that all OHCA patients should be taken directly to regional heart attack centres.
Introduction Mild hypothermia and fever control have been shown to improve neurological outcomes post cardiac arrest. Common methods to induce hypothermia include body surface cooling and intravascular cooling; however, a new approach using a catheter placed into the esophagus has recently become available.
Methods We report the fi rst three cases of temperature control using an esophageal cooling device (ECD). The ECD was placed orally in a similar fashion to orogastric tubes. Temperature reduction was achieved by connecting the ECD to a commercially available heat exchange unit (Blanketrol II or III).
The fi rst patient, a 59-year-old male (73 kg), was admitted after successful resuscitation from a protracted out-of hospital cardiac arrest. His initial temperature was 35°C, which is within our current institutional protocol of 34 to 36°C. Several hours after arrival, his temperature slowly increased to 35.8°C despite application of a cooling blanket and ice packs to the groin and axilla. The ECD was inserted and a reduction of temperature to 34.8°C was achieved within 3 hours. The patient expired on day 3. The second patient, a 54-year-old female (95 kg), was admitted after resuscitation from an out-of-hospital PEA arrest. Despite initiating our cooling protocol with surface-cooling blankets and cold intravenous saline, she mounted a fever peaking at 38.3°C shortly after admission. After ECD insertion and confi rming the external heat exchanger connection, her temperature gradually dropped to 35.7°C over a period of 4 hours. She subsequently made a favorable neurological recovery and was discharged home at day 23. The third patient, a 47-year-old male patient (86 kg) presented with an ongoing fever secondary to necrotizing pneumonia in the postoperative period after coronary artery bypass grafting. His fever was unresponsive to empiric antibiotic therapy, antipyretics, cooling blankets, and ice packs. ECD insertion resulted in a decrease in temperature from 39.5°C to 36.5°C in less than 5 hours. The patient eventually made a full recovery and was discharged home after 59 days. In all three patients, placement of the device occurred in less than 3 minutes and ease of use was reported as excellent by nursing staff and physicians. Conclusion The ECD is a novel technology that can be used for temperature management post cardiac arrest and for fever control in critically ill patients. Despite patients mounting a febrile response, temperature control was achieved and maintained successfully. The device was reported as being easy to use, by both physicians and nurses.
Introduction Clinical outcomes vary depending on the method used to induce therapeutic hypothermia following stroke or cardiac arrest. In swine, we tested the hypothesis that selective nasopharyngeal brain cooling (SNBC), in contrast to systemic hypothermia, adversely impacts cerebral perfusion. Methods In two groups of animals (34 to 35 kg), blood fl ow in the right middle cerebral artery (MCA) was measured using transcranial Doppler (TCD). In group 1, SNBC was initiated using perfl uorohexane aerosol (1 ml/kg/minute) and oxygen (1 l/kg/minute) through a nasopharyngeal catheter atomizer. In group 2, the animals were body surface cooled using water-circulating blankets (4°C). In both groups, brain temperature, intracranial pressure (ICP), core temperatures and vital signs were continuously recorded. Cooling was terminated once the brain reached 32°C and the animals were allowed to passively rewarm.
In the SNBC group, the brain target temperature was reached in 54 ± 11 minutes. The mean ICP dropped precipitously to a nadir of -15 mmHg. TCD showed signifi cant vasospasm in the MCA, compared with the internal carotid artery (ICA), during the entire cooling phase (Table 1 ). Upon termination of cooling, the brain temperature spontaneously rewarmed to core temperature in 13 ± 4 minutes.
Rewarming was associated with hyperemia and elevation of ICP. In group 2, there was no cerebral vasospasm or hyperemia during cooling and rewarming respectively. Conclusion SNBC is associated with signifi cant vasospasm of the MCA. In addition, spontaneous and rapid rewarming of the brain, vasodilation, rapid reperfusion, and rebound elevation of ICP, all occurring minutes after termination of SNBC, are likely to be detrimental to an already ischemic brain.
Introduction Uncontrolled shivering may have negative consequences by increasing metabolic demand and subsequently neutralize the benefi ts of therapeutic normothermia [1] . Previous anti-shivering protocols that utilize the least sedation have been described in therapeutic temperature modulation (TTM) [2] . Our aim is to describe and evaluate an anti-shivering protocol that emphasizes the least sedating regimen with the least number of pharmacologic agents for patients undergoing therapeutic normothermia. Twenty-seven patients of the nonsurvivors had ROSC >20 minutes and one patient had CPC 3 at hospital discharge. We observed no signifi cant diff erence between both groups in age (P = 0.161), time between emergency call and start of ALS (P = 0.788) and duration of basic life support performed by bystanders, general practitioners or paramedics (P = 0.649). The initial rhythm was asystole in one (12.5%) survivor and in 50 (62.5%) nonsurvivors (P = 0.009), ventricular fi brillation in six (75%) survivors and 13 (16%) nonsurvivors (P = 0.001), and pulseless electrical activity in one (12.5%) survivor and 17 (21%) nonsurvivors (P = 1.00). The cardiac arrest was witnessed in all survivors (100%) and in 49 (61%) nonsurvivors (P = 0.046). First measured rSO 2 was 38% (27 to 67) in the survivor group compared with 22% (8 to 32) in the nonsurvivor group (P = 0.004). Also a signifi cant diff erence was found in mean rSO 2 until 1 minute before ROSC between survivors and nonsurvivors, respectively 46% (40 to 74) and 34% (22 to 42). We observed no signifi cant diff erence in increase of rSO 2 until 1 minute before ROSC between survivors 12.5% (5 to 21) and nonsurvivors 11% (5 to 18) (P = 0.719). Conclusion We observed a signifi cant diff erence in fi rst measured rSO 2 and mean rSO 2 until 1 minute before ROSC between patients with good neurological outcome (CPC 1 or 2) at hospital discharge and patients with worse neurological outcome or nonsurvivors (CPC 3 or 4 or 5). However, no signifi cant diff erence was observed in the increase between both groups. Larger studies are necessary to confi rm these results. of this study were to investigate the association between hemoglobin, cerebral oxygenation (SctO 2 ) and outcome in post-CA patients.
Methods A prospective observational study in 82 post-CA patients during hypothermia in the fi rst 24 hours of ICU stay. Hemoglobin was determined hourly together with a corresponding SctO 2 by NIRS and SVO 2 in patients with a pulmonary artery catheter (n = 62).
Results Based on 2,099 paired data, we found a strong linear relationship between hemoglobin and average SctO 2 (SctO 2 = 0.70 × hemoglobin + 56 (R 2 = 20.84, P = 10 -6 )). Given the previously suggested SctO 2 target between 66 and 68%, hemoglobin levels below 10 g/dl generally resulted in suboptimal brain oxygenation. Forty-three patients (52%) had a good neurological outcome (CPC 1 to 3) at 180 days post CA. There was a signifi cant association between average hemoglobin above 12. Introduction The purpose of this study was to determine the rate and magnitude of response to hypoxia for three diff erent regional oxygen saturation (rSO 2 ) devices. rSO 2 technologies are focused on absolute accuracy without consideration of response characteristics. Current rSO 2 technologies assume that the oxygen saturation is a fi xed ratio of arterial and venous blood. Cerebral arteries have an oxygenationrelated vasoactivity that may change the arterial/venous ratio during hypoxia. Thus, absolute rSO 2 accuracy may be less important compared with sensitivity to changes in cerebral rSO 2 . Methods Ten subjects completed the study and are included in the analysis. One INVOS sensor (SAFB-SM) was placed on the left side and one Equanox (8000CA) or Foresight (1 July 2007 or 1 July 2005) sensor (alternated between subjects) was placed on the right side of the forehead for bilateral monitoring. Desaturation was induced by adjusting the inspiratory gas mixture of O 2 /N 2 . Desaturation was titrated from room air to achieve a plateau of 70% arterial oxygen saturation (SpO 2 ). Resaturation was induced by rapid change in FiO 2 to 1.0. After 5 minutes of SpO 2 100%, the process was repeated by desaturation to SpO 2 70% and rapid return to SpO 2 100%. Cerebral and pulse oximetry data were recorded during the study and the time of each FiO 2 change and plateau was recorded. rSO 2 levels at 10, 20, 40, 60 and 80% of the total SpO 2 response were calculated for each device to assess the rate of rSO 2 change. The rate of rSO 2 change in seconds and total rSO 2 change were compared. An NSE serum concentration threshold of 90 μg/l predicted poor outcome with a positive predictive value of 0.98 and a sensitivity of 0.51. Three patients survived with good outcome despite an NSE serum concentration >90 μg/l. In two of these patients NSE elevations had been documented prior to cardiac arrest. One patient had a neuroendocrine tumor of the pancreas, the other patient suff ered from encephalitis of unknown etiology and an osteomyelofi brosis. Potential confounders in the third patient were an ovarian carcinoma, the use of an intra-aortic balloon pump and blood transfusions shortly after cardiac arrest. Only 16 of 205 patients with NSE <17 μg/l had poor outcome, the majority of these patients died from causes other than hypoxic encephalopathy. Conclusion In patients with cardiac arrest and targeted temperature management at 33°C, an NSE serum concentration of >90 μg/l strongly indicates poor outcome. NSE producing tumors, acute brain diseases, severe hematologic diseases, use of an intra-aortic balloon pump and blood transfusions need to be considered as potential confounders. An NSE serum concentration of <17 μg/l largely excludes hypoxic encephalopathy incompatible with re-awakening.
frequently suff ered cardiogenic shock, had more organ dysfunctions and died more frequently, respectively, with hospital mortality of 79.5% versus 29.1%, P <0.001; see also Figure 1 .
Conclusion In patients hospitalized for acute heart failure, both prehospital and postadmission resuscitated cardiac arrest is a severe complication associated with signifi cantly morbidity and mortality. Conclusion Ten percent of patients resuscitated after CA and admitted to the ICU become OD, consisting of up to 31% of the total number of OD in our center. Patients resuscitated after CA who suff er severe irreversible brain damage or are brain dead can thus substantially expand the donor pool. This justifi es extensive resuscitation eff orts, if not to save lives, then to save organs. This might be reassuring for families, staff and the community. Methods This is a retrospective study of 43 polytraumatized patients with head injury who were intubated and treated by the prehospital unit and transported to the trauma center. Patients were grouped according to their BAL into BAL+ (>0.5 mg/l) and BAL-(≤0.5 mg/l). Inclusion criteria were age ≥18, Injury Severity Score ≥16 and head Abbreviated Injury Scale (AIS) ≥3. Physiological parameters and outcome with respect to survival to hospital discharge (STHD) and functional outcomes were analyzed. Severity of injuries was measured using the Trauma-Injury Severity Score (TRISS) and head injury using AIS. Functional outcome was measured using the Glasgow Outcome Scale (GOS), Cerebral Performance Category (CPC) and Glasgow Coma 3.9; 95% confi dence level; P = 0.027), and GCS at discharge was on the border of statistical signifi cance (15 vs. 14; 95% confi dence level; P = 0.05). Other variables (TRISS, AIS, STHD, CPC) did not show statistically signifi cant diff erences among groups. Conclusion Presence of alcohol in the blood had a positive eff ect on neurological outcome but there was no signifi cant diff erence in survival. However, the positive results in neurologic outcome in the BAL+ group may also be due to the fact that this group was younger. The small number of patients in the study is another limiting factor of the interpretation. Therefore, the neuroprotective role of alcohol needs further clarifi cation.
Introduction Cerebral ischemia (CeI) is a major complicating event after acute brain injury (ABI) in which endothelial dysfunction is a key player.
Methods We studied cellular markers of endothelial dysfunction and peripheral reactive hyperemia index (RHI) in 26 patients with ABI at admission and after 6 and 12 days, and compared these with healthy volunteers (n = 15). CeI was determined clinically or using computer tomography.
In patients with ABI, RHI at admission was signifi cantly reduced compared with healthy subjects (P = 0.003), coinciding with a decrease in circulating endothelial progenitor cells (EPC) (P = 0.002) ( Table 1 ). The RHI recovered in eight patients without development of CeI ( Figure 1 , black), but failed to fully recover by day 12 in three out of four patients that developed CeI (Figure 1 , red). Despite recovery of the RHI within 12 days in these patients (P = 0.003), the EPC count remained signifi cantly lower after 12 days in patients with ABI (P = 0.022) ( Introduction Since most patients with acute brain injury are treated head-up at 30 to 45°, there can be a height diff erence of up to 15 cm between the heart and the ear canal. This causes a diff erence between mathematical CPP and true CPP of up to 11 mmHg depending on the zero reference level used and the body length of the patient (Figure 1 ).
We conducted a survey to analyze the current practice on CPP targets and zero reference levels in diff erent ICUs. Methods Neuro-ICU physicians were invited to participate in a survey on ICP and CPP targets and the level of measurement.
The results of 30 centers are summarized in Table 1 . Most centers (83.3%) use head-up elevation of 30 to 45° and consider an ICP of 20 mmHg as the threshold to start treating ICP (80%). More variation is noted in minimal and maximal CPP threshold. All centers measure ICP at the ear canal. Twenty-seven centers (90%) measure MAP at the heart, three centers measure MAP at the ear canal. These three centers use >50, 60 and 65 mmHg as minimal CPP and raise the bed to 30 to 45°. The two centers using minimal CPP >60 mmHg do not apply an upper limit. Conclusion Considering the infl uence of position on CPP, the variations among centers and the narrow range of CPP thresholds, future studies and guidelines should describe where MAP is measured. Alternatively, we propose a new formula for CPP: true CPP = MAP(heart) -ICP(ear)height diff erence (heart to ear canal (cm)) × 0.73. Introduction The purpose of this study is to assess the rate of confi rmed infections, antibiotic exposure, and monitoring practices with normothermia protocol utilization for traumatic brain injury patients. Treatment and prevention of fever is a focus of therapy for patients with severe neurological injury as fever has been identifi ed as an independent risk factor for morbidity and mortality [1] . Introduction Osmotherapy with mannitol (Man) or hypertonic saline (HTS) is currently used to treat elevated intracranial pressure after severe acute brain injury (sABI); however, their eff ect on cerebral oxygenation and metabolism has not been extensively evaluated. Methods A retrospective analysis of a cohort of patients with sABI after traumatic brain injury (TBI) and subarachnoid hemorrhage (SAH) monitored with cerebral microdialysis (CMD), brain oxygen (PbtO 2 ) and ICP, who were treated with Man (20%, 0.5 g/kg) or HTS (7.5%, 100 ml) for ICP >25 mmHg. Osmotherapy was administered over 20 minutes and each patient's individual response to intervention was analyzed up to 120 minutes following infusion. Only episodes where no other hypertonic solute was administered within 2 hours before or after treatment were selected. Variables analyzed included CMD lactate, pyruvate, glucose, glutamate, lactate/pyruvate ratio, and main brain physiologic variables ICP, PbtO 2 , CPP. Analysis was conducted using mixed-eff ects multilevel regression. 
In patients with acute brain injury (ABI), increased cerebral energy demand is frequent, potentially leading to cerebral glucose depletion (GD) and poor outcome. In this setting, lactate may act as supplemental fuel. We examined dynamics of cerebral lactate supply during prolonged GD in ABI.
Methods We retrospectively analyzed severe ABI (18 TBI, eight SAH) monitored with brain oxygen and cerebral microdialysis (CMD) to measure hourly levels of cerebral extracellular glucose, lactate, pyruvate and lactate/pyruvate ratio (LPR). Variations of CMD variables were analyzed as a function of GD (defi ned as spontaneous decreases of CMD glucose from normal to low (<1.0 mM), at least 2 hours) and increased cerebral energy demand (LPR >25).
Results During GD (60 episodes; 26 patients), we found an increase of CMD lactate (4 ± 2.3 vs. 5.4 ± 2.9 mM) and LPR (27 ± 6 vs. 35 ± 9; all P <0.005) while brain oxygen and blood lactate remained normal. Dynamics of lactate and glucose supply were studied by analyzing the relationship between blood and CMD samples. No correlation between blood and brain lactate was found when brain glucose and LPR were normal (r = -0.12, P = 0.48; Figure 1 ), while this correlation became linear during GD, progressively rising to r = 0.53 (P <0.0001) when energy demand increased, suggesting increased cerebral lactate availability. The correlation between blood and brain glucose changed in the opposite direction, decreasing from r = 0.78 to 0.37 (P <0.0001) during GD and at LPR >25.
Conclusion Energy dysfunction is associated with increased supply of nonhypoxic cerebral lactate. Our data suggest lactate may act as alternative substrate after ABI when availability of cerebral glucose is reduced.
Introduction This study was designed to see whether there is a correlation between the transcranial Doppler (TCD) parameters and the CCP and intracranial pressure (ICP) monitoring during the cerebral hemodynamic changes and to evaluate ICP indirectly by TCD.
Methods A prospective and descriptive study conducted in our PICU from June 2011 to December 2013. We investigated 51 children with severe trauma brain injury (TBI). The TCD measurements were routinely performed bilaterally on the middle cerebral artery parallel to the ICP registration. The ICP and CPP data were correlated to PI and the correlation coeffi cient calculated. To control the linear correlation, the residuals were tested for normal distribution around the regression line. Introduction Improvement of recovery is a challenging process in cases with varying degrees of severe brain injury (BI) requiring intensive care. Amantadine sulfate (AS) is recommended for use in cases with brain injury. The Coma Remission Scale (JFK-CRS) consists of auditory-visualmotor-mouth-tongue functions, communication and awareness scales; provides a score between 0 and 23; and shows numeric recovery from coma. The aim of this study was to evaluate outcomes and eff ects of AS used for neurorecovery on the Glasgow Coma Scale (GCS) and JFK-CRS in our ICUs during the last 5 months. Methods After approval of the Ethics Committee, we recruited 12 patients with brain injury resulted from trauma or hemorrhage who had initial GCS of ≤9 and received AS (500 mg, twice daily over 10 days) during the recovery period. In all subjects, age, gender, diagnosis, initial APACHE II score, time of initiation of AS therapy, JFK-CRS and GCS scores, aspartate aminotransferase, alanine aminotransferase, BUN, creatinine, platelet count, electrocardiography fi ndings, electrolyte values and arterial blood gas values on days 1, 6, 10 and 14 were recorded. Results The patients' diagnoses included two post-CPR, fi ve intracranial and one subdural hematoma, one CVA, one postoperative aneurysm, one subarachnoid hemorrhage and one brain contusion. Table 1 (overleaf) presents the fi ndings. The AS therapy was initiated between days 3 and 33 of admission in all patients other than Patients 2 and 8. A dramatic improvement was observed in a patient with both GCS and JFK-CRS score of 5 when AS therapy was initiated in month 5 and the patient was discharged for home care. In Patient 9, AS therapy was withdrawn on day 5 due to persistent thrombocytopenia (TP) despite exclusion of other reasons; subsequently, improvement was observed in TP. The complications were relatively less severe with average acceptability. Conclusion We suggest that an AS dose of 1,000 mg/day (over 10 days) seems to improve neurorecovery in BI patients with good tolerability. Prospective controlled studies with large, homogeneous BI populations will better defi ne the role of AS for recovery and complications.
biomarkers, of long-term outcomes. Among these, ubiquitin carboxyterminal hydrolase L1 (UCH-L1) is currently being investigated to defi ne its potential prognostic value. The objective of this systematic review is to determine the ability of UCH-L1 to predict prognosis following a moderate or severe TBI. Methods The MEDLINE, Embase, The Cochrane Library and BIOSIS electronic databases, conference abstracts and existing narrative reviews were searched from their inception to July 2013. Cohort studies including patients with moderate or severe TBI having evaluated the prognostic value of UCHL-1 according to mortality or the Glasgow Outcome Scale (GOS) were considered. Data concerning patients, outcomes, study methods, and laboratory methods were abstracted. Pooled results were planned to be presented using mean diff erences and analyzed using random eff ect models, as well as sensitivity analyses to explain potential heterogeneity. Results Our search strategy yielded 2,257 articles, of which fi ve studies corresponded to our inclusion criteria (n = 730). All studies were performed by the same group of researchers. Five studies reported mortality (n = 515), two studies reported GOS (n = 58). Results from all included studies observed that UCHL-1 was a signifi cant predictor of mortality. However, only two studies represented a unique study population, thus precluding a meta-analysis. Conclusion In this systematic review, we observed that all published studies on UCHL-1 were conducted by the same group of investigators and presented results from an intersecting cohort of patients. Due to the paucity of data, we could not perform a pooled analysis and conclude on the association of this biomarker with long-term prognosis. Assays using UCHL-1 were only recently developed and further studies done by diff erent research teams will be needed to determine the reproducibility and validity of UCH-L1 as a potential prognostic tool. Introduction ICU patients may remain comatose after resolution of critical illness. Frequently this is due to delayed sedative clearance but may also result from increases in intracranial pressure (ICP) and cerebral edema. We proposed that measurement of the optic nerve sheath diameter (ONSD) is a rapid, bedside screening test that can be used to quickly identify patients with cerebral edema and increased ICP. Methods This was a prospective, observational study carried on consecutive patients admitted to a multidisciplinary medical and surgical ICU. Stable patients with unexplained coma and scheduled for brain imaging were included. Patients with obvious ocular trauma or on sedative, narcotic infusions were excluded. ONSD was measured using a 7.5 to 10 MHz linear array ultrasound transducer probe placed on the closed eye in the transverse axis. The ONSD was measured at a predefi ned point 3 mm posterior to the globe. Both eyes were measured and the mean value used. Introduction Increased intracranial pressure (ICP) adversely aff ects anesthesia due to a disturbed cerebral blood fl ow. In older patients this disturbance may increase the incidence of postoperative delirium (POD) and may lead to a poor outcome [1] . The standard hemodynamic protocol involves maintaining the mean arterial blood pressure (MAP), but in patients with intracranial hypertension it may not be enough to maintain adequate cerebral perfusion. The purpose of this study was to evaluate the protocol of maintaining cerebral perfusion pressure (CPP) in the prevention of postoperative delirium in older patients in abdominal surgery. Methods A total of 132 ASA 3 patients, undergoing major abdominal surgery (duration 5.2 (4.3 to 6.5) hours) with ICP >12 mmHg evaluated by a venous ophthalmodynamometry [2] , were included in our research. Patients were randomized into two groups: MAP group, in which MAP was maintained above 70 mmHg or within 20% from baseline (n = 78); or CPP group, in which CPP was maintained above 60 mmHg or within 20% from baseline (n = 54). ICP, MAP and CPP were assessed every hour of anesthesia. Time of recovery of consciousness, incidence of POD and length of stay in the ICU and in the hospital were also evaluated. Results Initial ICP was 14 ± 3 mmHg in the MAP group and 15 ± 2 mmHg in the CPP group. During the anesthesia it was stable without any signifi cant change. Decreasing of MAP after induction of anesthesia was similar in two groups and it was stable during the anesthesia. The frequency of use of vasopressors and infusion rate was higher in the CPP group. Time of recovery of consciousness in the MAP group was higher (28 ± 7 minutes vs. 18 ± 5 minutes (P <0.05)). The incidence of postoperative delirium was higher in the MAP group (18% vs. 11% in the CPP group (P <0.05)). There were no signifi cant diff erences between two groups in other complications. Total length of stay in the ICU and in the hospital was higher in the MAP group (6 ± 2 days vs. 4 ± 2 (P <0.05) and 15 ± 3 days vs. 12 ± 2 in the N group (P <0.05)). Introduction In critically ill patients, blood lactate on admission is associated with outcome, but in patients with aneurysmal subarachnoid hemorrhage (SAH) this has not been investigated. We studied the association of early circulating lactate and glucose with unfavorable disease course. The prognostic role of both lactate and glucose was studied, hypothesizing that both may be increased due to sympathetic activation after SAH [1] . The results of studies attempting to assess the risks of ischemic stroke in patients with burn injury have been confl icting. We investigated the risks of ischemic stroke in hospitalized burn injury patients in Taiwan to evaluate whether the risk is higher compared with the general population. Methods The data from 1 million National Health Insurance bene fi ciaries were utilized. All adult benefi ciaries were followed from 1 January 2005 until 31 December 2012 to identify those who developed ischemic stroke. Meanwhile, each identifi ed patient with burn injury was matched with 100 unexposed patients based on the high-dimensional propensity score. Cox regression models were applied to compare the hazards of ischemic stroke in the matched cohorts. Introduction Muscle wasting is a common consequence of longterm stay in the critical care environment, which may slow down the rehabilitation of survivors. Previous ultrasound studies have demonstrated a loss of cross-sectional area of lower limb muscles during a 10-day intensive care stay. In this study, we have looked at how markers of muscle architecture (muscle thickness, pennation angle and fascicle length) change in the lower limb, as well as looking at changes in muscle thickness in the upper limb. Methods Following ethical approval, patients who were intubated and ventilated in one of two critical care departments were assented to take part in the study by their next of kin. B-mode ultrasound scans of the right biceps, vastus lateralis and the medial head of gastrocnemius were performed on days 1, 5 and 10. Scans were not performed in patients once they were free of sedation. Muscle thickness (MT) was measured in all three muscles, with pennation angle (PA) being measured in the lower limb muscles. Fascicle length (FL) was derived from PA and MT. Results Twenty patients were recruited, of which 15 were scanned on day 5, and eight were scanned on day 10. In the biceps, there were no alterations in MT over 5 or 10 days. MT of the vastus lateralis signifi cantly decreased on day 5 (1.77 ± 0.06 mm muscle loss, P = 0.03) and day 10 (5.58 ± 0.09 mm muscle loss, P = 0.01). There was also a signifi cant loss in PA over 5 days (1.48 ± 0.63°, P = 0.01) and 10 days ( [1]. The presence of delirium in critical care is an independent risk factor for mortality; for every day of delirium, there is an additional 10% relative risk of death at 1 year [2] . A delirium prediction tool PRE-DELIRIC has been recently developed and calibrated in a multinational project [3] . This study aimed to determine the utility of PRE-DELIRIC on our ICU. Methods This study prospectively investigated 41 patients. Medical and surgical general ICU patients were included after 24 hours of sedation and mechanical ventilation. The researchers calculated PRE-DELIRIC scores for each patient. PRE-DELIRIC involves recording 10 variables, submitted into an online algorithm that estimates the percentage risk of delirium. We diagnosed delirium with the CAM-ICU which was performed 12 hourly [4] .
The PRE-DELIRIC scores predicted a mean rate of delirium of 39%. PRE-DELIRIC risk scores ranged from 4 to 93% (Figure 1 ). Six (15%) patients developed delirium in the fi rst 24 hours following extubation. Fifteen (37%) of patients were predicted 20% or less probability of delirium. Twelve (29%) patients developed delirium at any point during their ICU stay. This resulted in 36 total delirium bed-days. Conclusion Our observation that <30% of patients experienced delirium is less than the reported prevalence in similar settings and our own audits. This study demonstrates that there is some agreement between recorded rates of delirium and predicted rates using PRE-DELIRIC. We suggest that PRE-DELIRIC can be used in quality/audit work on UK ICUs in order to assess attempts to improve the management of delirium. Further work is required to assess the utility of PRE-DELIRIC as a risk assessment tool in individual patients.
Introduction Propofol infusion syndrome (PRIS) is a rare propofol complication, leading to cardiac failure. It was fi rst described in critically ill children and in adults with traumatic brain injury. Pathophysiology is unknown although common factors are the prolonged (>48 hours) use of high-dose (>5 mg/kg/hour) propofol combined with elevated levels of catecholamines and corticosteroids. Recently, case reports of earlyonset PRIS during anesthesia and in the early postoperative setting were published. In many of these, lactic acidosis is interpreted as onset of PRIS. Criticism off ers that it might concern a poor diff erential diagnostic approach or an observational bias. Also, lactic acidosis is not an obligate PRIS symptom and incidence of lactic acidosis during propofol sedation is unknown. To gain insight into the incidence and characteristics of early PRIS, we performed a systematic review on early PRIS cases. Introduction Among drugs used for sedation, propofol has a primary role [1] . Despite propofol being described to exert a relaxant eff ect on skeletal muscle, no data showing its action on diaphragm are reported. The aim of this observational study on humans is to apply ultrasound to assess propofol's eff ect on diaphragmatic contraction and motion during endoscopic procedures. Methods We investigated seven consecutive patients undergoing gastroscopy or colonoscopy in the endoscopy unit of our hospital.
Patients received propofol at a dose able to induce and maintain sedation to level 6 of the Ramsay Sedation Scale during the procedure. Measurements were obtained on right side of the thorax in millimeters; diaphragmatic motion (DM) and diaphragmatic motion at maximal inspiration (DM forced) were measured in M-Mode with a 3.5 MHz array convex probe placed on the midclavicular line using the liver acoustic window. Thickness at end inspiration (TEI) and thickness at end expiration (TEE) were measured in M-Mode with a 10 MHz vascular probe. The thickening fraction (TF) was calculated: (TEI -TEE) / TEE [2] . Time points of measurements were taken when the patient arrived in the surgery room (Baseline), 1 minute after level 6 of the Ramsey Sedation Scale was obtained (Sedation) and 5 minutes after the patient had a recovery to level 1 on the Ramsey Sedation Scale (Awakening). Data analyzed are reported in Table 1 and expressed as mean (SD). *ANOVA was used to compare data for repeated measurements. Post hoc statistical comparison with Bonferroni's test was used to identify signifi cant variations. Results During propofol administration TEI reduced 19% whereas after awakening it increased 14.5% but did not reach baseline. Conversely TEE did not change during the study. During propofol sedation, TF decreased 34% and returned to baseline after recovery. DM showed 29% reduction during propofol administration whereas the forced diaphragmatic motion tested when patients were conscious (forced DM) did not evidence any change. Conclusion In this observational study, ultrasound assessed that propofol causes a reduction of diaphragmatic contraction and motion during endoscopic procedures. References Introduction Delirium recognition in critically ill patients is considered to be important taking into account the poor outcomes associated with its occurrence. The purpose of this study was to evaluate knowledge pertaining to delirium as well as the implementation of screening practices. This study constituted a component of a survey that explored current sedation-related practices in South African ICUs. Methods Following approval from the University Human Research ethics committee, a validated questionnaire was distributed electronically to physician members of various medical databases in South Africa as South Africa does not have a formal registry of critical care practitioners. Results One hundred and twenty-six of 174 respondents indicated that they practice in the ICU setting. Sixty-six per cent were specialists and mainly anaesthesiologists (42%), whilst 32% were critical care subspecialists. The respondents indicated that on average 30 ± 20% of their patients experience delirium. Eighty per cent of the respondents indicated that delirium impacts signifi cantly negatively on patient outcomes whilst 1% indicated that there was no such association. Delirium screening is achieved mainly by clinical assessment (77%). Twenty-four per cent utilise an objective tool to screen for delirium and amongst them the CAM-ICU is utilised by 80%. Amongst delirious patients the sedative of choice is dexmedetomidine in the majority. However, 20% prescribe midazolam as a fi rst choice in this setting. Conclusion The fi ndings are comparable with reports of similar surveys conducted in other regions. The delirium screening method is inadequate as the vast majority do not utilise an objective method.
Loxapine to control agitation during weaning from mechanical ventilation: a randomized controlled trial S Gaudry 1 , B Sztrymf 2 , R Sonneville 3 , B Megarbanne 4 , C Clec'h 5 Introduction Weaning from prolonged mechanical ventilation (MV) in the ICU may be impeded by the occurrence of agitation. Loxapine had the ability to control agitation without aff ecting the effi cacy of spontaneous ventilation in an observational study, justifying the implementation of a randomized controlled trial. Methods We conducted a multicenter, placebo-controlled, parallel group, randomized trial at fi ve French ICUs between November 2011 and November 2013. Patients (aged >18 years) under MV for more than 48 hours who were potential candidates for weaning from the ventilator and who exhibited agitation defi ned by a Richmond Agitation Sedation Scale (RASS) >2 after sedation withdrawal were randomly assigned to receive either loxapine or placebo. All participants were masked to group of allocation. After randomization, patients received 150 mg loxapine or placebo by nasogastric tube. RASS was monitored every 4 hours. A second dose of loxapine or placebo was administered if agitation persisted or worsened. In case of severe agitation, usual sedation (benzodiazepines and morphinic agents) was immediately resumed. Extubation was contemplated when patients were conscious and calm. The primary endpoint was the time between the fi rst administration of loxapine or placebo and successful extubation (no reintubation in the following 48 hours). Three hundred patients were necessary to have 90% power to detect a 2-day reduction of weaning time in the loxapine group with a one-sided type I error rate of 5%. Results The trial was discontinued after 101 patients had been randomized because of insuffi cient enrollment. Fifteen patients withdrew consent, leaving 86 patients for analysis. Forty-seven patients were assigned to the loxapine group and 39 to the placebo group. Median time to successful extubation was 3.2 days in the loxapine group and 5 days in the placebo group (RR = 1.2, 95% CI = 0.75 to 1.88; P = 0.45). During the fi rst 24 hours, sedation was more frequently resumed in the placebo group (44% vs. 17%, P = 0.01). One patient had a transient seizure in the loxapine group. Conclusion These results are consistent with the hypothesis of 2 days reduction of the median weaning time in the loxapine group, but the diff erence was not statistically signifi cant. Loxapine reduces the need for resuming sedation during weaning from MV. Given the quality of the data and methodology, these results may be useful in future meta-analyses. 
We investigated the incidence, symptoms and risk factors for withdrawal associated with prolonged dexmedetomidine use. Dexmedetomidine is an α 2 -adrenergic receptor agonist, with anxiolytic, analgesic and sedative properties. Intended for short-term use, there is increasing literature describing prolonged use for sedation. However, this raises the potential of withdrawal syndrome and there is no recommendation for the discontinuation of dexmedetomidine. Other goals included determining the hemodynamic eff ects of discontinuation of dexmedetomidine and role of clonidine in patients with prolonged dexmedetomidine use. Methods A retrospective review of patients admitted to the critical care unit who had exposure to dexmedetomidine for longer than 48 hours, between 1 January 2014 and 15 July 2014. Data included patient demographics, dexmedetomidine exposure (bolus dose, total cumulative dose, duration), other sedative exposure, withdrawal symptoms measured by WAT-1 score, nursing subjective assessment and treatment given for withdrawal. Each potential withdrawal episode was reviewed by two reviewers. Hemodynamic parameters were analyzed to assess hemodynamic changes associated with discontinuation of dexmedetomidine. Descriptive statistics were used with t test and chi-square test. Median and interquartile range (IQR) are reported. Results A total of 53 patients accounted for 69 unique dexmedetomidine treatment courses. Median age at the time of dexmedetomidine infusion was 5 months (range 1 day to 3 years). Dexmedetomidine dose ranged from 0.1 to 2 μg/kg/hour with a median cumulative dose of 87 μg/kg (IQR 53, 156). Median duration of exposure to dexmedetomidine was 124 hours (IQR 76, 178) with a maximum duration of 466 hours. We identifi ed 24 separate episodes of withdrawal (incidence 35%). Most common symptoms were agitation (100%), fever (67%), vomiting/retching (46%), loose stools (29%) and decreased sleep (20%). Statistical analysis showed that factors signifi cantly associated Critical Care 2015, Volume 19 Suppl 1 http://ccforum.com/supplements/19/S1 S169 with withdrawal were cumulative dose (P = 0.01) and duration of use of dexmedetomidine (P = 0.02). Duration of opioids exposure prior to dexmedetomidine wean was also a risk factor for withdrawal (P = 0.01). Use of clonidine as a transition from dexmedetomidine did not protect against withdrawal (P = 0.59). Conclusion This study showed that withdrawal syndrome is associated with prolonged infusion of dexmedetomidine. Patients with higher cumulative doses and longer duration of exposure were more at risk. Our results suggested that clonidine use is not protective for withdrawal from dexmedetomidine. Introduction Sedation in the ICU is a basic therapeutic procedure to increase tolerance of invasive treatments and reduce discomfort. Extracorporeal membrane oxygenation (ECMO) is a highly invasive treatment and prolonged sedation may be required. Patients undergoing ECMO represent a challenge with respect to sedation. Initially, deep sedation may be required to optimize ventilation and circuit-patient fl ows and to minimize oxygen consumption. The other critical phase is represented by weaning from ECMO support. Optimal sedation is not clearly defi ned, moreover there are no data on sedation practices with dexmedetomidine (DEX) in adult patients undergoing ECMO. In contrast to other sedatives, DEX has analgosedative eff ects without respiratory depression, and could be useful to facilitate spontaneous respiratory activity during recovery from sedation. Methods We investigate the role of DEX as a sedative agent used during recovery from deep sedation and weaning from extracorporeal support in patients on vv-ECMO. From May 2014 to October 2014 we prospectively enrolled seven patients aff ected by ARDS of diff erent etiologies treated with vv-ECMO. The mean age was 53.7 ± 7.9 years and the mean ICU stay was 21.4 ± 11.5 days. Initially, all patients were sedated with association of opioids and GABA receptor agonists, following the internal protocol. At the time of weaning from ECMO, ruled out cardiovascular instability, we started the administration of DEX (0.7 μg/kg/hour, without initial bolus) with progressive decrease of the dose of other sedative drugs. Results The mean duration of DEX infusion was 6.1 ± 4.8 days. Except for one patient, who received DEX as a single drug after suspension of other sedatives, a low-dose infusion of another sedative (<50% compared with initial dose) was maintained. Three patients presented adverse events: two bradycardia and one hypotension. In four patients DEX was discontinued after recovery of respiratory function; in two patients deeper sedation for ventilatory dyssynchrony was needed so other sedative drugs were started. Only in one patient was the drug suspended for extreme bradycardia, resolved after suspension. Conclusion In our study, DEX allowed the reduction of doses of other sedative drugs during weaning from vv-ECMO; this may lead to a cooperative sedation, promoting spontaneous breathing. Side eff ects described and the cost-benefi t ratio must still be verifi ed extensively in patients during weaning from ECMO. Introduction Sedation in the ICU is crucial in reduction of patients' discomfort, in particular in patients undergoing mechanical ventilation to help tolerate intubation and reduce pain and anxiety. Propofol (Pr) is a widely used option, but other viable alternatives for short-term sedation (STS; that is, <24 hours) include benzodiazepines (BDZ) and dexmedetomidine (Dx). We aimed at pooling all available evidence on the comparative eff ects of Pr in terms of awakening and recovery times after STS in mechanically ventilated ICU patients. Methods We planned a systematic literature review searching Medline and Scopus and performed a meta-analysis on direct comparisons reporting on weaning time (Tw), duration of mechanical ventilation (Tmv), time to extubation (Tex) and length of stay in the ICU (Ticu). The primary analysis considered only data from RCTs, while in a secondary analysis observational studies were also included. Results The literature search identifi ed 15 relevant RCTs, of which 11 versus BDZ, and a further fi ve observational studies, of which one versus BDZ. When compared with BDZ, Pr associated with signifi cantly reduced Tw (-1.6 hours, 95% CI: -2.5 to -0.8), Tmv (-2.0 hours, 95% CI: -3.7 to -0.2), and Ticu (-5.0 hours, 95% CI: -8.5 to -1.4); no statistically signifi cant diff erence resulted when comparing Pr and Dx. When nonrandomized evidence was included, results did not change signifi cantly. Conclusion In conclusion, Pr is associated with shorter awakening and recovery times after STS than BDZ, while no diff erence could be shown when Pr was compared with Dx. 
Recently, several animal studies observed that dexmedetomidine (DEX), a new sedative and α 2 -adrenoceptor agonist, inhibited the infl ammatory responses [1] [2] [3] . Moreover, DEX was reported to have anti-infl ammatory eff ects in patients [4, 5] . However, these studies were the small-sized studies and there are few studies about the eff ects of long-term administration of DEX in severe septic patients. The present study evaluated the eff ects of long-term administration of DEX on infl ammatory responses and severity in severe septic patients. We hypothesize that the administration of DEX has benefi cial eff ects for severe septic patients. Methods In 66 patients (M/F 44/22, mean age 66 years) with severe sepsis, who were administered propofol (0.5 to 4.0 mg/kg/hour) only for sedation, 42 patients (M/F 28/14, mean age 67 years) were administered DEX (0.2 to 0.7 μg/kg/hour) for more than 24 hours in addition to propofol (DEX group). Twenty-four patients were not administered DEX (Control group). Primary outcome were changes in infl ammatory responses at 48 hours after the administration of DEX or none, and secondary outcomes were changes in APACHE II and SOFA scores at 48 hours after the administration of DEX or none. Results The administration of DEX occurred for a mean 130 hours (24 to 433 hours) in the DEX group. White blood cell counts, C-reactive protein (CRP) and procalcitonin (PCT) in both groups signifi cantly decreased after the administration of DEX or none. However, CRP and PCT in the DEX group were signifi cantly lower than those in the control group: CRP 7.7 (5.0) versus 13.6 (7.9) mg/dl; P <0.05, PCT 7.6 (11.7) versus 18.6 (11.6) ng/ml; P <0.05, mean (SD). APACHE II and SOFA scores in both groups decreased after the administration of DEX or none, but APACHE II and SOFA scores in the DEX group were lower than those in the control group: APACHE II 10.8 (4.8) versus 15.2 (5.1); P <0.05, SOFA 3.6 (2.0) versus 5.8 (2.9); P <0.05, mean (SD). Conclusion In the present study, the long-term administration of DEX has benefi cial eff ects of infl ammatory responses and severity for severe septic patients.
To describe the main indications, doses, infusion length and side eff ects of dexmedetomidine (DEX) administered to children and adolescents admitted to the pediatric ICU (PICU). Methods A retrospective observational study including children (<18 years) admitted to a Brazilian PICU who received DEX between November 2011 and June 2014. Demographic data, indications, initial dose, maximum dose and time of infusion of DEX, side eff ects and impact on heart rate (HR) and mean arterial pressure (MAP) 6 and 24 hours after the start of infusion. Results A total of 77 children with a median age of 15 (4 to 84) months, weight of 10 (5.7 to 20) kg and length of ICU stay of 8 (5 to 14) days received DEX, with a mortality rate of 9%. Indications were: weaning from mechanical ventilation (32.5%), neurosurgical postoperative (NCI) and upper airway surgery (VAS) (24.7%), non-invasive ventilation (13%), refractory tachycardia (6.5%) and other indications (23.3%). There was no diff erence between the initial and maximum doses and infusion length. There was a signifi cant decrease in MAP and HR after 6 hours infusion of DEX in the total group; however, no signifi cant diff erence occurred between groups when analyzing MAP and HR 24 hours after the start of infusion (P = 0.798 and 0.379, one-way ANOVA, respectively). In six patients (8%) DEX was suspended for possible side eff ects. Conclusion Increased DEX indications have been observed in the pediatric population. In this study DEX was demonstrated to be a safe and tolerable drug with few side eff ects, especially related to the cardiovascular system. References Introduction Pain assessment is associated with important out omes in ICU patients but remains challenging, particularly in non communicative patients. Use of a reliable tool is paramount to allow any implementation of sedation/analgesia protocols in a multidisciplinary team. This study compared psychometric properties (inter-rater agreement primarily; validity, responsiveness and feasibility secondarily) of three pain scales: Behavioural Pain Scale (BPS/BPS-NI, that is BPS for non-intubated patients), Critical Care Pain Observation Tool (CPOT) and Non-Verbal Pain Scale (NVPS), the pain tool routinely used in this 16-bed medical ICU. Methods In a prospective observational study of ED polytraumatized patients (n = 23, mean Acute Physiology and Chronic Health Evaluation II (APACHE II) score of 11 ± 6) we measured (in the fi rst 24 hours) plasma TAC by the ferric reducing activity/antioxidant power (FRAP). For control subjects, we used age-matched and gender-matched volunteers (n = 32). We also evaluated the contribution of antioxidant molecules (uric acid, bilirubin, and albumin) to these values. Results Polytraumatized patients show diff erences in TAC with reference to control subjects. ED polytraumatized patients show high FRAP values. We found that FRAP values were inversely correlated with APACHE II score (r = -0.266, P <0.01) suggesting that, in trauma patients, increased antioxidant response, as measured by the FRAP assay, could be a pathophysiological response to stress. Albumin and uric acid concentrations reproduced the FRAP trend with severity. Conclusion FRAP values in trauma ED patients are independently infl uenced by age (β = 0.271, P <0.021), APACHE II score (β = -0.356, P <0.002) and head trauma (β = -0.219, P <0.045). These results accentuate the infl uence of trauma location and severity in TAC changes. The TAC response in ED patients reinforces the need for adequate tailoring of treatments aimed at their recovery, such as antioxidant therapies. 
To determine which of three methods of pain management provided the best pain control in severe ASA III older patients in the surgical ICU (SICU). As technology improves, more older patients benefi t from surgery and need SICU care. Older surgery patients frequently present two medical problems. First is unspecifi c symptoms and decreased pain sensation resulting in delayed diagnosis. Second, they usually are not given enough perioperative pain relief. Optimal pain management results in perioperative stable hemodynamic status and decreased morbidity, mortality, length of stay and medical costs. Methods A retrospective cohort study chart review of 1,872 all-cause patients in a 16-bed SICU during April 2011 to September 2012. Unconsciousness, uncooperative, ASA <III and <65-year-old patients were excluded. The primary point was to compare eff ectiveness of three diff erent methods of pain management: P.R.N. i.v. Demerol/NSAID (D/N), i.v. patient-controlled analgesia (PCA) and patient-controlled epidural analgesia (PCEA), in three diff erent conditions: rest, movement and coughing, with visual analogue scales (VAS 0 to 100). Secondary point was patient satisfaction. Results A total of 1,292 patients were excluded. VAS results are presented in Table 1 as mean ± SD. At rest, the PCEA group is signifi cantly better than the other two groups. When at movement, there is no diff erence between the PCEA group and the D/N group but both are better than the PCA group. While coughing, the PCEA group is worse than the D/N group, although there is no diff erence between the PCEA group and the PCA group. The PCEA group gets the best grades in patient satisfaction. Conclusion PCEA provided better pain control at rest than the other two methods, whereas P.R.N. Demerol/NSAID and PCEA were somewhat better than PCA when patients were moving. While coughing, P.R.N. Demerol/NSAID provided the best pain control. However, patient satisfaction was signifi cantly better with PCEA.
it is supposed to trigger the production of HIF-1α and subsequently erythropoietin. Subsequently, researchers and clinicians started a scientifi c discussion about the potential clinical benefi t in support of humans exposed to high demand, such as critically ill patients. The objective of this study was therefore to evaluate the eff ect of xenon on serum levels of erythropoietin in healthy volunteers. Methods This is a monocenter, randomized, blinded, crossover trial, which was registered at ClinicalTrials.gov (NCT01285271). Healthy study test persons were spontaneously breathing randomly 1 hour of xenon 30% (Xe/O 2 30%/65%) or control gas (N 2 /O 2 30%/65%). The primary outcome parameter was the erythropoietin level 24 hours after exposure. Secondary outcome parameters are xenon's elimination kinetics measured in blood and exhalation samples.
The application of xenon increases erythropoietin levels with a maximum 24 hours after exposure (1.32 (0.99 to 1.66) P = 0.033) compared with the baseline values and compared with control values (0.87 (0.68 to 1.05) P = 0.012, Figure 1 ). Xenon was gas chromatographically traceable in blood and exhalation probes up to 24 hours after exposure. (Figure 1 ). Conclusion Etomidate use is associated with lower postoperative SVRI which is increased in the presence of G homozygosity for TNFβ polymorphism.
Introduction Operation therapy is more invasive than medication therapy and then operation-related medical errors (MEs) might be of more signifi cant impact than medication errors. We assessed the incidence and characteristics of operation-related MEs to improve patient safety in such patients. Methods The Japan Adverse Event (JET) study was a prospective cohort study which had evaluated AEs and MEs at two tertiary care hospitals. We included all adult patients aged ≥15 years old who had operations over a 2-month period. The primary outcome of this study was the operation-related MEs, defi ned as any deviation from appropriate process of an operation or perioperative care. Trained nurses placed at each participating hospital reviewed all charts daily on weekdays, along with laboratories, incident reports, and prescription queries to collect any potential event. They also collected the characteristics of the patients in the cohort. Some operation-related MEs are associated with operation-related AEs, which are operation-related preventable AEs. After those suspected events were collected, physician reviewers independently evaluated them and classifi ed them as operation- Conclusion Machine learning tools off er an appealing mathematical framework for modelling complex medical situations. This proof of concept demonstrates that the application of computational sciences to high-quality data such as the MIMIC-II database has the potential to lead to the development of meaningful tools which will ultimately be capable of assisting physicians in making the right decision at the right time for an individual patient. Only tight cooperation between clinicians and data scientists can help close the gap that currently separates these two worlds, for the ultimate benefi t of patients. Introduction Surrogate consent and time-sensitive recruitment in critical care research is challenging, yet low enrolment numbers or omitting ethnic groups skew results and conclusions. Patients from diff erent ethnic groups may respond diff erently to therapeutics [1] .
There are few data about the eff ect of ethnicity on recruitment into ICU trials. Our ICU recruits to national trials and serves an increasingly non-White British population (24% of population). We undertook this study to determine whether ethnicity aff ects ICU consent rates. Methods We performed a retrospective review of screening logs from three national UK trials (PROMISE, BALTI-P, GAiNS) and one local trial (Nociceptin in Sepsis). We analysed consent rates of eligible patients by ethnicity, age, sex, interventional or observational trial, and ethnicity of the researcher seeking consent. We performed chi-squared analysis, and entered signifi cant values into a logistic regression model using SPSS v22. Results We identifi ed 332 eligible patients across all trials, of whom 37 (11%) were not White British (nWB). Analysis demonstrated consent/assent refusal being signifi cantly associated with: nWB (14, 38%, P <0.001), interventional trial (21, 25%, P = 0.003) and diff erent researcher-patient ethnicities (P <0.001). Logistic regression analysis confi rmed these as independent factors (nWB OR = 4.5, 95% CI = 2.1 to 9.8, P <0.001; interventional trial OR = 2.7, 95% CI = 1.4 to 5.2, P = 0.003; data points missing for researcher-patient ethnicity so variable excluded). Conclusion This initial study suggests that ethnicity may aff ect assent/ consent to ICU research, with patients from diff erent ethnicities being four times less likely to be recruited. Whilst data are incomplete for researcher-patient ethnicity, our data suggest that this may be an important factor and may infl uence future consent processes. We believe that the role of ethnicity warrants further investigation, not only in clinical trials but also in areas such as organ donation. Introduction Decision tree analysis uses an algorithm to classify data items by recursively posing a series of questions about items within a dataset. Each question leads to another node and potentially more questions until a predefi ned end condition is reached or no more questions can be asked (Figure 1 ). We hypothesize that scores generated using the decision tree method will improve upon our existing Hamilton Early Warning Score (HEWS) for a composite endpoint of cardiac arrest, unplanned ICU admission or death. Methods A database of 156,642 electronically captured vital signs from 6,757 consecutively admitted patients to eight medical and surgical wards will be used to train and test the decision tree early warning score. One-third of the data will be withheld from the algorithm for use as a testing set. The algorithm will look for signifi cant changes in vitals 72 hours prior to an outcome and develop the score based upon the resulting relative risk of the composite endpoint happening given a certain vital sign. The scores and predictions generated by the decision tree analysis will then be compared with that of the inception HEWS cohort.
The planned analysis for determining the discriminatory and predictive ability of the decision tree HEWS will be conducted with area under the receiver operating characteristic curves. We will test whether the current HEWS has the appropriate sensitivity and specifi city when compared with that of the decision tree score. The AUROC will be calculated for both the training set of data as well as the separate population of additional medical and surgical patients. The two scores will also be plotted along an effi ciency curve, comparing the percentage of vitals that precede a critical event with the percentage of vitals that produce a EWS value greater than or equal to a given EWS value.
Conclusion Decision tree analysis methodology with real-life vital signs can produce an EWS superior to previous observational studies. Using a decision tree, especially one that composites all vitals, may show that certain vitals are more predictive of critical events than others. Data will be used to further improve our current HEWS score.
Can an electronic ICU support timely renal replacement therapy in resource-limited areas of the developing world S Gupta 1 Introduction Timely availability of a kidney specialist poses a formidable challenge in ICUs located in tier II and tier III cities of the developing world. Renal replacement therapy (RRT) is often required in the ICU for acute renal failure patients but availability of a nephrologist/ specialist is scarce, leading to unnecessary and risky transfer to higher centers in metropolitans or even worse to death. We explored whether a remotely monitored ICU -an electronic ICU (eICU) -would help mitigate this demand-supply gap. Methods This retrospective study was conducted at four Critinext affi liates where the eICU was being used to provide 24 × 7 support on 89 ICU beds from a remote command center with intensivist and other requisite staff . The eICU had complete access to the patient's real-time vitals, hemodynamic parameters, imaging, laboratory values, audiovisuals and appropriately engineered smart alerts. The eICU model was further extended in initiating and getting RRT done in patients whenever deemed necessary in times of unavailability of a specialist at the same site. Patient baseline demographics, including risk factors, severity score, all-cause mortality at 30 days, transfers to higher center for RRT and its prevention were recorded. Descriptive analysis was performed. Between-group comparison was performed by applying the chi-squared statistic, signifi cance was assumed at a value of P <0.05. Out of a total of 5,146 admissions, 752 inpatient fi les with acute kidney injury/acute renal failure were reviewed, January to July 2013 (n = 373) and July 2013 to January 2014 (n = 379) pre and post eICU implementation respectively. Results While baseline demographics and the patient profi le in the two groups did not show statistically signifi cant diff erence, mean APACHE II score was 14.25 ± 1.94 and 14.65 ± 1.76 pre and post eICU respectively; there was a statistically signifi cant diff erence in all-cause mortality at 30 days which decreased from 31 (8.3%) to 16 (4.2%) pre and post eICU respectively, a reduction of >49% (P = 0.030) and transfer out for RRT came down by >77%, from 15 (4%) to two (0.5%) post eICU implementation (P <0.002). Conclusion Over the years there is now broad consensus over the benefi ts of eICU intervention in deprived areas [1] . There is now a need for a paradigm shift to elevate specialized care to improve outcomes. Our small study has clearly indicated the benefi ts in outcome and economics even while providing intervention in such remote areas. An eICU as a bridge to the demand-supply gap needs to be explored and utilized further to its full potential in the emerging world. Figure 1 (abstract P503) . Sample heart rate decision tree.
The aim was to evaluate outcomes and resource use and to investigate the association between organizational factors and effi cient resource use in a large sample of Brazilian ICUs. Methods A retrospective cohort study in 59,483 patients (medical admissions: 39,734 (67%)) admitted to 78 ICUs (private hospitals, n = 67 (86%); medical or medical-surgical, n = 62 (79%)) during 2013. We retrieved demographic, clinical and outcome data from an electronic ICU quality registry (Epimed Monitor System). We surveyed ICUs using a standardized questionnaire regarding hospital and ICU structure, organization, staffi ng patterns, process of care and family care policies. Effi cient resource use was assessed by estimating standardized mortality rates (SMR) and standardized resource use (SRU) adjusted for the severity of illness according to the SAPS 3 score, as proposed by Rothen and colleagues [1] . Results The median admissions per center was 898 (IQR 585 to 1,715) and SAPS 3 score was 41 (33 to 52) points. Median ICU length of stay was 2 (1 to 5) days, and ICU and hospital mortality rates were 9.6% and 14.3%, respectively. Estimated SMR and SRU were 0.97 (0.72 to 1.15) and 1.06 (0.89 to 1.37), respectively. There were 28 (36%) most effi cient (ICUs with both SMR and SRU <median), 28 (36%) least effi cient (ICUs with both SMR and SRU >median), 11 (14%) overachieving (ICUs with low SMR and high SRU) and 11 (14%) underachieving (ICUs with high SMR and low SRU) ICUs. Most effi cient ICUs were usually located in private and accredited hospitals, with step-down units and training programs in critical care. In univariate analyses comparing most and least effi cient ICUs, ≥2 clinical protocols (OR = 7.22 (95% CI, 1.41 to 36.97)) and graduated nurse/bed ratio >0.25 (OR = 4.40 (1.04 to 18.60)) were associated with effi cient resource use. Daily checklists also tended to be associated with effi cient resource use (OR = 2.89 (0.95 to 8.72), P = 0.057). Conclusion We observed a great variability in outcome and resources in a large sample of Brazilian ICUs. Implementation of clinical protocols and nursing staffi ng patterns can be targets to improve the effi ciency in resource use in emerging countries such as Brazil.
Introduction The aim of the study was to assess the blood pressure (BP) and heart rate (HR) changes during shifts in ICU nurses in relation to their work experience. Our hypothesis was that less experienced nurses, in comparison with more experienced ones, would be subjected to more work stress and this could be demonstrated by higher changes in BP and HR during shifts. Methods We enrolled 23 nurses working in an 8-hour shift schedule at a general adult ICU. Demographic and clinical data were obtained by completing a short questionnaire. The nurses were invited to measure their BP and HR at the beginning, in the middle and at the end of their shift. An ESH/BSH-certifi ed automatic device was used for the BP and HR measurements. 
The survival of patients transported by ambulance to the emergency department (ED) depends on clinical conditions, patient-related factors and organisational prehospital set up. Data and information concerning patients in the prehospital system could form a valuable resource for assessing potential risk factors associated with adverse outcomes and mortality. Our aim was to describe ambulance transports to the ED and identify prognostic factors accessible in the prehospital phase and associated with 7-day mortality. Methods We included all adult patients (≥18 years) with a fi rst-time ambulance transport to the ED at Odense University Hospital in the period 1 April 2012 to 30 September 2013. Ambulance personnel recorded vital signs and other clinical fi ndings on a structured form on paper during the ambulance transport. Each contact was linked to information from population-based healthcare registers in order to identify comorbid conditions and information on mortality. Demographic factors and fi rst registered vital sign were analysed by univariate logistic regression analysis, with 7-day mortality as outcome. Results In total, 18,572 fi rst-time ambulance contacts were identifi ed in the period of inclusion. Overall 7-day mortality was 4.3% (95% CI = 4.0 to 4.6). Univariate analysis showed increasing age, Charlson Comorbidity Index ≥2, vital parameters outside the normal reference range and summoned physician-assisted mobile emergency care units to be associated with 7-day mortality. Further analyses are currently being carried out. Conclusion We found that several prehospital-registered vital signs recorded by ambulance personnel at fi rst contact with the patient were prognostic factors of 7-day mortality.
Introduction Critically ill patients may require interhospital transfer for specialist care or because of a lack of local ICU capacity. Harm is assumed from delays that result, but it is not clear whether these delays are due to transfer distances or defi ciencies in the organisation of care. Methods In total, 151 of 15,602 deteriorating ward patients in the (SPOT)light study [1] were transferred rather than admitted locally. We defi ned delay as the time from critical care assessment in the fi rst hospital to arrival in critical care in the second hospital. We used hospital postcodes to derive latitude and longitude, and calculated both geodesic (straight-line) distances ( Figure 1 ) and road distances between the sites using R version 3.1.1 [2] . We compared daytime versus overnight (7:00 pm to 7:00 am) transfer durations assuming traffi c would contribute less to delay overnight. Mapping and visualisation was performed on Quantum GIS version 2.4 [3] .
The median delay to admission was 22 hours (range 41 to 167 hours). The median geodesic distance was 18 km (range 1 to 141 km), and road distance was 24 km (range 2 to 180 km). Correlations between time delay and geodesic/road distances were weak ( Figure 2 , R 2 = 0.015 and 0.011, respectively). Transfer delays in the daytime and overnight were similar (Wilcoxon rank sum, P = 0.6). Conclusion Interhospital transfers are subject to clinically signifi cant delays, and substantial travel distances. Delays are only weakly correlated to distances travelled and may refl ect delays resulting from organisational ineffi ciencies. We infer that eff orts to improve the effi ciency of transfer should focus on local organisational issues. There was no diff erence in the duration taken for overnight versus daytime transfers.
Introduction Discharge from intensive care is a potentially vulnerable time for patients who are recovering from critical illness. Recent data from the ANZICS group have highlighted that the mortality diff erence in those patients who are discharged out of hours is nearly twice that of those discharged during the day [1] . These results have been replicated in our institution with a mortality of 8.7% (discharged 22:00 to 06:59) versus 4.8% (discharged 07:00 to 21:59). In the UK, NICE CG50 advised that transfer from critical care to the ward out of hours should be avoided and documented as an adverse event. We postulated that one important factor in our hospital is the decreased medical and nursing cover overnight and so looked at the delay from discharge to fi rst medical review and to outreach review. Methods The case notes of 100 consecutive patients discharged to the ward between September 2013 and October 2013 were examined to identify the time of discharge from the ICU and the subsequent fi rst review by the receiving medical team and the Critical Care Outreach team. The grade of the doctor reviewing the patient was recorded.
Results Of these 100 patients, 22 were discharged between 22:00 and 07:59. From the 100 case notes requested, only 50 were available for examination. Forty patients were discharged to the wards, with only 37 having further documented medical reviews in the notes. Only 62% of patients were reviewed by a consultant following intensive care, with over 20% of patients waiting more than 24 hours for any medical review. During this time 18% of patients received a review by the nurseled outreach team. See Figure 1 .
Conclusion It is clear that a highly vulnerable group of patients who are recovering from critical illness [2] receive inadequate early follow-up within the hospital. We postulate that the delay in medical review and the lack of senior review may be caused by over 40% being discharged overnight and contribute to the increased mortality seen in our institution and the ANZICS study [1] with nighttime discharges.
Introduction Wild mushroom poisoning (MP) is an important medical emergency that may have bad clinical outcome. We aimed to evaluate the clinical and laboratory features of patients with wild MP admitted to our emergency department in the Central Black Sea Region and to inform the emergency department physicians about early diagnosis and management of wild MP in the light of obtained data. Methods This study was designed retrospectively by examining fi les of the patients with wild MP who were admitted to Ondokuz Mayis University Emergency Department, between January 2008 and December 2012. Patients older than 18 years were included in the study. Patients were evaluated according to gender, age, location, duration between mushroom intake and the start of clinical symptoms, time of application to hospital, clinical features and fi ndings and treatment method. The number of patients has been compared with the regional distribution of population, monthly temperature and average annual rainfall. Critical Care 2015, Volume 19 Suppl 1 http://ccforum.com/supplements/19/S1
Results A total of 420 patients poisoned by wild mushrooms were studied. The male/female ratio was 1/1.5. The age of patients changed from 18 to 92 and mean age was 46 years. MP constituted 13.3% of all intoxication cases. The time when the fi rst symptom occurred after mushroom intake was a mean 2 (0.17 to 2.15) hours. Of the patients, 47.6% lived in villages, 38.6% in towns and 13.8% in city centers. Admissions were mostly made in autumn, with 57.6%. Eighty-six percent of intoxications happened because of wild mushrooms collected in nature. The most frequent symptoms were nausea (93.8%), and vomiting (87.1%). Increase in liver function tests in 47 patients was observed. Two of these patients died while 10 patients were transferred to further centers for liver transplantation. The remaining patients were discharged from the hospital. Conclusion Wild MP can cause bad clinical outcome. The public should be informed about the probable hazards of wild mushroom ingestion because collection and consumption of wild mushrooms from nature is common. Public health units should take protective precautions against wild MP. Education of health personals regarding MP will lead to successful results in patient management. Introduction Many patients with drug overdose are sedated, but do not have medical reasons to warrant ICU admission. Historically, monitoring behavior and suicide risk was done in the ICU, until the patient was awake enough for psychiatric consultation. Methods A medical psychiatry unit (MPU) was instituted as part of the Department of Clinical Psychiatry. For all patients with drug overdose in the emergency department, a risk assessment was made by the intensivist. Those without ICU indication (such as cardiac or respiratory monitoring) were admitted to the MPU. Alternatively, when awake enough, they were seen by the psychiatrist immediately. We performed an analysis of all patients with drug overdose, admitted to our ICU (before MPU n = 88, after MPU n = 191). We used the Welch t test for comparisons.
Results After institution of the MPU, there was a 28% reduction in the number of patients with drug overdose per month, admitted to the ICU. Also, patients admitted to the ICU were sicker and stayed longer (see Table 1 ). There were no patients admitted to the ICU after initial MPU admission. 
It is estimated that about one in 10 patients may be harmed by adverse events during their hospital stay [1] . Transforming organizational culture to improve patient safety culture is considered important. We conducted a prospective, controlled study to assess the impact of a standardized patient safety course on an ICU's patient safety culture, using a validated patient safety culture assessment tool. Methods Staff from two ICUs -ICU1 (tertiary referral hospital) and ICU2 (district hospital) -in Hong Kong were recruited to compare changes in the measured safety culture before and after a patient safety course. The BASIC Patient Safety course was only administered to staff from ICU1, and safety culture was assessed in both units before and after, using a survey based on the Hospital Survey on Patient Safety Culture [2] . Relative risk (95% CI) of improvement: baseline to follow-up in hospitals in patient safety domains, adjusted for duration of work in the unit (≤10 years vs. >10 years), was calculated. [1] . This audit looked at whether ICU documentation of investigations involving ionising radiation could be improved. Anticipated benefi ts would be improved communication between the multidisciplinary team and betterinformed decision-making.
Methods Patients admitted to the ICU between 21 September 2014 and 2 October 2014 were included. If an investigation did not involve ionising radiation or was not requested by intensive care clinicians it was excluded. The indication for imaging was noted, and patient notes were analysed no less than 48 hours after the imaging was reported. Results As shown in Figure 1 , imaging requests were generally poorly documented (61%). In total, 17/26 (65%) chest X-rays (CXRs) were documented. A total of 0/2 CT scans were documented, despite one showing acute changes. In total, 17/20 (85%) CXRs requested following procedures carried out on ITU (such as insertion of central venous catheters) were documented, and the three not documented had no signifi cant fi ndings. The six other CXRs were requested to investigate worsening respiratory function. None were documented. Five had signifi cant fi ndings. Conclusion Investigations following procedures were generally well documented, but investigations seeking pathology were not documented at all, regardless of the fi ndings. This may have infl uenced the management of the patient and compromised patient safety. As such, the audit was presented at a departmental meeting to emphasise the importance of imaging documentation. A place for investigations
Introduction ICU workfl ow for physicians, nurses and other healthcare providers is often complicated by distractions, interruptions, diffi culties to concentrate, multitasking and the need to frequently change priorities in patient care-related procedures. It is therefore a continuous challenge to communicate and implement plans in an early, effi cient and reliable way within the team and to constantly keep track about tasks that are completed, postponed or that are about to be forgotten.
To facilitate every caregiver's workfl ow and to early and effi ciently communicate, we implemented the After Round Comprehensive Plan Summary (ARCoPS) tool. Methods Most of our patient care-related therapeutic and diagnostic decisions are made during the morning round consisting of intensivists, nursing team, surgeons, respiratory therapists, dieticians, clinical pharmacists and physiotherapists. Immediately thereafter, all plans for the next 24-hour period are again discussed, summarized, confi rmed, prioritized and organized by the multidisciplinary team under aspects of optimal patient care and workfl ow effi ciency by entering the plan data into a fl at screen visualized template connected to our intranet immediately accessible at every physician and nursing caregiver workstation.
Results Twelve months after implementation of the ARCoPS tool we conducted caregiver interviews and identifi ed the following eff ects: increase in treatment confi dence through standardized multidisciplinary decision-making; reduced loss of information through immediate plan summarization by the whole team; reduction of ambiguity and misunderstanding about care plans through early written documentation; higher level of security not to forget procedures or tasks; positive acceptance of the tool to fl exibly change priorities of care procedures; positive acceptance of the tool to mark accomplished tasks providing visual feedback about the care plan status; individual caregiver workfl ow economization through permanent treatment plan availability; and higher job satisfaction throughout the caregiver team. Conclusion ARCoPS tool utilization has become a daily routine in our ICU. It functions as an eff ective communication and workfl ow tool and has helped us to reduce patient care-related misunderstandings and delays. It also enhanced the economics of our work sequence, which also highly contributes to a better level of patient safety. Furthermore, it has markedly contributed to an improved level of quality of work for caregivers. 
The aim of this Thai-SICU was to study the incidence and outcome of adverse events in nine SICU university-based hospitals in Thailand.
Methods This multicenter prospective observational study was done in >18 years old, admission >6 hours surgical patients admitted to the large, postgraduate medical training university-based SICU during April 2011 to January 2012. Patient data were divided into three main phases as admission, discharge data and daily CRFs during the ICU stay. The patients were followed until they were discharged from the ICU or up to 28 days of their ICU admission and up to 28 days following discharge from the ICU if they survived. Results Following a 19.7-month recruitment period, a total of 4,654 patients (17,579 ICU-days) were included in the analysis process. Admitted patients had the median age of 64 years. Most of the patients were admitted directly from the OR for postoperative monitoring with median APACHE II score 10, 23% were admitted with priority I who needed aggressive hemodynamic resuscitation and respiratory support. ICU mortality and 28-day mortality were 9.61% and 13.80%. Each day of ICU increment was associated with a 1. Conclusion This is the largest systemic surveillance observation in the SICU. The study results are the reference for future research and also provide information for patient and relative advice when confronted with adverse events during SICU admission.
Retrospective observational cohort study of mortality and length of stay for surgical ICU admissions M Hameed 1 , M Maruthappu 2 , D Marshall 3 , M Pimentel 1 , L Celi 4 , J Salciccioli 3 , J Shalhoub 3
The aim was to know the frequency, criteria and implications of rejection for ICU admission to our ICU unit, a secondlevel hospital (18 beds). Methods An observational retrospective study in a time interval of 6 months (January to June 2013). We retrospectively registered all patients rejected for admission to our unit, analyzing the clinical rejection report used in our hospital. From this report we extracted diff erent variables: demographical (age, sex), provenance (emergency room, hospital), clinical (comorbidity, functional situation, diagnosis, reason of requesting admission), rejection motive ('too good' , 'too bad' , futility, lack of beds, patient rejection), whether it was defi nitive or conditional, whether the patient was admitted afterwards, and the state at hospital discharge. We realized a descriptive analysis (frequencies) and multivariant analysis of the factors related to futility rejection. Results There were 165 rejections, which represents 25% of total ICU patients evaluated for admission. A total of 59.4% were male. Mean age was 69 ± 7 years (19 to 98). In total, 53.9% had more than two comorbidities (pluripathological) and 31.5% moderate to severe functional disability. The cause of rejection was in 55.2% of situations that the patient was 'too good' , 37.6% related to qualitative futility, 4.8% was 'too bad' and in 1.2% a mix of lack of space (beds) and patient rejection. In the multivariant analysis the signifi cant variables related to futility rejection were age (by years) with an OR of 1.05 (1.02 to 1.08), severe functional disability, OR of 4.35 (2.09 to 9.06), and the hospital provenance with an OR of 2.82 (1.1 to 7.2). Conclusion Rejection for admission to ICU units is a frequent medical activity in our day-to-day job. The type of patient most rejected is cardiologic, mostly evaluated for thoracic pain probably ischemic but with low risk. In second place we found patients for which we decide rejection based on subjective qualitative futility, related mostly to age, prior functional disability and provenance. [1] [2] [3] . This prospective service review assesses the impact of age, APACHE II score and WHO functional score towards admission acceptance or refusal to ITU in a tertiary-level facility. Methods Design: a planned prospective review of all referrals over a 14day period. Data collection: review (LT, DP, SP) of case notes of patients referred to ITU with the following variables collected: age, sex, APACHE II scores, WHO functional status score, grade of referrer and source of referral. Data were collected on 37 patients: 22 accepted to ITU and 15 refused admission. Statistics: data were analyzed using GraphPad 6.05. Categorical variables were expressed as mean and standard error of mean. For unpaired variables, statistical signifi cance is determined using unpaired t test. P <0.05 is considered statistically signifi cant.
The WHO functional status was the most signifi cant variable aff ecting admission (P <0.001). The APACHE score of patients admitted to ITU was signifi cantly lower than refused patients (P = 0.039). Patient age did not aff ect admission status (P = 0.15). See Table 1 . Introduction This Six Sigma project was initiated to evaluate and improve the transfer of care of patients from the OR to the ICU. Medical errors are responsible for billions of dollars in increased healthcare spending. Miscommunication among healthcare providers is a major contributor to these errors, with handoff s a particularly vulnerable period in the care process. At our institution, surgical patients with scheduled admissions to the ICU are fi rst recovered in the postanesthesia care unit (PACU). With this process, multiple, unstructured, and individual handoff s occur in parallel between providers, which may lead to communication errors, diff erential information sharing, content variability, care delays, and ineffi ciency. Methods A multidisciplinary QI project was initiated with input from the ICU, anesthesia and surgical services. A series of PDSA cycles were conducted, which began by defi ning the current process via direct observation and value stream mapping of orthopedic and neurosurgical patients. A new process was then introduced, including direct transfer of the patient to the ICU and a single, structured, bedside report between all care providers. A standardized handoff tool was implemented. We used process times, wait times and information content as process measures and handoff errors as outcome measures. A 10-point satisfaction score was also measured. Results Following implementation of the new transfer process, the average wait time decreased by 58 minutes, process time decreased by 9 minutes, and lead time decreased by 66.5 minutes. The handoff error rate decreased by 1.3 errors/patient and fi rst-time quality rate increased by 67%. Staff satisfaction improved from 48% to 73%. By elimination of the PACU stay, the costs involved in admission to the PACU were deferred. Conclusion A single, multidisciplinary bedside handoff process between the OR and ICU leads to cost and time savings. By elimination of redundant, nonvalue-added processes, less opportunity for medical errors occurred, with substantial improvements in fi rst-time quality. Such a process can be successfully attained while aff ecting staff satisfaction positively.
Introduction Prolonged hospital stay prior to admission to the ICU was previously shown to be independently associated with poorer outcome [1, 2] . This is probably due to slow deterioration of physiological function in hospital and infl uenced by processes leading to critical care admission [2] . We investigated whether commonly measured severity scoring systems (Acute Physiology and Chronic Health Evaluation (APACHE) II, Intensive Care National Audit and Research Centre (ICNARC) and Sequential Organ Failure Assessment (SOFA) scores) are signifi cantly diff erent in patients admitted with prolonged pre-ICU hospital length of stay, and describe mortality and hospital length of stay.
Methods A retrospective analysis of prospectively collected data of all emergency admissions in the ICNARC database of a 44-bed adult critical care unit within a major trauma centre over a 2-year period. Demographic data, APACHE II score, SOFA score, ICNARC model score, mortality, and length of hospital stay prior to and after ICU admission were collected. Five groups of patients were defi ned as follows: those admitted to ICU within 1 week of hospitalization (group 1), within 8 to 14 days (group 2), within 15 to 21 days (group 3), within 22 to 28 days (group 4), and more than 28 days (group 5). Chi-squared and ANOVA tests were performed using the SOFA statistics package. Results A total of 2,248 emergency admissions were analysed. The majority of patients were admitted within 1 week of hospital admission (n = 1,897). They were younger and had lower APACHE II scores (15 vs. 19; P <0.001). APACHE II scores were the same in all other groups (groups 2 to 5). Patients admitted to the ICU 3 weeks following hospital admission had signifi cantly higher hospital mortality (up to 50%; P <0.001) and ICU length of stay (12 ± 15 vs. 8 ± 10 days; P <0.001). ICU mortality remained the same in all groups (20 to 28%). ICNARC and SOFA scores were equal between the groups. The post-ICU lengths of stay were signifi cantly longer in groups 3 to 5. In-hospital CPR prior to admission to ICU was lower in patients from groups 4 and 5, probably signifying appropriate DNAR decisions made on the ward. Conclusion Prolonged pre-ICU hospital admission is associated with higher hospital but not ICU mortality. Commonly measured scores do not refl ect this higher mortality in patients admitted after prolonged stay in hospital. Further research into parameters that may refl ect changes in physiological reserves may strengthen these scores for such patients.
Introduction Hospital costs are a constant concern within health, especially in the ICU. Hospital admissions and average life expectancy have been growing gradually mainly in older and critical patients. This study is aimed to observe the direct costs of patients admitted to an ICU and their relation to the SAPS 3, length of stay in the ICU and fi nal outcome.
Methods A retrospective observational study in which the direct costs were studied (materials, medicines, oxygen therapy and hospital fees) for 1,790 ICU patients from November 2013 to November 2014. The readmissions within 48 hours were excluded and also 10% of patients who had the highest and lowest costs. The remaining 1,401 patients were divided by age groups.
Of the patients studied, 54.6% were male. Average age was 57.8 years (18 to 105 years). The biggest ICU average cost was in the group of patients 81 to 90 years old (US$793.00), as well as longest ICU stay (9.25 days), highest SAPS 3 (53.96) and higher ICU and in-hospital mortality (14.29% and 19.25% respectively). This study shows that the direct cost of the ICU stay for older patients was higher than for younger patients. The diff erence was explained by the higher severity measured by SAPS 3 in the older age groups (Figure 1) , and the required greater length of stay in the ICU ( Figure 2 ). As might be expected, the mortality in the group of older patients was also signifi cantly higher.
Conclusion This study showed that greater age is associated with higher severity measured by SAPS 3, higher direct costs, and higher mortality both in the ICU and in-hospital environment. Introduction Pregnancy and labour usually progress uneventfully; however, serious complications can occur and develop rapidly, necessitating critical care admission and support. Successive confi dential enquires have highlighted defi ciencies in this area and suboptimal care leading to increased morbidity and mortality [1] . The National Maternity Hospital is the largest maternity hospital in the Republic of Ireland where a total of 8,954 babies were delivered in 2013. It is a stand-alone institution and onsite facilities include a twobed dedicated anaesthesia lead high-dependency unit. Methods A retrospective observational study was carried out from January 2011 to January 2014 especially looking at the following parameters: admitting diagnosis, demographics, length of stay and the number of admissions requiring transfer for tertiary-level care.
In total, 29,344 deliveries occurred. A total of 376 HDU admissions were recorded in this period, representing 1.28% of all admissions. The average age of patients admitted to the HDU was 34 years. The predominant reasons for admission were hypertensive disease of pregnancy (49.7%), haemorrhage (antepartum/postpartum) (36.4%), sepsis (4.2%) and other reasons (11.1%) including cardiac rhythm disturbances, neurological complications and pre-existing medical disease. In 2013 the average length of stay was 2 days. A total 6.1% of those admitted to HDU required transfer for tertiary-level ICU care in other centres during the study period, this represented 0.07% of all deliveries. Conclusion There is signifi cant demand within our institution for HDU care for our patients, with the number of admissions increasing in 2013.
The main admitting diagnoses are hypertensive disease of pregnancy and haemorrhage with an increase in the number of patients being admitted for management of sepsis in 2013. This highlights the increasing awareness, recognition and management of this condition in pregnancy. The increased number of HDU admissions in 2013 could also be explained by the recent introduction of an early warning score for the deteriorating patient in our hospital but this would require further evaluation. The low number of transfers of patients to other tertiary centres underpins the importance of an anaesthesia lead service.
Introduction The objectives of this survey were to establish the prevalence of symptoms of post-traumatic stress in mixed staff groups working in adult and paediatric intensive care settings and to examine the main themes in staff descriptions of the most traumatic event they had experienced at work. Methods A total of 355 health professionals working on three adult and four paediatric units at two centres were asked to rate their current level of post-traumatic stress symptoms on the Trauma Screening Questionnaire (TSQ). Results Paediatric/neonatal intensive care staff were more likely to score above the clinical cutoff point for post-traumatic stress symptoms on the TSQ in relation to an incident at work than adult intensive care staff in this sample (PICU n = 33/193 (17%) vs. AICU n = 13/162 (8%), P <0.001). For the 172 staff who provided a description of the most traumatic event they had experienced, the following themes were in 88 (4.6%) cases. The mortality rate was 10.9%. The following factors were associated with mortality in univariate logistic regression: age, body mass index, past medical history, TBSA, FTBSA, intoxication (CO, CN), inhalation injury, fl ame burns, self-infl icted burns (all P <0.0001), sex (P <0.001), and admission date (P <0.01). Simple periodic regression showed a biannual seasonal eff ect on mortality, documented with a 1-year periodic (P <0.01) and a 6-month periodic (P = 0.01) dependency. Multivariate analysis with or without periodic terms identifi ed age, past medical history, TBSA, FTBSA, inhalation, intoxication and admission date as the only factors independently associated with mortality. Methods Twenty-two patients suff ering GP admitted to the ICU, between January 2002 and December 2012, were included. The Acute Physiology and Chronic Health Evaluation (APACHE) II prognostic score scale was used in order to assess the severity of illness on the fi rst ICU day. The Sequential Organ Failure Assessment (SOFA) score was used to measure organ dysfunction, and the Birmingham vasculitis activity score for Wegener granulomatosis (BVAS/WG) was used to assess vasculitis activity. The outcome measurements taken into account were ICU mortality and ICU length of stay. Results One patient was admitted twice during this period. The sample comprised 11 males and 11 females (50%, respectively). Featuring an average age of 52 years, 78% of them were admitted to the ICU because of respiratory failure, 50% were due to diff use alveoli hemorrhage, 36% due to sepsis, 4% hypovolemic shock and fi nally 4% because of tuberculosis. According to the BVAS/WG, 20 patients corresponded to severe disease, one to limited diseases and one to persistent disease. The average ICU length of stay was 20.6 days and as inpatients 43 days. While comparing the SOFA score between alive and deceased patients there was a 0.5-point diff erence (P = 0.077), 63% of the alive patients were diagnosed while they were in the ICU. Plasmapheresis was found to be a protector factor (P <0.05). Conclusion The BVAS/WG score was signifi cantly diff erent between alive and deceased patients. Plasmapheresis was found to be a survival predictor. This study has shown that both SOFA and APACHE II scores have no prognostic value in these patients.
There is increasing demand for revision hip surgery in older patients with poor frailty. Our previously submitted work demonstrated that frailty predicts the need for medical review [1] . We reviewed patients for a further 16 months to see whether frailty impacts on care [2] . This is the largest reported study reviewing frailty and the need for organ supports and outcomes in complex orthopaedic surgery.
Methods A retrospective note review of all patients from January 2012 to April 2014 undergoing revision hip surgery. Data collected included frailty, comorbidities, operative blood loss, anaesthetic technique and level of organ supports and patient location at 30 days.
A total of 389 patients with a mean age of 68.7 years were identifi ed. Frail patients were signifi cantly more likely to need vasopressors postoperatively (P = 0.012). Each increase in frailty score was associated with 0.16 increase in length of stay on the HDU (P = 0.025). Analysis of patient location at 30 days shows that frail patients stay in hospital longer (P = 0.00). Frail patients also bleed more intraoperatively (P = 0.00 with a coeffi cient value of 239; that is, for every point increase in frailty, average blood loss increases by 239 ml). For each increase by unit of blood transfused, the length of stay increased by 5.3 days (P = 0.000). The use of epidural is not associated with increased need for postoperative vasopressors (P = 0.598). See Figure 1 .
Conclusion Frailty is associated with increased intraoperative resource use and postoperative care requirements independent of choice of anaesthetic technique. This type of surgery should be subject to health economic analysis as demand amongst the frailer surgical population increases. References demographics, physiological measurements and coexisting conditions and can be used to evaluate ICU performance, to stratify patients in clinical trials and to assist in-hospital and healthcare decisions such as resource allocation. The aim of the project was to determine whether a general score derived from routine laboratory parameters could be used to predict mortality rates in patients admitted to the ICU in the UK.
Methods P values were calculated using the t test, Mann-Whitney U test and chi-squared test, depending on distribution of data, in order to determine which variables were signifi cantly diff erent in the survivors and nonsurvivors of critical illness. Signifi cant variables were categorised into subgroups according to medically relevant landmarks and univariately analysed by assessing the correlation with mortality. Forward logistic regression models were used to choose the parameters to include in our score. ROC curves illustrated the sensitivity and specifi city of selected variables via their AUC. Results Age, platelets, ALT and APACHE II were selected to be included in the new laboratory-based score. The AUC for the score was 0.714, which was higher than each of the individual laboratory parameters.
The AUC was increased further to 0.781 by including all 14 variables (age, lactate, FiO 2 , urea, creatinine, ALT, APACHE II, platelet, bicarbonate, haemoglobin, pH, ionised Ca, carboxyhaemoglobin and albumin), although this improvement was not considered signifi cant as the confi dence intervals of the two scores (4 and 14 variables) overlapped. Conclusion A laboratory-based score was successfully established in ICU patients, revealing an AUC of 0.714 which is comparable with established scores in a similar population. The compilation of the variables to produce a laboratory-based score showed greater prognostic power than individual variables. Model developers require an AUC of >0.7 to be termed useful; however, in order to be used in a clinical setting the AUC must be at least 0.75. Further research including internal and external validation studies must be performed to optimise the model before clinical implementation. Among ICU patients a high level of trait anxiety is relatively common and associated with intrusions, a symptom of PTSD. Independently, childhood trauma and stress exposure throughout life have been associated with depression. In cardiac surgery patients admitted to the ICU postoperatively, the eff ect of trait anxiety on the relationship between cumulative life stress and stress-related psychopathology remains unknown. Therefore we aimed to assess the mediating or moderating role of trait anxiety in this at-risk patient population. Methods In this multicenter follow-up study of the Dexamethasone for Cardiac Surgery (DECS) trial, validated self-report questionnaires were sent 1.5 to 4 years after cardiac surgery and ICU treatment to assess symptoms of PTSD and depression, in relation to cumulative life stress (that is, childhood trauma, major stressful life events) and trait anxiety as determinants of psychopathology. Data were available for 1,125 out of 1,244 (90.4%) eligible participants. Mediating and moderating analyses were performed with multivariable linear regression to assess the eff ect of trait anxiety. Subgroup analyses were performed for both sexes.
Results Trait anxiety partially mediates the relationship between cumulative life stress and PTSD (β-value reduction from 0.325 to 0.068; P = 0.000 to P = 0.003) and fully mediates the association between cumulative life stress and depression (β-value reduction from 0.282 to 0.015; P = 0.000 to P = 0.507). Trait anxiety was not a moderating factor between cumulative life stress and psychopathology. Full mediation of trait anxiety was found in female patients (n = 247), whereas only partial mediation was seen in male patients (n = 878) with regard to PTSD symptoms. As for depression, full mediation was present in both female and male patients. Conclusion In cardiac ICU patients, trait anxiety mediates the infl uence of cumulative life stress on the occurrence of PTSD and depression symptoms. Further prospective research is necessary to establish these factors as reliable measures for the early identifi cation of ICU patients at risk for stress-related psychopathology. Introduction Although the ICU survival rate has increased in the last decade, the negative eff ects on mental health and related quality of life become more clear. In the literature the prevalence of anxiety and depressive symptoms post ICU ranges from 10 to 43% [1] . Early recognition and treatment of anxiety and depressive symptoms is important because depression caries a risk for suicide, limited quality of life, and delayed return to work. We studied hospital anxiety and depression (HAD) symptoms after ICU discharge.
Methods Patients who were treated in our ICU from 1 January 2013 until 31 December 2013 for more than 5 days were invited to visit our post-ICU aftercare clinic. Six weeks after discharge they received a letter of invitation together with a health-related questionnaire, the Hospital Anxiety and Depression Scale (HADS) questionnaire [2] . Patients were asked to return the questionnaire prior to their visit. All data were analyzed and if the HADS score indicated a clinically signifi cant anxiety or depression, patients were referred to a psychologist for further analyses and treatment. All patient data were analyzed retrospectively. Results Seventy-nine patients, 54 men and 43 women, mean age 57 years. Median APACHE II and IV was 18 and 60 respectively. Median ICU and hospitals days were 9 and 20 respectively. Seventy-six percent were mechanically ventilated with a median of 5 days. Median time after ICU discharge to aftercare visit was 165 days. Patients were divided into three categories: 1, no HAD (45.4%); 2, possible HAD (9.3%); and 3, clinically signifi cant HAD (45.4%). Women compared with men showed signifi cantly more HAD symptoms (26.8% vs. 18.6%, P <0.05). Patients with subarachnoid hemorrhage, neurotrauma and multitrauma patients showed more HAD symptoms. Pain, fatigue, muscle weakness, impairment of daily activity dyspnea, and hoarseness were signifi cantly associated with clinically signifi cant HAD. No association between age and HAD was found. Diagnosis at ICU admission, length of stay, severity of illness, delirium and use of sedatives were not associated with HAD. Conclusion Prevalence of clinically signifi cant post-ICU HAD was 45.4%. Female sex and post-ICU physical complaints -pain, fatigue, muscle weakness, impairment in daily activities, hoarseness and dyspneawere signifi cantly associated with HAD.
effi cacy of such interventions. Previous work has shown that intensive care patients undergo many stressful experiences, which can aff ect their long-term psychological well-being. Studies have demonstrated a high prevalence of depression, anxiety or post-traumatic stress disorder after intensive care admissions. Methods A systematic review was carried out according to the Prisma statement. A search was conducted of Medline, Embase and Psychinfo databases. Inclusion criteria included studies of populations of adult patients in mixed or general ICUs. No study designs were excluded, but studies that focused on specifi c disease states were excluded. Included studies were assessed for risk of bias, using a quality checklist. Results A total of 1,743 papers were retrieved, of which 18 studies were eligible for inclusion in the review. Studies had a combined population of 1,970 patients admitted to 38 ICUs from Europe, Asia and North America. Eleven studies were randomized controlled trials (RCTs).
Interventions were classifi ed as four groups -music; therapeutic touch; diary and psychotherapeutic interventions. Ten studies found that music interventions were eff ective in the short term; however, followup results were limited and some studies were low quality. There was moderate quality evidence from three studies for the eff ectiveness of diary interventions, with medium-term follow-up results. There was mixed-quality evidence for therapeutic touch interventions in the short term from three studies. The two psychotherapeutic interventions studied were of moderate quality, and one showed promising results at 12-month follow-up.
The evidence for the effi cacy of nonpharmacological interventions to reduce short-term or long-term stress in intensive care patients was of low to moderate quality. Studies included mainly shortterm and medium-term follow-up. This highlights the need for largerscale, better-quality RCTs with longer-term outcome measurement. However, the results indicate that nonpharmacological, including psychological, approaches are likely to be benefi cial for reducing shortterm or long-term stress in intensive care patients. Introduction Patients recovering from critical illness suff er many physical and psychological problems during their recovery, including muscle weakness, fatigue, signs and symptoms of PTSD, anxiety and depression [1] . At present, specialist intensive care follow-up and rehabilitation is inconsistent and in many geographical areas is nonexistent. As a result, many survivors of critical illness will require using existing community rehabilitation services [2] . The aim of this present service evaluation was to understand the utilisation of community rehabilitation services by critical care survivors. Methods A database of acute referrals to community rehabilitation services was retrospectively analysed from 1 May 2014 to 31 October 2014. Age, referring specialty and reason for referral for rehabilitation were documented. This database was cross-checked with the critical care database in Glasgow Royal Infi rmary to identify which individuals had been admitted to critical care during their admission. Results Over this 6-month period 769 patients were referred from their parent specialty for community rehabilitation in North East Glasgow. Thirty-three of the 769 patients (4.3%) referred had a critical care stay during their admission. Of these, eight patients were referred for rehabilitation by orthopaedics, eight by medicine for the older patients, 11 from acute medicine and the remaining six from other specialties. Six of the 769 patients who had a critical care admission were of working age (<1%). Two individuals were admitted to critical care following trauma whilst four had complex social needs prior to their critical care admission. This included an individual with a high body mass index. None of the individuals of working age were referred as a consequence of their critical care stay. Conclusion This service evaluation demonstrates that very few critical care survivors are referred to community rehabilitation services, particularly those of working age. More work is required to understand optimal rehabilitation pathways in this patient group.
Brazil. The early mobilization protocol consists of fi ve phases: 1passive exercises for the unconscious patient; 2 -active exercises associated with respiratory exercises (patient lying on the bed); 3phase 2 exercises with the patient sitting on the bed; 4 -phase 2 exercises with the patient sitting on a chair or in a standing position; 5 -phase 4 exercises plus walking. All hospital records from patients, between September 2013 and August 2014, were included in this study. Data extracted from hospital records were: age, gender, diagnosis (arrhythmia, coronariopathies, congestive heart failure and other pathologies), length of stay, number of discharge and number in each phase of the early mobilization protocol. Pearson chi-square test was used to compare the number of mobilizations (phase 4 and 5) per group of diagnoses. Odds ratios were calculated for those comparisons found to be statistically signifi cant (P <0.05). Results A total of 697 hospitals records were analyzed. Patients had on average (SD) 67.8 (13.1) years and the majority of them were males (57%). Our results revealed that 65% of patients in the CICU received phase 4 and 43% of patients in the CICU received phase 5 of the early mobilization protocol. No diff erences in the proportion of patients receiving phase 4 or 5 were found among arrhythmia, coronariopathies and congestive heart failure groups. The only diff erence found was between congestive heart failure group and other cardiovascular pathologies (P <0.001). The congestive heart failure group was mobilized 5.6 times (95% CI: 2.7 to 11.5) and 3.2 times (95% CI: 1.7 to 5.7) more than the other cardiovascular pathologies group in phase 4 and 5, respectively. Conclusion A considerable proportion of patients was mobilized without any serious complications in the CICU. Our fi ndings suggest that patients diagnosed with arrhythmia, coronariopathies and congestive heart failure can be equally mobilized in an ICU. Introduction Various therapeutic interventions needed in critical care may refl ect a high risk of death. We evaluated associations between commonly used interventions and hospital mortality in Finnish ICU patients.
Methods We retrieved data on adult patients treated in Finnish ICUs between 2003 and 2013 from the Finnish Intensive Care Consortium database. We used the Therapeutic Intervention Scoring System (TISS-76) for categorizing ICU interventions and the Simplifi ed Acute Physiology Score (SAPS II) for quantifying severity of illness. We excluded readmissions, patients with missing outcome, SAPS II and TISS data. We also excluded very common interventions (arterial line, bolus intravenous medication), very rare ones (prevalence <1%), and interventions only applicable in specifi c populations (intracranial pressure monitoring, intra-aortic balloon assist). We grouped several TISS categories when applicable. We performed a backward stepwise binary logistic regression analysis with TISS items to assess the impact of each intervention on hospital mortality (expressed as odds ratio (OR) with 95% confi dence intervals (CIs)). Age, admission type, and SAPS score (minus age and admission type scores) were adjusted for in the multivariate analysis. Conclusion In this large retrospective multicenter study, the TISS item associated with the highest risk of death was cardiac arrest and/or countershock. Unexpectedly, the independent eff ect of emergency admission was of comparable magnitude in terms of impact on hospital mortality. Of these, in-ICU cardiac arrest might be amenable to preventive measures and should be studied further.
correction for natural decline in HRQOL, the mean scores of four dimensions -physical functioning (P <0.001), physical role (P <0.001), general health (P <0.001) and social functioning (P = 0.003) -were still signifi cantly lower 5 years after ICU discharge compared with their preadmission levels, although eff ect sizes were small (<0.5). Conclusion Five years after ICU discharge, survivors still perceived a signifi cantly lower HRQOL than their preadmission HRQOL (by proxies), and that of an age-matched general population. Importantly however, after correction for natural decline, the eff ect sizes were small suggesting that patients regain their age-specifi c HRQOL 5 years after their ICU stay. Introduction ECMO support in ARDS is an emerging strategy when conventional treatment modalities fail. ECMO has advantages on oxygenation and circulation but also it has some unfavorable eff ects. The most serious complication is brain death due to cerebrovascular hemorrhage. An apnea test is the most important component in confi rming brain death. For patients supported by ECMO, apnea testing remains challenging. Brain-death diagnosis is often made without an apnea test.
Methods We present two cases who receive V-V ECMO support after progression to ARDS. After initiation of ECMO we used sedation to prevent movement and improve adaptation to mechanical ventilation. Also we used anticoagulation with heparin to prevent thromboembolic events and ECMO circuit occlusion. On daily follow-up we noticed that patients had lost their pupil reactions to light. Their sedation was ceased and a computed brain tomography was performed. Both patients had intracerebral hemorrhage. We decided to determine brain death with apnea tests. We increased ECMO blood fl ow and fi O 2 and then decreased sweep gas fl ow and disconnected the patient from mechanical ventilation respectively. In one patient we did not see any spontaneous breathing eff orts after carbon dioxide retention. We concluded that the apnea test was successful and confi rmed brain death. On the other hand, we confi rmed the brain death of the other patient with cerebral angiography due to the occurrence of hypoxia and hypotension during apnea testing. Results We experienced some challenges while determining brain death in patients under ECMO support for ARDS. It is challenging to conduct the apnea testing during ECMO support. Auxiliary tests are required for patients who cannot tolerate the changes needed to conduct the apnea test. With increasing use of ECMO therapies, clinicians may come face to face with more complicated life-ending decisions.
Conclusion Current guidelines do not include brain death criteria using supportive therapies such as ECMO and therefore should be updated.
One of the main goals of intensive care medicine is to reduce the mortality of critically ill patients. However, it is essential to recognize end-of-life care as an integral component of critical care. Besides survival, the success of intensive care should also include the quality of lives preserved and the quality of dying. The objective of this study was to evaluate the incidence and type of end-of-life decisions (ELD) in critical patients that died in an ICU. Methods Analysis of all patients included in an ICU running database and who died from 1 November 2013 to 31 October 2014. The following variables were evaluated: age, gender, reason for admission, SAPS II, length of ICU stay and type of ELD. To classify ELD, four concepts were defi ned: 'Comfort care' , a change from curative therapy to comfort care therapy; 'Limited therapy' , maintenance of curative therapy but without escalating it (for example, no renal substitution); 'Decision not to resuscitate' , not to perform advanced life support if cardiac arrest occurs; and 'Without previous end-of-life decisions' , when there was no prior decision regarding the ELD. Results A total of 507 patients were admitted to the ICU and 132 died (26%). Reasons for admission in those who died were septic shock (47%), post cardiac arrest (13%), cardiogenic shock (8%), and nontraumatic brain bleeding (8%). Fifty-three patients (40%) died after a 'Comfort care' decision, 28 patients (21%) after 'Decision not to resuscitate' and 14 (11%) after a 'Limited therapy' decision. Thirty-seven patients died 'Without previous end-of-life decisions' . However, specifi cally in this group, when looking for individual records, 32 patients died (86%) in the fi rst 48 hours after the admission and four (11%) had evidence of brain death and were organ donors, which leaves one patient (3%) in whom there was no ELD. Conclusion In this study, 'Comfort care' was the main ELD, which is in line with the concept that ELD are essential to ensure that care provided is Critical Care 2015, Volume 19 Suppl 1 http://ccforum.com/supplements/19/S1 S197 consistent with quality of life and death. The apparent large proportion of patients 'Without previous end-of-life decisions' was due to patients who died in the fi rst 48 hours after ICU admission corresponding to conditions refractory to treatment. Additionally, this study also draws our attention to better plan ICU admissions and hospital outreach in order to reduce early ICU mortality. 
There is a paucity of data about whether our treatment philosophy is diff erent for our patients as compared with what we would have wanted for ourselves, or while acting as surrogate decisionmakers for our loved ones. Methods An anonymous survey was sent to all the members of Australia and New Zealand Intensive Care Society and the College of Intensive Care Medicine (CICM). The fi rst section comprised a hypothetical case scenario spanning over 6 weeks of ICU stay for a patient. At four diff erent stages of the ICU stay, responders were requested to answer multiplechoice questions regarding the philosophy of treatment, based on their perceived prognosis of the patient at that particular time. The following two sections contained the same set of questions with the hypothetical scenario of responders acting as surrogate decision-makers for the patient and that of responders being patients themselves, in the same situation. The responses were compared amongst three sections at each stage using the chi-square test.
Results A total of 115 responses were received from the fellows of CICM. The results are presented in Tables 1 and 2 . Conclusion Of the ICU physicians who would withdraw care for their patient, the majority would also want the same for themselves. The disparity between decision to continue to treat the patients versus treating self or family increased with increasing length of stay. Reference Introduction To analyze the perception of parents regarding their return to the hospital where their children died to participate in a conversation with doctors and to analyze the feelings of parents about their participation in a study evaluating the care provided in the moments leading up to the death of children.
Methods A descriptive exploratory qualitative study. The study sites were the pediatric ICUs of the Hospital São Lucas and Hospital de Clinicas de Porto Alegre. Fifteen parents of children who died in the PICUs studied participated in the study. Data collection occurred in 2010 and was conducted through semistructured interviews. Data were analyzed using thematic content analysis. The research was approved by the research ethics committees of both hospitals.
The ability to return to the hospital and talk to medical assistants was considered by parents as a positive and enlightening opportunity. Parents who participated in the study understood this moment as an opportunity to be heard and demonstrated the intention to contribute with their experiences in order to improve care in the hospitals studied. Conclusion We conclude that there is a need to implement measures to provide palliative care to parents after the death of their children.
It is necessary to consider the possibility of providing families with follow-up meetings with the multidisciplinary team after the death of children.
priorities for improvement: provide families with a guide/navigator; educate providers about the fragility of family trust; improve provider communication skills; inform patients about the long-term eff ects of critical illness; and develop strategies to facilitate continuity of care between providers. Conclusion Patients and family members are an untapped resource and engaging them as researchers is a viable strategy to identify opportunities for quality improvement that are patient and family centred. Introduction The purpose of this study was to assess the visiting restrictions placed on families visiting adult patients on critical care units within trauma hospitals in England. Whilst it is well recognised that high-quality care for patients is of paramount importance, we should also be aware that supporting patients' families off ers longterm benefi ts for patient, family and hospital. In our own unit we are reviewing whether we could adopt a more fl exible attitude to visiting times and assessing how to provide a more welcoming environment to relatives. To inform our own review and in order to develop a best practise approach, we surveyed all of the major trauma centres in England.
Methods A telephone survey on visiting times was conducted in 53 adult critical care units in trauma centres in England. A list of trauma centres was obtained from the NHS England website. All critical care units (other than obstetric high dependency units and coronary care units, unless part of a cardiothoracic critical care unit) within each hospital were surveyed. Each respondent was asked about the visiting hours, whether children were allowed to visit and how many visitors to a bed space. Results Fifty-three units with between four and 75 beds and covering the whole of England were surveyed: there was a 100% response rate. Visiting hours varied between hospitals and between units within the same hospital. Nine units (17%) had open visiting hours, although most gave advice on times to avoid such as nursing handover. The majority of units (44.83%) operated restricted visiting with a median (range) of 6 (2 to 9) hours. All units allowed a maximum of two visitors to the bedspace. Children were allowed in nine units without restriction, the remaining units advised that it may not be appropriate for children to visit and it was at the discretion of the parents and medical staff . Conclusion The majority of adult critical care units in England, including our own, have restricted visiting policies. Visiting policies are a source of debate amongst staff in intensive care with concerns about open visiting including increased workload and interruptions to normal routine [1] . This is consistent with the views of staff at our own unit who, in appreciative enquiry, have expressed mixed opinions about extending visiting times. Extending visiting times is only part of a wider project to improve the way relatives experience intensive care whilst ensuring both medical and nursing staff feel supported, creating an environment for optimal communication. Following introduction of the FWR we have audited family satisfaction using the validated FS-ICU questionnaire [1] .
Methods This was a prospective study of relatives' satisfaction for patients completing their critical care episode. The questionnaire was completed anonymously and data collected. This was a pragmatic study, no changes were made to communication strategies. Results There is a high degree of satisfaction across all domains of the FS-ICU including treatment of family and provision of information ( Figure 1 ). One hundred per cent found FWR to be helpful, only 55% had anticipated this. Fifteen per cent changed their perception of critical care. It enabled 15% to raise new concerns. One hundred per cent were able have questions answered satisfactorily. Linked to the FS-ICU, we have seen marked improvements in decision-making and satisfaction.
Conclusion We have shown progressive improvement over 3 years across all domains. Marked improvement in information provision and decision-making support from 53% to 96% over 3 years since introducing the FWR correlates with the improved overall satisfaction (Figure 1 ). Interestingly FWR is more helpful than relatives anticipated. The FWR was very well received and our results suggest an unrecognised need is being met. Because this was a pragmatic study, we feel this is a true representation of family satisfaction. It is encouraging that communication, information and decision-making support continue to improve. They have become embedded in the fabric of our critical care practice and lead to marked improvement in satisfaction for families. Reference
Introduction For the families of critically ill patients, the death of a loved one in the ICU is often an unexpected and traumatic event, characterised by diffi cult decisions regarding withholding or withdrawing life-sustaining therapy. Increasingly the importance of bereavement care (BC) in the ICU is being acknowledged, although reports continue to highlight the inadequacies around end-of life care in the critical care environment. In 1998, the Intensive Care Society (ICS) published guidelines mapping out BC in the ICU [1] . We aimed to compare BC in ICUs across England against the recommendations set out by the ICS. Methods All adult ICUs in England were contacted over a 2-week period, using a standardised questionnaire based on the nine domains identifi ed by the ICS. All answers were collected anonymously using SurveyMonkey®. An 80% compliance rate was deemed acceptable. Results From the 148 ICUs identifi ed, 113 answered the questionnaire (76%). Forty-three per cent of the responders had access to training in BC and in communication skills, and 54% had a named member of staff responsible for training, writing, auditing and developing the BC policy. When asked about the presence of a written BC policy only 45% responded positively, and even less (19%) had provisions for audit and development of the service. Information to staff about cultural and religious rites around the time of death, and to relatives on what to do after a death was available in 81% and 96% respectively. The general practitioner was informed of the deaths taking place in the ICU in 77% of the cases. In more than 70% of the participating ICUs, eff orts were made to ensure privacy of the grieving relatives and to have dedicated follow-up facilities for the bereaved. Even though staff support programmes were recognised as paramount, only 54% of the ICUs had formal ones set up.
Conclusion This is the fi rst national audit of BC in the ICU since the initial ICS guideline publication. Even though most ICUs provided relatives with information around the time of death, training, auditing and adequate facilities do not meet the recommended standards. The lack of adherence is defi nitely multifactorial and requires further research.
In the meantime, vigorous implementation of these guidelines is warranted in order to ensure optimal care for the bereaved families. Reference