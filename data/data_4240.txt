Metagenomics poses opportunities for clinical and public health virology applications by offering a way to assess complete taxonomic composition of a clinical sample in an unbiased way. However, the techniques required are complicated and analysis standards have yet to develop. This, together with the wealth of different tools and workflows that have been proposed, poses a barrier for new users. We evaluated 49 published computational classification workflows for virus metagenomics in a literature review. To this end, we described the methods of existing workflows by breaking them up into five general steps and assessed their ease-of-use and validation experiments. Performance scores of previous benchmarks were summarized and correlations between methods and performance were investigated. We indicate the potential suitability of the different workflows for (1) time-constrained diagnostics, (2) surveillance and outbreak source tracing, (3) detection of remote homologies (discovery), and (4) biodiversity studies. We provide two decision trees for virologists to help select a workflow for medical or biodiversity studies, as well as directions for future developments in clinical viral metagenomics.
Unbiased sequencing of nucleic acids from environmental samples has great potential for the discovery and identification of diverse microorganisms (Tang and Chiu, 2010; Chiu, 2013; Culligan et al., 2014; Pallen, 2014) . We know this technique as metagenomics, or random, agnostic or shotgun high-throughput sequencing. In theory, metagenomics techniques enable the identification and genomic characterisation of all microorganisms present in a sample with a generic lab procedure (Wooley and Ye, 2009) . The approach has gained popularity with the introduction of next-generation sequencing (NGS) methods that provide more data in less time at a lower cost than previous sequencing techniques. While initially mainly applied to the analysis of the bacterial diversity, modifications in sample preparation protocols allowed characterisation of viral genomes as well. The fields of virus discovery and biodiversity characterisation have seized the opportunity to expand their knowledge (Cardenas and Tiedje, 2008; Tang and Chiu, 2010; Chiu, 2013; Pallen, 2014) .
There is interest among virology researchers to explore the use of metagenomics techniques, in particular as a catch-all for viruses that cannot be cultured (Yozwiak et al., 2012; Smits and Osterhaus, 2013; Byrd et al., 2014; Naccache et al., 2014; Pallen, 2014; Smits et al., 2015; Graf et al., 2016) . Metagenomics can also be used to benefit patients with uncommon disease etiologies that otherwise require multiple targeted tests to resolve (Chiu, 2013; Pallen, 2014) . However, implementation of metagenomics in the routine clinical and public health research still faces challenges, because clinical application requires standardized, validated wet-lab procedures, meeting requirements compatible with accreditation demands (Hall et al., 2015) . Another barrier is the requirement of appropriate bioinformatics analysis of the datasets generated. Here, we review computational workflows for data analysis from a user perspective.
Translating NGS outputs into clinically or biologically relevant information requires robust classification of sequence reads-the classical "what is there?" question of metagenomics. With previous sequencing methods, sequences were typically classified by NCBI BLAST (Altschul et al., 1990) against the NCBI nt database (NCBI, 2017) . With NGS, however, the analysis needs to handle much larger quantities of short (up to 300 bp) reads for which proper references are not always available and take into account possible sequencing errors made by the machine. Therefore, NGS needs specialized analysis methods. Many bioinformaticians have developed computational workflows to analyse viral metagenomes. Their publications describe a range of computer tools for taxonomic classification. Although these tools can be useful, selecting the appropriate workflow can be difficult, especially for the computationally less-experienced user (Posada-Cespedes et al., 2016; Rose et al., 2016) .
A part of the metagenomics workflows has been tested and described in review articles (Bazinet and Cummings, 2012; Garcia-Etxebarria et al., 2014; Peabody et al., 2015; Sharma et al., 2015; Lindgreen et al., 2016; Posada-Cespedes et al., 2016; Rose et al., 2016; Sangwan et al., 2016; Tangherlini et al., 2016) and on websites of projects that collect, describe, compare and test metagenomics analysis tools (Henry et al., 2014; CAMI, 2016; ELIXIR, 2016) . Some of these studies involve benchmark tests of a selection of tools, while others provide brief descriptions. Also, when a new pipeline is published the authors often compare it to its main competitors. Such tests are invaluable to assessing the performance and they help create insight into which tool is applicable to which type of study.
We present an overview and critical appraisal of available virus metagenomic classification tools and present guidelines for virologists to select a workflow suitable for their studies by (1) listing available methods, (2) describing how the methods work, (3) assessing how well these methods perform by summarizing previous benchmarks, and (4) listing for which purposes they can be used. To this end, we reviewed publications describing 49 different virus classification tools and workflows-collectively referred to as workflows-that have been published since 2010.
We searched literature in PubMed and Google Scholar on classification methods for virus metagenomics data, using the terms "virus metagenomics" and "viral metagenomics." The results were limited to publications between January 2010 and January 2017. We assessed the workflows with regard to technical characteristics: algorithms used, reference databases, and search strategy used; their user-friendliness: whether a graphical user interface is provided, whether results are visualized, approximate runtime, accepted data types, the type of computer that was used to test the software and the operating system, availability and licensing, and provision of a user manual. In addition, we extracted information that supports the validity of the workflow: tests by the developers, wet-lab experimental work and computational benchmarks, benchmark tests by other groups, whether and when the software had been updated as of 19 July 2017 and the number of citations in Google Scholar as of 28 March 2017 (Data Sheet 1; https://compare.cbs.dtu.dk/ inventory#pipeline). We listed only benchmark results from in silico tests using simulated viral sequence reads, and only sensitivity, specificity and precision, because these were most often reported (Data Sheet 2). Sensitivity is defined as reads correctly annotated as viral-on the taxonomic level chosen in that benchmark-by the pipeline as a fraction of the total number of simulated viral reads (true positives / (true positives + false negatives)). Specificity as reads correctly annotated as non-viral by the pipeline as a fraction of the total number of simulated nonviral reads (true negatives / (true negatives + false positives)). And precision as the reads correctly annotated as viral by the pipeline as a fraction of all reads annotated as viral (true positives / (true positives + false positives)). Different publications have used different taxonomic levels for classification, from kingdom to species. We used all benchmark scores for our analyses (details are in Data Sheet 2). Correlations between performance (sensitivity, specificity, precision and runtime) and methodical factors (different analysis steps, search algorithms and reference databases) were calculated and visualized with R v3.3.2 (https://www.r-project.org/), using RStudio v1.0.136 (https://www.rstudio.com).
Next, based on our inventory, we grouped workflows by compiling two decision trees to help readers select a workflow applicable to their research. We defined "time-restrained diagnostics" as being able to detect viruses and classify to genus or species in under 5 h per sample. "Surveillance and outbreak tracing" refers to the ability of more specific identification to the subspecies-level (e.g., genotype). "Discovery" refers to the ability to detect remote homologs by using a reference database that covers a wide range of viral taxa combined with a sensitive search algorithm, i.e., amino acid (protein) alignment or composition search. For "biodiversity studies" we qualified all workflows that can classify different viruses (i.e., are not focused on a single species 
We found 56 publications describing the development and testing of 49 classification workflows, of which three were unavailable for download or online use and two were only available upon request (Table 1) . Among these were 24 virusspecific workflows, while 25 were developed for broader use, such as classification of bacteria and archaea. The information of the unavailable workflows has been summarized, but they were not included in the decision trees. An overview of all publications, workflows and scoring criteria is available in Data Sheet 1 and on https://compare.cbs.dtu.dk/inventory#pipeline.
The selected metagenomics classification workflows consist of up to five different steps: pre-process, filter, assembly, search and post-process ( Figure 1A ). Only three workflows (SRSA, Isakov et al., 2011 , Exhaustive Iterative Assembly, Sch√ºrch et al., 2014 , and VIP, Li et al., 2016 incorporated all of these steps. All workflows minimally included a "search" step ( Figure 1B , Table 4 ), as this was an inclusion criterion. The order in which the steps are performed varies between workflows and in some workflows steps are performed multiple times. Workflows are often combinations of existing (open source) software, while sometimes, custom solutions are made.
A major determinant for the success of a workflow is the quality of the input reads. Thus, the first step is to assess the data quality and exclude technical errors from further analysis. This may consist of several processes, depending on the sequencing method and demands such as sensitivity and time constraints. The pre-processing may include: removing adapter sequences, trimming low quality reads to a set quality score, removing low quality reads-defined by a low mean or median Phred score assigned by the sequencing machine-removing low complexity reads (nucleotide repeats), removing short reads, deduplication, matching paired-end reads (or removing unmated reads) and removing reads that contain Ns (unresolved nucleotides). The adapters, quality, paired-end reads and accuracy of repeats depend on the sequencing technology. Quality cutoffs for removal are chosen in a trade-off between sensitivity and time constraints: removing reads may result in not finding rare viruses, while having fewer reads to process will speed up the analysis. Twenty-four workflows include a pre-processing step, applying at least one of the components listed above ( Figure 1B, Table 2 ). Other workflows require input of reads pre-processed elsewhere.
The second step is filtering of non-target, in this case nonviral, reads. Filtering theoretically speeds up subsequent database searches by reducing the number of queries, it helps reduce false positive results and prevents assembly of chimaeric virus-host sequences. However, with lenient homology cutoffs, too many reads may be identified as non-viral, resulting in loss of potential viral target reads. Choice of filtering method depends on the sample type and research goal. For example, with human clinical samples a complete human reference genome is often used, as is the case with SRSA (Isakov et al., 2011) , RINS (Bhaduri et al., 2012) , VirusHunter , MePIC (Takeuchi et al., 2014) , Ensemble Assembler (Deng et al., 2015) , ViromeScan (Rampelli et al., 2016) , and MetaShot (Fosso et al., 2017) . Depending on the sample type and expected contaminants, this can be extended to filtering rRNA, mtRNA, mRNA, bacterial or fungal sequences or non-human host genomes. More thorough filtering is displayed by PathSeq (Kostic et al., 2011) , SURPI (Naccache et al., 2014) , Clinical PathoScope (Byrd et al., 2014) , Exhaustive Iterative Assembly (Sch√ºrch et al., 2014) , VIP (Li et al., 2016) , Taxonomer , and VirusSeeker (Zhao et al., 2017) . PathSeq removes human reads in a series of filtering steps in an attempt to concentrate pathogen-derived data. Clinical PathoScope filters human genomic reads as well as human rRNA reads. Exhaustive Iterative Assembly removes reads from diverse animal species, depending on the sample, to remove non-pathogen reads for different samples. SURPI uses 29 databases to remove different non-targets. VIP includes filtering by first comparing to host and bacterial databases and then to viruses. It only removes reads that are more similar to non-viral references in an attempt to achieve high sensitivity for viruses and potentially reducing false positive results by removing non-viral reads. Taxonomer simultaneously matches reads against human, bacterial, fungal and viral references and attempts to classify all. This only works well on high-performance computing facilities that can handle many concurrent search actions on large data sets. VirusSeeker uses the complete NCBI nucleotide (nt) and non-redundant protein (nr) databases to classify all reads and then filter non-viral reads. Some workflows require a custom, user-provided database for filtering, providing more flexibility but requiring more user-input. This is seen in IMSA (Dimon et al., 2013) , VirusHunter , VirFind (Ho and Tzanetakis, 2014) , and MetLab (Norling et al., 2016) , although other workflows may accept custom references as well. In total, 22 workflows filter non-virus reads prior to further analysis ( Figure 1B, Table 3 ). Popular filter tools are read mappers such as Bowtie (Langmead, 2010; Langmead and Salzberg, 2012) and BWA (Li and Durbin, 2009) , while specialized software, such as Human Best Match Tagger (BMTagger, NCBI, 2011) or riboPicker (Schmieder, 2011) , is less commonly used ( Table 2) .
Prior to classification, the short reads may be assembled into longer contiguous sequences (contigs) and generate consensus sequences by mapping individual reads to these contigs. This helps filter out errors from individual reads, and reduce the amount of data for further analysis. This can be done by mapping reads to a reference, or through so-called de novo assembly by linking together reads based on, for instance, overlaps, frequencies and paired-end read information. In viral metagenomics approaches, de novo assembly is often the method of choice. Since viruses evolve so rapidly, suitable references are not always available. Furthermore, the short viral genomes generally result in high sequencing coverage, at least for hightitre samples, facilitating de novo assembly. However, de novo assembly is liable to generate erroneous contigs by linking together reads containing technical errors, such as sequencing (base calling) errors and remaining adapter sequences. Another source of erroneous contigs may be when reads from different organisms in the same sample are similar, resulting in the formation of chimeras. Thus, de novo assembly of correct contigs benefits from strict quality control and pre-processing, filtering and taxonomic clustering-i.e., grouping reads according to their respective taxa before assembly. Assembly improvement by taxonomic clustering is exemplified in five workflows: Metavir (Roux et al., 2011) , RINS (Bhaduri et al., 2012) , VirusFinder (Wang et al., 2013) , SURPI (in comprehensive mode) (Naccache et al., 2014) , and VIP (Li et al., 2016) . Two of the discussed workflows have multiple iterations of assembly and combine algorithms to improve overall assembly: Exhaustive Iterative Assembly (Sch√ºrch et al., 2014) and Ensemble Assembler (Deng et al., 2015) . In total, 18 of the tools incorporate an assembly step ( Figure 1B , Table 4 ). Some of the more commonly used assembly programs are Velvet (Zerbino and Birney, 2008) , Trinity (Grabherr et al., 2011) , Newbler (454 Life Sciences), and SPAdes (Bankevich et al., 2012) (Table 2) .
In the search step, sequences (either reads or contigs) are matched to a reference database. Twenty-six of the workflows we found search with the well-known BLAST algorithms BLASTn or BLASTx (Altschul et al., 1990 ; Table 2 ). Other oftenused programs are Bowtie (Langmead, 2010; Langmead and Salzberg, 2012) , BWA (Li and Durbin, 2009) , and Diamond (Buchfink et al., 2015) . These programs rely on alignments to a reference database and report matched sequences with alignment scores. Bowtie and BWA, which are also popular programs for the filtering step, align nucleotide sequences exclusively.
Diamond aligns amino acid sequences and BLAST can do either nucleotides or amino acids. As analysis time can be quite long for large datasets, algorithms have been developed to reduce this time by using alternatives to classical alignment. One approach is to match k-mers with a reference, as used in FACS (Stranneheim et al., 2010) , LMAT (Ames et al., 2013) , Kraken (Wood and Salzberg, 2014) , Taxonomer , and MetLab (Norling et al., 2016) . Exact k-mer matching is generally faster than alignment, but requires a lot of computer memory. Another approach is to use probabilistic models of multiple sequence alignments, or profile hidden Markov models (HMMs). For HMM methods, protein domains are used, which allows the detection of more remote homology between query and reference. A popular HMM search program is HMMER (Mistry et al., 2013) . ClassyFlu (Van der Auwera et al., 2014) and vFam (Skewes- Cox et al., 2014) rely exclusively on HMM searches, while VMGAP (Lorenzi et al., 2011) , Metavir (Roux et al., 2011) , VirSorter (Roux et al., 2015) , and MetLab can also use HMMER. All of these search methods are examples of similarity search-homology or alignment-based methods. The other search method is composition search, in which oligonucleotide frequencies or k-mer counts are matched to references. Composition search requires the program to be "trained" on reference data and it is not used much in viral genomics. Only two workflows discussed here use composition search: NBC (Rosen et al., 2011) and Metavir 2 (Roux et al., 2014) , while Metavir 2 only uses it complementary to similarity search (Data Sheet 1).
All search methods rely on reference databases, such as NCBI GenBank (https://www.ncbi.nlm.nih.gov/genbank/), RefSeq (https://www.ncbi.nlm.nih.gov/refseq/), or BLAST nucleotide (nt) and non-redundant protein (nr) databases (ftp://ftp.ncbi. nlm.nih.gov/blast/db/). Thirty-four workflows use GenBank for their references, most of which select only reference sequences from organisms of interest (Table 2) . GenBank has the benefits of being a large, frequently updated database with many different organisms and annotation depends largely on the data providers.
Other tools make use of virus-specific databases such as GIB-V (Hirahata et al., 2007) or ViPR (Pickett et al., 2012) , which have the advantage of better annotation and curation at the expense of the number of included sequences. Also, protein databases like Pfam (Sonnhammer et al., 1998) and UniProt (UniProt, 2015) are used, which provide a broad range of sequences. Search at the protein level may allow for the detection of more remote homology, which may improve detection of divergent viruses, but non-translated genomic regions are left unused. A last group of workflows requires the user to provide a reference database file. This enables customization of the workflow to the user's research question and requires more effort.
Classifications of the sequencing reads can be made by setting the parameters of the search algorithm beforehand to return a single annotation per sequence (cut-offs). Another option is to return multiple hits and then determine the relationship between the query sequence and a cluster of similar reference sequences. This process of finding the most likely or best supported taxonomic assignment among a set of references is called post-processing. Post-processing uses phylogenetic or other computational methods such as the lowest common ancestor (LCA) algorithm, as introduced by MEGAN (Huson et al., 2007) . Six workflows use phylogeny to place sequences in a phylogenetic tree with homologous reference sequences and thereby classify them. This is especially useful for outbreak tracing to elucidate relationships between samples. Twelve workflows use other computational methods such as the LCA taxonomy-based algorithm to make more confident but less specific classifications (Data Sheet 1). In total, 18 workflows include post-processing ( Figure 1B ).
For broader acceptance and eventual application in a clinical setting, workflows need to be user-friendly and need to be validated. Usability of the workflows varied vastly. Some provide web-services with a graphical user-interface that work fast on any PC, whereas other workflows only work on one operating system, from a command line interface with no user manual. Processing time per sample ranges from minutes to several days ( Table 3) . Although web-services with a graphical user-interface are very easy to use, such a format requires uploading large GBsized short read files to a distant server. The speed of upload and the constraint to work with one sample at a time may limit its usability. Diagnostic centers may also have concerns about the security of the data transferred, especially if patientidentifying reads and confidential metadata are included in the transfer. Validation of workflows ranged from high-i.e., tested by several groups, validated by wet-lab experiments, receiving frequent updates and used in many studies-to no evidence of validation ( NBC (125), and Rega Typing Tool (377 from two highly cited publications).
Next, we summarized workflow performance by aggregating benchmark results on simulated viral data from different publications (Figure 2) . Twenty-five workflows had been tested for sensitivity, of which 19 more than once. For some workflows, sensitivity varied between 0 and 100, while for others sensitivity was less variable or only single values were available. For 10 workflows specificities, or true negative rates, were provided. Six workflows had only single scores, all above 75%. The other four had variable specificities between 2 and 95%.
Precision, or positive predictive value was available for sixteen workflows. Seven workflows had only one recorded precision score. Overall, scores were high (>75%), except for IMSA+A (9%), Kraken (34%), NBC (49%), and vFam (3-73%).
Runtimes had been determined or estimated for 36 workflows. Comparison of these outcomes is difficult as different input data were used (for instance varying file sizes, consisting of raw reads or assembled contigs), as well as different computing systems. Thus a crude categorisation was made dividing workflows into three groups that either process a file in a timeframe of minutes ( 
For 17 workflows for which these data were available, we looked for correlations by plotting performance scores against the analysis steps included (Figure 3) . Workflows that included a pre-processing or assembly step scored higher in sensitivity, specificity and precision. Contrastingly, workflows with postprocessing on average scored lower on all measures. Pipelines that filter non-viral reads generally had a lower sensitivity and specificity and precision remained high.
Next, we visualized correlations between the used search algorithms and the runtime, and the performance scores (Figure 4 ). Different search algorithms had different performance scores on average. Similarity search methods had lower sensitivity, but higher specificity and precision than composition search. The use of nucleotide vs. amino acid search also affected performance. Amino acid sequences generally led to higher sensitivity and lower specificity and precision scores. Combining nucleotide sequences and amino acid sequences in the analysis seemed to provide the best results. Performance was generally higher for workflows that used more time.
Finally, we inventoried the overall runtime of 17 workflows ( Table 5 ) and separated them based on the inclusion of analysis steps that seemed to affect runtime. This indicated that workflows that included pre-processing, filtering, and similarity search by alignment were more time-consuming than workflows that did not use these analysis steps.
Based on the results of our inventory, decision trees were drafted to address the question of which workflow a virologist could use for medical and environmental studies (Figures 5, 6 ).
Based on available literature, 49 available virus metagenomics classification workflows were evaluated for their analysis methods and performance and guidelines are provided to select the proper workflow for particular purposes (Figures 5, 6) . Only workflows that have been tested with viral data were included, thus leaving out a number of metagenomics workflows that had been tested only on bacterial data, which may be applicable to virus classification as well. Also note that our inclusion criteria leave out most phylogenetic analysis tools, which start from contigs or classifications. The variety in methods is striking. Although each workflow is designed to provide taxonomic classification, the strategies employed to achieve this differ from simple one-step tools to analyses with five or more steps and creative combinations of algorithms. Clearly, the field has not yet adopted a standard method to facilitate comparison of classification results. Usability varied from a few remarkably user-friendly workflows with easy access online to many command-line programs, which are generally more difficult to use. Comparison of the results of the validation experiments is precarious. Every test is different and if the reader has different study goals than the writers, assessing classification performance is complex.
Due to the variable benchmark tests with different workflows, the data we looked at is inherently limited and heterogeneous. This has left confounding factors in the data, such as test data, references used, algorithms and computing platforms. These factors are the result of the intended use of the workflow, e.g., Clinical PathoScope was developed for clinical use and was not intended or validated for biodiversity studies. Also, benchmarks usually only take one type of data to simulate a particular use case. Therefore, not all benchmark scores are directly comparable and it is impossible to significantly determine correlations and draw firm conclusions.
We do highlight some general findings. For instance, when high sensitivity is required filtering steps should be minimized, as these might accidentally remove viral reads. Furthermore, the choice of search algorithms has an impact on sensitivity. High sensitivity may be required in characterization of environmental biodiversity (Tangherlini et al., 2016) and virus discovery. Additionally, for identification of novel variant viruses and virus discovery de novo assembly of genomes is beneficial. Discoveries typically are confirmed by secondary methods, thus reducing the impact in case of lower specificity. For example, RIEMS showed high sensitivity and applies de novo assembly. MetLab FIGURE 2 | Different benchmark scores of virus classification workflows. Twenty-seven different workflows (Left) have been subjected to benchmarks, by the developers (Top) or by independent groups (Bottom), measuring sensitivity (Left column), specificity (Middle column) and precision (Right column) in different numbers of tests. Numbers between brackets (n = a, b, c) indicate number of sensitivity, specificity, and precision tests, respectively.
combines de novo assembly with Kraken, which also displayed high sensitivity. When higher specificity is required, in medical settings for example, pre-processing and search methods with the appropriate references are recommended. RIEMS and MetLab are also examples of high-specificity workflows including preprocessing. Studies that require high precision benefit from pre-processing, filtering and assembly. High-precision methods are essential in variant calling analyses for the characterization of viral quasispecies diversity (Posada-Cespedes et al., 2016) , and in medical settings for preventing wrong diagnoses. RINS performs pre-processing, filtering and assembly and scored high in precision tests, while Kraken also scored well in precision and with MetLab it can be combined with filtering and assembly as needed.
Clinicians and public health policymakers would be served by taxonomic output accompanied by reliability scores, as is possible with HMM-based search methods and phylogeny with bootstrapping, for example. Reliability scores could also be based on similarity to known pathogens and contig coverage. However, classification to a higher taxonomic rank (e.g., order) is more generally reliable, but less informative than a classification at a lower rank (e.g., species) (Randle-Boggis et al., 2016). Therefore, the use of reliability scores and the associated trade-offs need to be properly addressed per application.
Besides, medical applications may be better served by a functional rather than a taxonomic annotation. For example, a clinician would probably find more use in a report FIGURE 3 | Correlations between performance scores and analysis steps. Sensitivity, specificity and precision scores (in columns) for workflows that incorporated different analysis steps (in rows). Numbers at the bottom indicate number of benchmarks performed.
of known pathogenicity markers than a report of species composition. Bacterial metagenomics analyses often include this, but it is hardly applied to virus metagenomics. Although FIGURE 4 | Correlation between performance and search algorithm and runtime. Sensitivity, specificity and precision scores (in columns) for workflows that incorporated different search algorithms, using either nucleotide sequences, amino acid sequences or both, and workflows with different runtimes (rows). Numbers at the bottom indicate number of benchmarks performed.
valuable, functional annotation further complicates the analysis (Lindgreen et al., 2016) .
Numerous challenges remain in analyzing viral metagenomes. First is the problem of sensitivity and false positive detections. Some viruses that exist in a patient may not be detected by sequencing, or viruses that are not present may be detected because of homology to other viruses, wrong annotation in databases or sample cross-contamination. These might both lead to wrong diagnoses. Second, viruses are notorious for their recombination rate and horizontal gene transfer or reassortment of genomic segments. These may be important for certain analyses and may be handled by bioinformatics software. For instance, Rega Typing Tool and QuasQ include methods for detecting recombination. Since these events usually happen within species and most classification workflows do not go deeper into the taxonomy than the species level, this is something that has to be addressed in further analysis. Therefore, recombination should not affect the results of the reviewed workflows much. Further information about the challenges of analyzing metagenomes can be found in Edwards and Rohwer (2005) (2017). An important step in the much awaited standardization in viral metagenomics (Fancello et al., 2012; Posada-Cespedes et al., 2016; Rose et al., 2016) , necessary to bring metagenomics to the clinic, is the possibility to compare and validate results between labs. This requires standardized terminology and study aims across publications, which enables medically oriented reviews that assess suitability for diagnostics and outbreak source tracing. Examples of such application-focused reviews can be found in the environmental biodiversity studies (Oulas et al., 2015; Posada-Cespedes et al., 2016; Tangherlini et al., 2016) . Reviews then FIGURE 5 | Decision tree for selecting a virus metagenomics classification workflow for medical applications. Workflows are suitable for medical purposes when they can detect pathogenic viruses by classifying sequences to a genus level or further (e.g., species, genotype), or when they detect integration sites. Forty workflows matched these criteria. Workflows can be applied to surveillance or outbreak tracing studies when very specific classification are made, i.e., genotypes, strains or lineages. A 1-day analysis corresponds to being able to analyse a sample within 5 h. Detection of novel variants is made possible by sensitive search methods, amino acid alignment or composition search, and a broad reference database of potential hits. Numbers indicate the number of workflows available on the corresponding branch of the tree. FIGURE 6 | Decision tree for selecting a virus metagenomics classification workflow for biodiversity studies. Workflows for the characterisation of biodiversity of viruses have to classify a range of different viruses, i.e., have multiple reference taxa in the database. Forty-three workflows fitted this requirement. Novel variants can potentially be detected by using more sensitive search methods, amino acid alignment and composition search, and using diverse reference sequences. Finally, workflows are grouped by the taxonomic groups they can classify. Numbers indicate the number of workflows available on the corresponding branch of the tree.
provide directions for establishing best practices by pointing out which algorithms perform best in reproducible tests. For proper comparison, metadata such as sample preparation method and sequencing technology should always be included-and ideally standardized. Besides, true and false positive and negative results of synthetic tests have to be provided to compare between benchmarks.
Optimal strategies for particular goals should then be integrated in a user-friendly and flexible software framework that enables easy analysis and continuous benchmarking to evaluate current and new methods. The evaluation should include complete workflow comparisons and comparisons of individual analysis steps. For example, benchmarks should be done to assess the addition of a de novo assembly step to the workflow and measure the change in sensitivity, specificity, etc. Additionally, it remains interesting to know which assembler works best for specific use cases as has been tested by several groups (Treangen et al., 2013; Scholz et al., 2014; Smits et al., 2014; V√°zquez-Castellanos et al., 2014; Deng et al., 2015) . The flexible framework should then facilitate easy swapping of these steps, so that users can always use the best possible workflow. Finally, it is important to keep reference databases up-to-date by sharing new classified sequences, for instance by uploading to GenBank.
All these steps toward standardization benefit from implementation of a common way to report results, or minimum set of metadata, such as the MIxS by the genomic standard consortium (Yilmaz et al., 2011) . Currently several projects exist that aim to advance the field to wider acceptance by validating methods and sharing information, e.g., the CAMI challenge (http://cami-challenge.org/), OMICtools (Henry et al., 2014) , and COMPARE (http:// www.compare-europe.eu/). We anticipate steady development and validation of genomics techniques to enable clinical application and international collaborations in the near future.
AK and MK conceived the study. SN designed the experiments and carried out the research. AK, DS, and HV contributed to the design of the analyses. SN prepared the draft manuscript. All authors were involved in discussions on the manuscript and revision and have agreed to the final content.
This work was supported by funding from the European Community's Horizon 2020 research and innovation programme under the VIROGENESIS project, grant agreement No 634650, and COMPARE, grant agreement No 643476.