Introduction Autophagy is well known as one of the biogenic responses against various stresses, which possesses the benefi cial roles for survival, but little is known about the dynamics and its signifi cance during the septic condition. We hypothesized that autophagy is induced during the septic condition, and contributes to protect from tissue damage which subsequently leads to organ dysfunction. We confi rm whether the autophagic process is accelerated or sustained in an acute phase of sepsis and we also determine its physiological role. Methods Sepsis was induced by cecal ligation and puncture (CLP) in mice. We examined the kinetics of autophagosome and auto lysosome formation which may explain the status of autophagy by western blotting, immunohistochemistry, and electron microscopy. To investigate a precise role of autophagy in CLP-induced sepsis, chloroquine, an autophagy inhibitor, was administered to the CLP-operated mice, and blood chemistry, pathology of the liver and survival were evaluated. Results Autophagy demonstrated by the ratio of LC3-II/LC3-I was induced over the time course up to 24 hours after CLP. The ratio was particularly increased in the liver, heart and spleen. Autophagosome formation became maximal at 6 hours and declined by 24 hours after CLP. Autolysosome formation as evaluated by both fusion of GFP-LC3 dots with LAMP1 immunohistochemistry and electron microscopy was also increased after the procedure. Furthermore, inhibition of autophagy by chloroquine during the CLP procedure resulted in elevation of serum AST levels, and signifi cantly increased mortality in mice. Conclusion Autophagy was induced in several organs over the time course of the CLP sepsis model and then the process was gradually completed to degradation of the components. Our data suggest autophagy plays a protective role in organ dysfunction in sepsis.
Introduction Acute myocardial depression in septic shock is common [1] . Myocardial depression is mediated by circulating depressant substances, which until now have been incompletely characterized [2] .
The aim of our study was to observe the eff ects of TNFα on the model of perfused rat heart. Methods After profound anesthesia with pentothal, the Wistar rats were killed by exsanguination. After sternotomy, the heart was taken and connected to the Langendorf column. The apex of the heart was hooked to a strength sensor. Biopac student laboratory software was used to record and analyse heart contractions. Contractions were recorded every 5 minutes during periods of 20 minutes. Control measurements were fi rst recorded. We measured four parameters: heart rate, contraction force, speeds of contraction and relaxation for control, during TNFα (20 ng/ml) exposure and after removal of TNFα. We express the variations of parameters as percentage of the control ± SEM. A paired t test was used to compare heart rate, contraction amplitude, speeds of contraction and relaxation with TNFα and control measurements and after removal of TNFα. Results Eight rat hearts Wistar (weight = 325 ± 23 g) were studied. See Table 1 . 
Heart rate 78 ± 6* 91 ± 5
Introduction Traditional whole blood experiments suggest that sepsis causes abnormal red blood cell (RBC) deformability. To investigate this at the cellular level, we employed a novel biophysical method to observe individual RBC membrane mechanics in patients with septic shock. Methods We collected blood samples from patients with septic shock until either death or day 5 of admission. Thermal fl uctuations of individual RBCs were recorded allowing a complete analysis of RBC shape variation over time. Mean elasticity of the cell membrane was then quantifi ed for each sample collected.
We recruited nine patients with septic shock. Table 1 shows mean RBC thermal fl uctuation and SOFA scores. Conclusion RBC thermal fl uctuation analysis allows variations in RBC elasticity during sepsis to be quantifi ed at a cellular level. We could not identify any specifi c trend between sepsis severity and erythrocyte elasticity. Cells demonstrated both increases and decreases in fl uctuation independent of SOFA score. This is contrary to current evidence that suggests RBC deformability is reduced during sepsis. Reference
Introduction Whole blood experiments suggest that cardiopulmonary bypass (CPB) causes red blood cell (RBC) trauma and changes in deformability that may contribute to postoperative microcirculatory Introduction Neutrophil gelatinase-associated lipocalin (NGAL)/ lipocalin2, known as a sensitive biomarker of acute kidney injury, prevents bacterial iron uptake, resulting in the inhibition of its overgrowth [1] . We previously demonstrated that this protein was discharged into gut lumen from crypt cells in septic conditions, and inhibited the growth of Escherichia coli [2] . However, it remains unclear which pathway is associated with the upregulation of NGAL. We therefore designed the present study to reveal whether the patternrecognition receptor of bacteria, the Toll-like receptor (TLR) family, plays a pivotal role for NGAL secretion from gut crypt cells. Methods With our institutional approval, the ileum and colon of male C57BL/6J mice (6 to 7 weeks) were everted and washed by Ca 2+ and Mg 2+ free PBS buff er fi ve times. Tissues were incubated with Ca 2+ and Mg 2+ free PBS containing 30 mM EDTA for 1 hour to isolate crypt cells of gut. The cell suspension was fi ltered through a cell strainer (40 μm) twice, and deposited the crypt cells by centrifugation at 700×g. The isolated crypt cells were resuspended in PBS and stained with 0.25% amido black for labeling paneth cells. The 5×10 5 crypt cells were resuspended in 50 ml HBSS containing 2.5% fetal bovine serum and 1% penicillin-streptomycin. The crypt cells were incubated at 37°C with or without TLR ligands: lipopolysaccharide (TLR4 ligand, 10 μg/ml) and CpG-DNA (TLR9 ligand, 8 μg/ml). After a 2-hour incubation period, the crypt cells were deposited and eluted mRNA to measure the expression of both NGAL and TLR mRNA using real-time PCR. Results More than 70 to 80% of collected cells were stained by amido black. LPS signifi cantly upregulated the expression of NGAL and TLR4 mRNA in ileum and colon crypt cells (P <0.05). Although the CpG-DNA did not upregulate NGAL and TLR9 mRNA in ileum crypt cells, the apparent expression of NGAL and TLR9 mRNA was found in colon crypt cells (P <0.05). Conclusion Bacterial stimulation of TLR4 and TLR9 pathways plays a pivotal role in the expression of NGAL mRNA in gut, suggesting that NGAL, derived from gut crypt cells, could contribute to the regulation of the intraluminal microfl ora in the critically ill. References
Introduction Most individuals infected with the 2009 pandemic H1N1 infl uenza A virus (IAV) (H1N1pdm) experienced uncomplicated fl u. However, in a small subset of patients the infection rapidly progressed to primary viral pneumonia (PVP) and a minority of them developed ARDS. Inherited and acquired variability in host immune responses may infl uence susceptibility and outcome of IAV infection. However, the molecular nature of such human factors remains largely elusive. Methods We report three adult relatives with the autosomal dominant GATA-2 defi ciency. P1 and his son P2 had a history of myelodysplastic syndrome and a few episodes of mild respiratory infections. They developed PVP by H1N1pdm which rapidly evolved to ARDS. They died at the age of 54 and 31, respectively. Results Patients were heterozygous for a novel R396L mutation in GATA2. Like other patients with GATA-2 defi ciency, the three relatives had absence of peripheral NK and B cells and monocytopenia. However a high number of plasma cells, which were found to be pauciclonal, were observed in peripheral blood from P1 during H1N1pdm infection. P1 and P2 had normal levels of immunoglobulins and IgG antibodies against common viruses. Microneutralization test showed that P1 produced normal titers of neutralizing antibodies against H1N1pdm and against the previous annual H1N1 strain. Our results suggest that a few clones of long-living memory B cells against IAV expanded in P1; and that these cells produced cross-reactive antibodies against H1N1pdm, similar to those recently described. During the fl u episode P1 had a strong increase of IFNγ-producing T cells and of IFNγ production. The Th1-related chemokines CXCL10 and CXCL9, as well as IFNγ, MCP-1 and IL-8, were strongly elevated in serum from P1 and P2 in the course of H1N1pdm infection. Conclusion GATA-2 defi ciency is the fi rst described Mendelian inborn error of immunity underlying severe IAV infection. Primary immunodefi ciencies predisposing to severe IAV infections may debut, even in adults without a history of previous severe infections. The massive IFNγ-mediated cytokine storm may explain the fatal course of H1N1pdm infection in our patients.
Introduction Adenosine exerts anti-infl ammatory and tissue protective eff ects during systemic infl ammation. While the anti-infl ammatory properties may induce immunoparalysis and impede bacterial clearance, the tissue protective eff ects might limit organ damage. The eff ects of a common loss-of-function variant of the adenosine monophosphate deaminase 1 gene (AMPD1), which is associated with increased adenosine formation, in patients with sepsis are unknown. Methods In a prospective cohort, genetic-association study, the eff ects of the presence of the AMPD1 gene on immune function, multiorgan dysfunction and mortality in septic patients was studied. Pneumosepsis patients (n = 402) and controls without infection (n = 101) were enrolled. Results In pneumosepsis patients and controls, a similar prevalence of the 34C>T (rs17602729) mutation in the AMPD1 gene was found. Univariate logistic regression analysis showed a tendency of increased mortality in patients with the CT genotype, compared with patients with the CC genotype (OR 1.53; 95% CI 0.95 to 2.5). Moreover, carriers of the CT genotype tended to suff er more from multiorgan dysfunction, OR 1.4 (0.84 to 2.3) and 3.0 (0.66 to 13.8), for CT and TT, respectively (P = 0.07). In septic carriers of the CT genotype, the ex vivo production of TNFα by LPS-stimulated monocytes was attenuated (P = 0.005), Introduction Hypogammaglobulinemia has been frequently found in adult patients with severe sepsis and septic shock. Furthermore, it seems that at least a low serum level of IgM is correlated with higher mortality in sepsis. The mechanisms of hypogammaglobulinemia in septic shock have not yet been explained. It has been hypothesized that outfl ow of immunoglobulins into the extravascular space due to increased capillary permeability could reduce immunoglobulin serum concentrations. Angiopoietin-2, which directly disrupts the endothelial barrier, is markedly elevated in sepsis and other infl ammatory states and its serum level has been correlated with microvascular leakage, end-organ dysfunction and death in sepsis. Methods In the prospective, noninterventional study, we assessed the correlation between the capillary leakage marker angiopoetin-2 and serum levels of IgG and IgM in 41 patients with community-acquired severe sepsis or septic shock on admission. Blood samples were obtained during the fi rst 12 hours after admission to hospital. Results Mean age of patients (17 females) was 70 years. Median APACHE II and SOFA scores at admission were 24 and 11, respectively. The mortality rate was 45%. Thirty-four percent of all patients had level of IgG <650 mg/dl. The median concentration of angiopoietin-2 in the hypo-IgG group was 11,958 pg/ml, which was not statistically diff erent (Mann-Whitney; P >0.05) than in the rest of patients with normal levels of IgG (15,688 pg/ml). The concentration of IgM <40 mg/dl was found in only four patients (10%) and all died. Pearson's correlation test showed that the correlation between the concentrations of angiopoietin-2 and IgG (correlation coeffi cient 0.191) or IgM (correlation coeffi cient 0.0408), respectively, were not statistically signifi cant (P <0.05). Conclusion At present the hypothesis that increased microvascular leakage is responsible for hypogammaglobulinemia in septic patients could not be accepted. Studies on larger number of patients are needed. In addition, it is necessary to further explore other possible mechanisms, such as increased catabolism and consumption of antibodies or inadequate synthesis of immunoglobulins, which could also be responsible for hypogammaglobulinemia in sepsis.
Introduction Septic encephalopathy is a frequent complication in severe sepsis but its pathogenesis and mechanisms are not fully understood. Oxygen supply and utilization are critical for organ function, especially for the brain, a tissue extremely dependent on oxygen and glucose. Disturbances in oxygen utilization are common in sepsis and a number of mitochondrial dysfunctions have been described in diff erent tissues in septic animals as well as in septic patients. Our group described mitochondrial dysfunctions in the brain during experimental sepsis.
Methods Experimental sepsis was induced by endotoxemia (LPS 10 mg/ kg i.p.) in Sprague-Dawley rats and by polymicrobial fecal peritonitis in Swiss mice. Brain glucose uptake was observed in vivo in endotoxemic rats using positron emission tomography with [ 18 F]fl uorodeoxyglucose and autoradiography with 2-deoxy-14 C-glucose.
Results Mice with polymicrobial sepsis present hypoglycemia, hyperlactatemia and long-term cognitive impairment. We observed a rapid increase in the uptake of fl uorescent glucose analog 2-deoxy-2-((7-nitro-2,1,3-benzoxadiazol-4-yl)amino)-D-glucose in brain slices from septic mice in vitro. A similar increase in brain glucose uptake was observed in vivo in endotoxemic rats. Remarkably, the increase in glucose uptake started 2 hours after LPS injection, earlier than other organs. The brains of mice with experimental sepsis presented neuroinfl ammation, mitochondrial dysfunctions and oxidative stress, but mitochondria isolated from septic brains generated less ROS in vitro in the fi rst 24 hours. This led us to investigate the role of NADPH oxidase, an enzyme induced during innate immune response, as a potential source of reactive oxygen species in experimental sepsis. Inhibiting NADPH oxidase with apocynin acutely after sepsis prevented cognitive impairment in mice.
Our data indicate that a bioenergetic imbalance and oxidative stress is associated with the pathophysiology of septic encephalopathy. We are observing a new metabolic phenotype in the brain during sepsis, characterized by a rapid increase in glucose uptake and mitochondrial dysfunctions that may be secondary to infl ammation and hypoxia.
Introduction Pathophysiology of brain dysfunction associated with sepsis is still poorly understood. Potential mechanisms involve oxidative stress, neuroinfl ammation and blood-brain barrier alterations. Our purpose was to study the metabolic alterations and markers of mitochondrial dysfunction in a clinically relevant model of septic shock. Methods Twelve anesthetized (midazolam/fentanyl/pancuronium), invasively monitored, and mechanically ventilated pigs were allocated to a sham procedure (n = 5) or sepsis (n = 7), in which peritonitis was induced by intra-abdominal injection of autologous feces. Animals were studied until spontaneous death or for a maximum of 24 hours. In addition to global hemodynamic and laboratory assessment, intracranial pressure and cerebral microdialysis were assessed at baseline, 6, 12, 18 and 24 hours after sepsis induction. After death, brains were removed and brain homogenates were studied to assess markers of mitochondrial dysfunction.
Introduction Identifying a group of patients at high risk of developing infectious complications is the fi rst step in the introduction of eff ective pre-emptive therapies in specifi c patient groups. Quantifying cytokine gene expression also furthers our understanding of trauma-induced immunosuppression. Our group has already demonstrated that a predictive immunological signature derived from mRNA expression in elective thoracic surgical patients accurately predicts pneumonia risk [1] . Methods In total, 121 ventilated polytrauma patients were recruited. mRNA was extracted from PaxGene tubes collected within 2 hours of the initial insult, at 24 and 72 hours. T-helper cell subtype specifi c cytokines and transcription factors mRNA was quantifi ed using qPCR. Ten healthy controls served as a comparator. Results The Median Injury Severity Score (ISS) was 29. Time 0 bloods demonstrated a reduction in TNFα † , IL-12 § , IL-23 ‡ , RORγT* and T bet § , and an increase in IL-10* and IL-4 † mRNA levels in comparison with the control group (*P <0.0001, † P <0.001 to 0.0001, ‡ P <0.01 to 0.001, § P <0.05 to 0.01).
There was a positive correlation between ISS and IL-10 ‡ whilst both IL-23 § Introduction Measurement of biomarkers is a potential approach to early assessment and prediction of mortality in septic patients. The purpose of this study was to ascertain the prognostic value of proadrenomedullin (pADM), measured in all patients admitted to the ICU of our hospital with a diagnosis of severe sepsis or septic shock during 1 year.
Methods A cohort study of 117 patients >18 years with severe sepsis according to the Surviving Sepsis Campaign, in an ICU of a university hospital. Demographic, clinical parameters and pADM, C-reactive protein and procalcitonin were studied during 1 year. Descriptive and comparative statistical analysis was performed using the statistical software packages Statistica Stat Soft Inc 7.1 and MedCalc 9.2.1.0. Results We analyzed 117 consecutive episodes of severe sepsis (15%) or septic shock (85%) in the ICU. The median age of the patients was Introduction Sepsis results from complex interactions between infecting microorganisms and host responses, often leading to multiple organ failures and death. Over the years, its treatment has been standardized in early goal-oriented therapies, which may benefi t from circulating biomarkers for early risk stratifi cation. We aimed to evaluate the prognostic value of presepsin (sCD14-ST), a novel marker of bacterial infection. Methods We performed a nested case-control study from the randomized controlled Albumin Italian Outcome Sepsis (ALBIOS) trial, enrolling patients with severe sepsis or septic shock from 100 ICUs in Italy. Fifty survivors and 50 nonsurvivors at ICU discharge were selected, matched for age, sex, center and time of enrollment after inclusion criteria were present. EDTA-plasma samples were collected at days 1, 2 and 7 after enrolment for presepsin (immunechemiluminescence assay PATHFAST Presepsin, URL 320 pg/ml, CV 5%; Mitsubishi Chemicals) and procalcitonin assay (PCT, Elecsys BRAHMS Cobas® PCT, URL 0.046 ng/ml, CV 8.8%; Roche Diagnostics).
Results Clinical characteristics were similar between the two groups, except for a worse SOFA score at day 1 in decedents. Presepsin at day 1 was signifi cantly higher in decedents (2, 268 (1,145 to 4,305) pg/ ml, median (Q1 to Q3)) than in survivors (1,184 (855 to 2,158) pg/ml, P = 0.001), while PCT did not diff er (18.5 (3.3 to 45.7) vs. 10.8 (2.6 to 46.4) ng/ml, P = 0.31). Presepsin decreased over time in survivors, but remained elevated in decedents (974 (674 to 1,927) vs. 2,551 (1,438 to 5,624) pg/ml at day 7, P = 0.02 for time-survival interaction); PCT decreased similarly in the two groups (P = 0.19). Patients with early elevated presepsin had worse SOFA score, higher number of MOFs, hemodynamic instability (lower mean arterial pressure at baseline and after 6 hours), and mortality rate at 90 days (75% vs. 42%, logrank P <0.001). The association between presepsin and outcome was more marked in patients with late enrollment (6 to 24 hours), and in septic shock. Early presepsin had better prognostic accuracy than PCT (AUROC 0.69 vs. 0.56, P = 0.07), and improved discrimination over SOFA score, especially in septic shock. Conclusion Early presepsin measurements may provide important prognostic information in patients with severe sepsis or septic shock, and may be of crucial importance for early risk stratifi cation.
Introduction Infections are a major complication during the postoperative period after heart transplantation (HT). In our hospital, nosocomial pneumonia is the most frequent infection in this period. The objective of this study is to determine the epidemiological and microbiological characteristics of this disease in our centre. Methods A descriptive retrospective study of all medical records of HT performed in a single institution from 1991 to 2009 followed until June 2010. Clinical and microbiological variables were considered. Centre for Diseases Control (CDC) criteria were used to defi ne nosocomial infections. Invasive aspergillosis was considered if there were criteria for probable aspergillosis according to IDSA criteria. Results In 594 HTs there were 97 infectious episodes in 75 patients (12.6%). Eighty-fi ve patients (14.3%) died during hospitalization. Infection is the second cause of mortality during the postoperative period (17.9% of dead patients). The most common locations of infections were pneumonia (n = 31, 31.9% of infection episodes), bloodstream (n = 24, 24.7%), urinary tract (n = 14, 14.4%), surgical site (n = 13, 13.4%) and intraabdominal infections (n = 13, 13.4%). Patients with pneumonia were treated according to knowledge in a specifi c moment, thus diff erent antibiotics were used. The duration of antibiotic therapy was 20 ± 15.5 days. In nine episodes of pneumonia according to the CDC no germ was isolated in the cultures. Six of the episodes were polymicrobial infections. The most frequent microbes isolated were E. coli (n = 7, 22.5% of pneumonia cases), A. fumigatus (n = 7, 22.5%), S. aureus (n = 3, 9.68%), P. aeruginosa (n = 3, 9.68%), P. mirabilis, K. pneumoniae, E. cloacae, E. faecalis, C. glabrata, and S. marcescens (one case each, 3.22%). Pneumonia was suspected but not confi rmed in 75 patients. Despite this, antibiotic treatment was maintained for a media of 17.35 ± 7.01 days: 56 wide-spectrum treatments and 18 targeted therapy after knowing the antibiogram. The length of ICU stay was 38.4 ± 70.8 (3 to 264) days, of hospital stay was 66.2 ± 80.5 (3 to 304) days and of mechanical ventilation was 27.3 ± 50.2 (3 to 264) days. The mortality of patients with pneumonia was 32.3%. Conclusion Nosocomial pneumonia is the most frequent infection in our series. Despite when infection was not confi rmed, antibiotic therapy was maintained in suspect cases. We found a high incidence of aspergillosis. Limitations because of wide duration of this study should be considered.
that numbers of CVC, intubation and surgery, the use of muscle relaxant and steroid were independent risk factors for developing VAP. Ventilator days and ICU length of stay were longer in the VAP group (25 vs. 6 and 25 vs. 7 days, respectively). Lastly, the hospital mortality rate was signifi cantly higher in the VAP group (33% vs. 12%, P = 0.008). Conclusion The incidence of VAP was 9.2% in the SICU of Siriraj Hospital, which was comparable with previous reports. Bundles of care to prevent VAP should include weaning from a ventilator. Muscle relaxant and steroid should be administered according to strong indication.
Meticulous care of the airway should be implemented as protocol in order to prevent complications that can result in the development of VAP. Reference Introduction This is a 1-year prospective study to determine the incidence, source and etiology of hospital-acquired bloodstream infection (HABSI) in the Indian context. The resistance pattern was also reviewed.
Methods A single-centre prospective study in a 35-bed ICU. HABSI was defi ned according to current CDC guidelines. HCAP, catheterassociated UTI (CAUTI) and skin-related infections causing BSI was also defi ned according to recent guidelines and analysed. Results Out of 332 positive samples, 90 samples (n = 45) were HABSI. The microbiological analysis showed 60% were Gram-negative, 6% were candida and 27% were Gram-positive. The commonest isolate was klebsiella and MRSA was commonest in Gram-positive. The source of HABSI showed CRBSI was the commonest cause at 69%, which correlates with international data. Ventilator-associated pneumonia and CAUTI caused 9.5% BSI respectively. The resistance pattern among Gram-negative bacteria showed multidrug-resistant (MDR) and extreme drug-resistant (XDR) isolates were highest. See Tables 1 and 2 . Introduction Catheter-related bloodstream infection (CRBSI) is a complication of central venous catheters (CVCs) with an attributable morbidity, mortality and cost [1] . We examined patient risk factors for CRBSI in an adult parenteral nutrition (PN) population.
The study was carried out in a 525-bed tertiary-referral teaching hospital over a 14-year study period (1997 to 2010). All inpatients referred for PN via CVCs were included. Prospectively collected data were recorded in a specifi c PN record. The CRBSI audit group met quarterly to review all sepsis episodes, assigning a diagnostic category (CRBSI or non-CRBSI). Patient risk factors for development of CRBSI were examined using a logistic regression model to take account of the dichotomous nature of the outcome. Odds ratios from a model incorporating demographic and clinical data were tested for statistical signifi cance. Introduction Many patients develop infections following operations.
Decreased immune competence has been demonstrated in acute neurological conditions. A strong cytokine-mediated antiinfl ammatory response was observed in stroke patients at infection, although infection due to the decreased proinfl ammatory mediators can be expected as well. To investigate this question the following experiment was performed.
Methods Twenty-two urinary bladder cancer patients with radical cystectomy and lymphadenectomy were studied. Blood samples were taken on day 0 (before) and days 1, 3, 6, 9 and 14 after operation as well as on days 30, 60, 90 and 270 during follow-up. TNFα, soluble TNFα receptor I and IL-6 levels in sera were determined by HS ELISA and/or ELISA. Plasma ACTH and cortisol values were measured by RIA kits. Results From 22 patients, eight deep wound and urine infections were found in 14 days and six urine and wound infections in 30 days after surgery, all survived. All patients were bacterially contaminated, as wound samples taken at the end of operation demonstrated. On day 0 the circulating TNFα values were lower in infected patients. TNF started to increase from day 3 to day 9, never reaching values of the uneventful healing group. Soluble TNF receptor I, IL-6, ACTH, and cortisol concentrations did not demonstrate any diff erence on day 0 but from day 1 started to increase transiently, reaching higher levels in septic patients. Conclusion A low proinfl ammatory response is a key facilitating factor for the development of infection. Measuring serum TNFα levels before and after operations can thus predict the outcome. evaluation during 3 days in May 2012 including direct observation of hand hygiene compliance by control nurses and hand cultures of 50 healthcare workers (HCW). Based on the WHO Guidelines on Hand Hygiene in Health Care [1] , cleaning of hands with alcohol-based hand rubs (Sterillium) was prescribed before touching a patient and before aseptic procedures, after body fl uid exposure risk and after touching a patient and touching his/her surroundings. Promotion of the hand hygiene program consisted of lectures and web-based self-learning, posters located near points of care and verbal reminders by control nurses. New observations of hand hygiene by control nurses during 3 days and hand cultures of 50 healthcare providers were performed in September 2012. Consumption of alcohol-based hand rub (product volume use per patient-days) was used as a surrogate marker of hand hygiene over time. The diff erence in hand hygiene compliance during the two periods was examined using a chi-squared test. Diff erences in hand cultures were examined using a Student's t test. Time trends in the consumption of alcohol-based hand rub were examined using linear correlation. P <0.05 was considered statistically signifi cant. The study was approved by the institutional Ethics Review Board.
Results During the survey, in May 158 opportunities to observe hand hygiene were presented and 286 in September. Overall compliance improved from 34.2% (54/158) to 51% (146/286), χ 2 = 11.7 (P <0.001). In May, 50 HCW had a mean of 63.20 ± 39.37 colony-forming units (CFU) on their hands compared with 43.0 ± 40.19 CFU on the hands of 50 HCW in September (P = 0.024). We also observed an initial increased use of alcohol-based hand rubs from 21 ml per patient-day in May to a maximum 72 ml per patient-day in June, but a decline to 44 ml per patient-day in September, Pearson correlation coeffi cient = 0.31 (P = 0.61). Conclusion Implementation of a new hand hygiene program at our ICU resulted in improved hand hygiene compliance and less CFU on the hands of HCW. There was no signifi cant increased use of alcohol-based hand rubs over time. The results indicate that constant awareness is vital for success. Reference
Introduction ICU-acquired infection is directly related to hospital mortality. Hand hygiene is an eff ective, low-cost intervention that can prevent the spread of bacterial pathogens, including multidrugresistant organisms. Historical compliance with hand hygiene guidelines by physicians, nurses and other care providers is poor. Methods Present expectations by the Infection Control Committee are to 'pump in, pump out' of every room, using 63% isopropyl alcohol. We performed 17,622 observations of hand hygiene in the surgical ICU from March through October 2012, and intervened to change behavior by providing monthly feedback to specifi c provider groups and services. We made use of the Unit Coordinator to measure compliance of all individuals in the ICU. Results Overall compliance by physicians was 82.1%, for nonphysicians was 84.8%. Feedback to physicians, individually and by service, dramatically increased hand hygiene compliance, defi ned as both on entry and exit from the patient room, over the study period. See Figure 1 .
Conclusion Physician behavior is responsive to monthly feedback that is specifi c to the individual or surgical service. Use of the Unit Coordinator was very eff ective at gathering a very large sample size in a short period of time.
Introduction The Benefi ts of Universal Glove and Gowning (BUGG) study is a cluster-randomized trial to evaluate the use of wearing gloves and gowns for all patient contact in the ICU. The primary outcome is VRE and MRSA acquisitions; secondary outcomes include frequency of healthcare worker visits, infection rates, hand hygiene compliance and adverse events.
Methods We enrolled 20 ICUs in 15 states. ICUs collected nasal and perianal swabs on all patients at admission and discharge/transfer. After a 3-month baseline period, 10 units were randomized to the intervention arm and required to wear gloves and gowns for all patient contact. An intervention toolkit was created based on site feedback and compliance reports. Swab collection compliance was fed back and discussed during site conference calls on a weekly basis. Site coordinators monitored compliance with gloves and gowns, hand hygiene and frequency of HCW visits and reviewed patient charts for adverse events.
Results During the 12-month study period, 100,210 swabs were collected. After the baseline period, we were able to achieve and maintain swab compliance rates between 85 and 97%. Monthly discharge compliance increased by 21% by the beginning of the intervention period ( Figure 1 ). Observers found 86% compliance with universal glove and gowning over 1,242 30-minute observation periods ( Figure 1 ). Ninety charts at each site were reviewed for adverse events. Conclusion Over a diverse group of US hospitals, we achieved high compliance with surveillance cultures and implementing universal gloving and gowning was achieved quickly with high compliance. Introduction Sepsis accounts for a very high mortality. The Surviving Sepsis Campaign recommends a fi rst 6 hours resuscitative bundle to improve patient outcome. Despite this, the bundle is poorly performed because of several organizational and cultural barriers. In recognition of this, we guess that an Educational and Organizational Intervention out of the ICUs could impact on septic patient outcome. In order to test our hypothesis we carried out, in 12 hospitals, a pre-intervention survey of the human and organizational resources (HOR) available in the management of septic patients. The aim is to seek any barrier potentially aff ecting correct Guidelines implementation. Methods Thirty-nine medical wards (MW) and 12 emergency departments (ED) were enrolled. Every unit was asked to fi ll in a pre-agreed HOR Checklist focused on the main requirements suggested by the Guidelines.
Results Analysing the human resources available, we see that the bedto-doctor ratio signifi cantly (P <0.01) increases from the day to the night shift: from 6 to 43 beds per doctor on the MW (median). Otherwise, the ED staff remains roughly the same: from 3.5 to 2.5 doctors on duty (median). The analysis of the organizational tools (Table 1 ) points out a low percentage of hospitals having: a Diagnostic and Therapeutic Protocol for sepsis management (8.3%), some Hospital Empirical Antibiotic Therapy Guidelines (0%) and an Infective Source Eradication Protocol (8.3%). Moreover, just 25% of hospitals involve an infectious diseases expert in every case of severe sepsis or septic shock. Conclusion We guess that the poor availability of HOR showed by the hospitals could have a role in the Guidelines implementation and in the patient's outcome. Only a comparison between these results and data collected from a Clinical Checklist, focused on sepsis bundle compliance, and from a patient's outcome summary could confi rm our hypothesis. This is the aim for our next part of the study. Reference
Introduction The incidence of patients carrying ESBL-positive bacteria in our ICU (12 in 780 admissions in 2011) was not considered problematic. However, routine cultures had identifi ed ESBL-negative patients who had become colonized with ESBL strains during their ICU stay. Self-disinfecting siphons, preventing bacterial growth by antibacterial coating and intermittent heating, and biofi lm formation by electromechanical vibration, were placed in all sinks in the ICU. The aim of the present study was to evaluate the eff ect of this intervention. Methods An intervention study in a 12-bed ICU. The intervention involved placement of 19 self-disinfecting siphons (Biorec). All patients with an expected ICU stay of 2 days or more between January 2011 and December 2012 were studied. Samples of throat, sputum and rectum were taken at admission and twice weekly, and cultured for ESBLs. Between June 2011 and October 2011, sinks in patient rooms were cultured regularly for ESBLs. After the intervention in April 2012, multiple repeat cultures were taken. Whenever the species and antibiogram of bacteria cultured from patients and sinks matched, they were typed by AFLP.
Results Before intervention Multiple ESBL-forming strains were found in sinks of all patient rooms. Eighteen patients who were ESBL-negative on ICU admission became colonized with 11 diff erent ESBL strains, that were present in sinks of their admission rooms ( Figure 1 ). Four contaminations were proven by AFLP-tying. One patient died of ESBLpositive E. cloacae pneumonia. After intervention All sinks were negative for ESBL strains. No further patients became ESBL colonized during the ICU stay. Conclusion Wastewater sinks were the likely source of ESBL colonization for 18 ICU patients. After placing self-disinfecting siphons Introduction The present study investigated the eff ects of a single dose of intraperitoneal (i.p.) IgG and IgGAM administration on various behavioral alterations in a cecal ligation perforation (CLP)-induced sepsis model in rats. Methods Female Wistar albino rats (200 to 250 g) were divided into fi ve groups (n = 8): a naive Control group, a Sham operated group receiving conventional antibiotic treatment, a CLP group receiving CLP procedure and conventional antibiotic treatment, and IgG and IgGAM groups which were also applied 1 g/kg, i.p. IgG and IGAM therapy 5 minutes after the CLP procedure. Ten, 30 and 60 days after the surgery, animals underwent three behavioral tasks: an open fi eld test to evaluate the locomotor activity, an elevated plus maze test to measure the level of anxiety, and a forced swim test to assess the possible depressive state. The results acquired from these tests were used to estimate the eff ect of immunoglobulin therapy on behavioral changes in CLP-induced sepsis in rats.
In the open fi eld test, the CLP group showed a signifi cant decrease in total squares passed on days 10 and 30. Similarly, total numbers of rearing and grooming were dramatically decreased in the CLP group in comparison with control and sham groups (P <0.005).
In the elevated plus maze test, the number of entries to open arms decreased in the CLP group. In the forced swim test, there was a tendency for increase in immobility time in the CLP group, although the data were statistically insignifi cant. All of these values which were indicating the importance of behavioral alterations were improved on day 60. Immunoglobulin therapy prevented the occurrence of these behavioral changes. Especially, animals in the IgGAM group conserved the values quite near to those of the control group in measured parameters.
Conclusion Sepsis, even though it has been treated with conventional antibiotics, caused a negative eff ect on behavioral parameters. In this study, IgG and IgGAM treated animals in the presence of CLP did not show these behavioral changes. Therefore our results suggest that a single dose of i.p. IgG and IgGAM treatment, which was applied immediately after the sepsis procedure, prevents behavioral defects observed following sepsis.
Introduction Thrombomodulin is an endothelial cell cofactor and glycoprotein for thrombin-catalyzed activation of protein C. A recombinant human soluble thrombomodulin (rhsTM) has been recently developed, and this new agent has a unique amino-terminal structure exhibiting anti-infl ammatory activity including sequestraction and cleavage of high-mobility group box 1(HMGB-1). Methods In this study, 13 patients with septic disseminated intravascular coagulation (DIC) were treated with rhsTM, which is Recomodulin® Inj. 12800 (Asahi Kasei Pharma Co., Tokyo, Japan). Patients with septic DIC were treated with 130 to 380 U/kg/day. Results There were signifi cant results for improvement of APACHE II score and DIC diagnostic criteria score for critically ill patients after treatment using rhsTM (P <0.01). Improvement for platelet count and D-dimer level were also observed in this study (P <0.05). Activation of antithrombin (AT) also was signifi cantly increased after treatment (P <0.05). Hospital mortality was 15.4% in this study. Conclusion The rhsTM might be one of most important endogenous regulators of coagulation, acting as the major inhibitor of thrombin as well as AT III. This new agent may play an important role in treatment for septic DIC.
Introduction Antithrombin III (AT III) has been known to contribute to anti-infl ammatory response as well as its anticoagulation. Our previous Introduction Sepsis and septic shock are complex infl ammatory syndromes. Multiple cellular activation processes are involved, and many humoral cascades are triggered. Presumably, endothelial cells play a pivotal rule in the pathogenesis of sepsis, not only because they may infl uence the infl ammatory cascade but also because, upon interaction with excessive amounts of infl ammatory mediators, the function of these cells may become impaired. It is likely that a general dysfunction of the endothelium is a key event in the pathogenesis of sepsis [1] . HMG-CoA-reductase inhibitors have been shown to exhibit pronounced immunomodulatory eff ects independent of lipid lowering. Most of these benefi cial eff ects of statins appear to involve restoring or improving endothelial function [2] . We hypothesize that statins can improve endothelial dysfunction in septic patients. Methods A double-blinded, placebo-controlled, randomized trial was undertaken. We enrolled adult patients within 24 hours of severe sepsis or septic shock diagnosis and randomized them to placebo or atorvastatin 80 mg/day for a short term. Endothelial dysfunction was assessed measuring plasmatic levels of IL-6, ET-1, VCAM-1 by ELISA and measuring fl ow-mediated vasodilatation of the brachial artery at basal, 24 and 72 hours after randomization.
Results We studied 47 patients, 24 in the placebo group (mean age 52 ± 20 years, 29.1% male; APACHE II risk score 23.5 ± 7.3) and 23 in the statin group (mean age 49.5 ± 18 years, 53.4% male; APACHE II risk score 23 ± 6.9). The baseline characteristics of the placebo group were similar to statin patients as well as the mean length of stay in the ICU (8.6 ± 7.4 and 9.1 ± 8 days, respectively) and the time on vasopressors (49.3 ± 47.1 and 59 ± 91.1 hours, respectively). No signifi cant diff erence was observed on the temporal variation of biomarker levels (IL-6, VCAM-1, ET-1) between treatment and control groups. The intrahospital mortality rate was 26% in the statin group and 45% in the placebo group (P = 0.17).
Introduction A novel sorbent hemoadsorption device for cytokine removal (CytoSorbents, USA) was developed and successfully tested in animal models of sepsis. The experience in the clinical setting is still limited to case reports. In this fi rst clinical trial, we tested the hypothesis that treatment with sorbent hemoadsorption could safely and eff ectively reduce cytokines in septic patients with acute lung injury (ALI). Methods Ventilated patients fulfi lling the criteria for severe sepsis and ALI were enrolled in this multicenter randomized, controlled, openlabel study comparing standard of care with or without hemoperfusion treatment. Primary endpoints were safety and IL-6 reduction. Treated patients underwent hemoperfusion at fl ow rates of ~200 to 300 ml/ minute for 6 hours per day for 7 consecutive days. The overall mean reduction in individual plasma cytokines for the control and treatment groups during the treatment period was calculated using a generalized linear model. Results Forty-three patients (18 treated, 25 control) completed the study and were further analyzed. Incidence of organ dysfunction at enrollment (treatment vs. control) was: septic shock (94% vs. 100%, P = 0.42), acute respiratory distress syndrome (67% vs. 56%, P = 0.33), and renal failure (39% vs. 24%, P = 0.54). During 115 treatments no serious device-related adverse events occurred. On average, there were no changes in hematology and other blood parameters except for a modest reduction in platelet count (<10%) and albumin (<5%) with treatment. Hemoperfusion decreased IL-6 blood concentration signifi cantly (-49.1%, P = 0.01), with similar reductions of MCP-1 (-49.5%, P = 0.002), IL-1ra (-36.5%, P = 0.001), and IL-8 (-30.2%, P = 0.002). The 28-day mortality (28% vs. 24% control, P = 0.84) and 60day mortality (39% vs. 32% control, P = 0.75) did not diff er signifi cantly between the two studied groups. Conclusion In this fi rst clinical study of a novel sorbent hemoadsorption device in patients with severe sepsis and ALI, the device appeared to be safe and decreased the blood concentration of several cytokines. Further research is needed to study the eff ect of the device on the clinical outcome of septic patients.
response; and the changes of endotoxin and proinfl ammatory molecules.
Methods Forty septic/septic shock patients with renal failure were enrolled in the study. All patients had preoperative endotoxin >0.6 level/units (EAA Spectral D) and were submitted to high-volume hemodiafi ltration (50 ml/kg/hour, Prismafl ex; Gambro) with a new treated heparin-coated membrane (oXiris; Gambro). At T0 (pretreatment) and T1 (24 hours) the main clinical and biochemical data were evaluated. All data are expressed as mean ± SD. One-way ANOVA test with Bonferroni correction was used to evaluate the data changes. P <0.05 was considered signifi cant.
Results Table 1 presents the main results of this study. Conclusion In septic/septic shock patients with renal failure, CRRT with a new treated heparin-coated membrane (oXiris; Gambro) is clinically feasible, and has a positive eff ect on renal function and hemodynamics. An adsorbing eff ect on proinfl ammatory mediators may have a role in these results. These data and the trend toward a decrease of endotoxin during the treatment warrant further investigation. Reference Introduction Endotoxin, a component of the outer membrane of Gramnegative bacteria, is considered an important factor in pathogenesis of septic shock [1] . The aim of our study was to determine whether endotoxin elimination treatment added to the standard treatment would improve organ function in patients with septic shock. Methods Adult patients with septic shock who required renal replacement therapy (RRT), with a confi rmed endotoxemia, and suspected Gram-negative infection were consecutively added to the study within the fi rst 24 hours after diagnosis. All patients received full standard treatment for septic shock. Endotoxin elimination was performed using the membrane oXiris (Gambro, Sweden), a medical device for continued RRT with the unique feature of endotoxin adsorbtion. An endotoxin activity assay was used to monitor endotoxin elimination therapy at baseline (T0), 3 hours (T1), 12 hours (T2), 24 hours (T3), 48 hours (T4), and 72 hours (T5). Our key indicators were the improvement in hemodynamics and organ function, and decrease of endotoxin activity (EA) in blood. Continuous variables are presented as mean values with standard deviations. Results High EA level at baseline (0.74 ± 0.14 endotoxin activity units (EAU)) signifi cantly decreased during RRT with oXiris membrane to 0.46 ± 0.02 (T1), 0.34 ± 0.01 (T2), 0.4 ± 0.02 (T3), 0.46 ± 0.04 (T4), 0.35 ± 0.07 (T5) EAU (P <0.05). MAP increased from baseline 72 ± 14 to 81 ± 18, 76 ± 6, 77 ± 7, 83 ± 13, 87 ± 10 mmHg (P <0.05), and the mean norepinephrine use decreased from 0.23 ± 0.04 to 0.19 ± 0.02, 0.11 ± 0.01, 0.09 ± 0.01, 0.04 ± 0.01, 0.0 μg/kg/minute (P <0.05) at T0, T1, T2, T3, T4, T5, respectively. The SOFA score had decreased from 14 ± 4 to 12 ± 2, 9 ± 3, 7 ± 3 points (P <0.05), and the procalcitonin level declined from 107 ± 123 to 45 ± 41, 29 ± 30, 17 ± 157 ± 1 ng/ml (P <0.05) at T0, T3, T4, T5.
Conclusion RRT with oXiris membrane resulted in the eff ective elimination of endotoxins from the blood. The therapy was associated with an increase in blood pressure, a reduction of vasopressor requirements, and an improvement of organ function. The application of the endotoxin activity assay was useful for bedside monitoring of endotoxemia in ICU patients. Introduction Severe sepsis and septic shock remain the most serious problem of critical care medicine with a mortality rate of 30 to 55% [1] . Several studies have demonstrated positive eff ects of selective adsorption of LPS on blood pressure, PaO 2 /FiO 2 ratio, endotoxin removal and mortality [2, 3] . The purpose of the study was to evaluate the effi ciency of using the selective adsorption of LPS, Toraymyxin -PMX-F (Toray, Japan) and Alteco® LPS Adsorber (Alteco Medical AB, Sweden), in the complex treatment of patients with severe sepsis. Methods Forty-six patients with Gram-negative sepsis in the postoperative period were enrolled into the study. Toraymyxin -PMX-F was used in the PMX-F group (n = 14), while Alteco LPS adsorption was used in the Alteco LPS group (n = 32). The clinical characteristics are listed in Table 1 . The SOFA score, PaO 2 /FiO 2 , procalcitonin (PCT), C-reactive protein (CRP), endotoxin activity assay (EAA) was noted before, 24 and 48 hours after the selective adsorption of LPS.
Results At 48 hours after PMX-F, signifi cantly decreased PCT from 17.5 (5.0; 40.9) to 7.1 (4.8; 13.0) ng/ml, P = 0.028, decreased CRP from 180 (133; 286) to 132 (68; 155) mg/l, P = 0.015 and SOFA score from 7.0 (3,0; 8.0) to 6.0 (3,0; 7.0), P = 0.007. At 24 hours after Alteco LPS, signifi cantly decreased PCT from 8.7 (3.0; 25.9) to 4.8 (2.1; 10.0) ng/ml. The 28-day mortality rate was 14.2% (n = 2) in the PMX-F group and 31.3% (n = 10) in the Alteco LPS group. Introduction Corticosteroid (CS) therapy in sepsis remains controversial and was fi rst introduced in sepsis management for its antiinfl ammatory property. CS has found a role in septic shock amelioration with inconsistent outcomes. The Surviving Sepsis Campaign (SSC) includes CS as a Level 2C recommendation in septic shock [1] . Adapting and practicing SSC guidelines vary between critical care units. Accordingly, a survey was conducted to elucidate the usage of CS for septic shock by UK critical care physicians (CCPs). Methods Following approval by the UK Intensive Care Society (ICS), the survey was publicised on the ICS website and its newsletter. Results A total of 81 intensivists responded to this online survey. Seventy-four (92.5%) CCPs prescribed CS only if the septic shock is poorly responsive to fl uid resuscitation and vasopressor therapy. Six (7.5%) initiated CS at the same time as vasopressor therapy. None initiated CS for patients with severe sepsis. No CS other than hydrocortisone is being used. The most commonly used intravenous regimen is 50 mg 6 hourly (65%) followed by 50 mg 8 hourly (11%). Only 10% of CCPs would prescribe it by infusion. Less commonly used regimens were 100 mg 8 hourly (6%) and 100 mg 6 hourly (5%). Only 5% would consider adding fl udrocortisone. Prior to initiating CS, 5% of CCPs would perform a short synacthen test, while 94% would not. The majority (89%) of CCPs would stop CS after resolution of shock state or when vasopressor infusion is terminated whilst 11% after a fi xed duration. Withdrawal of CS also diff ered, in that 25% tapered/weaned steroids, 31% stopped it abruptly and 44% of CCPs would base their CS cessation pattern on the clinical context. Only 46% of CCPs believe that CS is benefi cial whereas 44% were unsure of the benefi ts in septic shock. Only 29 (36%) responders indicated that their critical care unit had a written protocol for CS in septic shock. Conclusion The perceptions, usage and cessation of CS in septic shock vary but do appear to have shifted in the last decade. A UK survey in 2003 identifi ed that only 60% of ICUs used CS for septic shock and over 22% perform a short synacthen test [2] . It appears that many intensivists are using CS for septic shock, despite confl icting outcome data. We all strive to practice evidence-based medicine but until we have a robust, reliable and methodical randomised control trial that attempts to resolve the CS debate, practice will remain diverse on this subject, as refl ected by our survey. References Introduction From December 2009 to December 2010, 47 patients in Scotland presented with confi rmed anthrax infection manifested by soft tissue disease related to heroin injection. These cases represent the fi rst known outbreak of a recently recognized form of anthrax, termed injectional anthrax, which appears to be associated with a high mortality rate (28% in confi rmed cases from the UK outbreak). While epidemiologic data from this outbreak have been published, no report has systematically described fi ndings in patients at presentation or compared these fi ndings in nonsurvivors and survivors. Methods To better describe injectional anthrax, we developed a questionnaire and sent it to clinicians who had cared for confi rmed cases during the outbreak. Completed questionnaires describing 27 patients, 11 nonsurvivors and 16 survivors, were returned. Results In preliminary analysis of categorical data, a signifi cantly (Fisher exact test) greater proportion of patients with compared with without the following fi ndings did not survive; history of alcohol use (P = 0.05); the presence of lethargy (P = 0.01), confusion (P = 0.03), nausea (P = 0.04), abdominal pain (P = 0.02), or the need for vasopressors (P = 0.002), oxygen, mechanical ventilation, or steroids (all P = 0.004) at presentation; and excessive bleeding at surgery (P = 0.01). Initial analysis of continuous data demonstrated that, compared with survivors at presentation, nonsurvivors had signifi cantly (one-way ANOVA) increased respiratory rate, percent neutrophils on complete blood count, hemoglobin, INR, C-reactive protein, and bilirubin and signifi cantly decreased temperature, systolic blood pressure, platelets, sodium, albumin, calcium (corrected for albumin), base excess and bicarbonate (all P ≤0.05).
Conclusion The implications of the apparent diff erences noted between nonsurvivors and survivors in this survey of cases from the fi rst known outbreak of injectional anthrax require further study. However, these diff erences might inform the design of research during future outbreaks or of methods to identify patients most in need of anthrax-specifi c therapies such as toxin-directed antibodies.
Introduction Based on the results of our previous studies [1] we have identifi ed clinical risk factors for the emergence of Gr(+) infections in our ICU and we have developed a new algorithm for combating them. The choice of the particular antibiotic drug is guided by additional risk factors for severity of illness and data on the infectious focus. The response to therapy and its duration are also stated. The aim of the current study was to evaluate the effi cacy and safety of this preemptive approach.
Methods A randomized prospective controlled trial was carried out from September 2010 to September 2012. Patients were submitted to block randomization and stratifi ed on the basis of their initial SAPS II exp score. Antibiotic therapy was started on the day of inclusion in the treatment group and only with proven Gr(+) pathogen in the control group. Initial data were gathered on demographics, diagnosis, proven risk factors for sepsis-related mortality, severity of infl ammatory response, ventilator-associated pneumonia and organ dysfunction. Dynamics of SIRS, CPIS and SOFA scores, subsequent infectious isolates, ventilator-free days, length of ICU stay and outcome were followed for each patient. Results A total of 170 patients were enrolled. No statistically signifi cant diff erences in their basal characteristics were found. The subsequent score values, length of ICU stay and the number of ventilator-free days were also comparable between groups. The majority of Gr(+) pathogens were isolated between 6 and 10 days of inclusion. No diff erences were found regarding the concomitant Gr(-) fl ora and the related antibiotic therapy. The new organ dysfunction severity was similar in both groups (P = 0.37). The in-hospital mortality was 26.2% in the treatment group versus 18.6% in the control group (P = 0.56). Signifi cant diff erences between the Kaplan-Meier estimates of survival were also not found (log-rank test P = 0.81). No major adverse reactions were observed. Conclusion The implementation of this new policy failed to reduce the degree of organ dysfunction severity and was not associated with signifi cant survival benefi t. Moreover, even though it did not reach statistical signifi cance, a second peak of Gr(+) isolates was observed as a possible complication of the preemptive therapy. Whether this approach could lead to vancomycin MIC creep or there could still be a niche for it later in the course of treatment and/or in nontrauma patients remains to be further explored. Reference Introduction Acinetobacter baumannii (A. baum) is a leading cause of septicemia of patients hospitalized in the ICU with high mortality rates. The aim of our study is to investigate the risk factors associated with A. baum bacteremia and its mortality rates. Introduction The French military hospital at the Kaboul International Airport (KaIA) base provides surgical care for International Force and Afghan National Army soldiers, and also local patients. The development of multiresistant bacteria (MRB) nosocomial infections has raised a major problem complicating the care of combat casualties [1] . The aim of this study is to assess the prevalence of MRB carriage on admission to the ICU in this combat support hospital. Methods We used a prospective observation study on patients admitted to the French military ICU in KaIA over 3 months (July to September 2012). All hospitalized patients were assessed for the presence of colonization with MRB: nasal and rectal swabs were performed to identify, respectively, methicillin-resistant Staphylococcus aureus (MRSA) and extended-spectrum β-lactamases bacteria (ESBLB).
The following data were recorded for each patient on admission: demographic characteristics, bacteriological results, length of stay, type of previous hospitalization. Results Sixty-three patients were admitted. The mean length of stay (MLS) was 3 ± 3 days, and the mean age was 25 ± 14 (13 patients <15 years). Patients were hospitalized for combat-related trauma (74%), noncombat-related trauma, medical pathologies (10%), and postoperative care (8%). They were Afghans (92%) or westerners (8%). Swabs were not realized for eight patients. Forty-three percent revealed an ESBLB colonization: Escherichia coli (22 patients), Klebsiella pneumoniae (one patient), Acinetobacter baumanii (one patient). No patients were colonized with MRSA. Ten patients (16%) were directly admitted to the ICU, 12 (19%) had been hospitalized before admission, 39 (62%) were transferred after resuscitative and stabilization care in a level 2 unit. For the two last categories, the MLS (for previous hospitalization) was respectively 14 ± 28 days and 8 ± 6 hours. Among patients transferred after care in a level 2 unit, MLS was no diff erent between colonized and noncolonized patients: 8 ± 7 versus 9 ± 6 hours (P = 0.5, Mann-Whitney test).
Conclusion In this study, prevalence of colonization with ESBLB at admission is very high, suggesting a high prevalence of MDR colonization in the local population in Afghanistan. It remains important to intensify the prevention policy against MRB cross-transmission in the deployed ICU. Critical Care 2013, Volume 17 Suppl 2 http://ccforum.com/supplements/17/S2
Introduction The aim of this study is to describe the clinical and epidemiological profi le of ICU patients receiving tigecycline (TGC) and to evaluate the potential benefi ts of TGC higher doses. Methods All patients admitted to our ICU between 1 June 2009 and 31 May 2012 who received TGC were evaluated. Cases were excluded when infections were not microbiologically confi rmed. Results Over the study period, 100 patients fulfi lled the inclusion criteria: 54 in the SD group (50 mg every 12 hours) and 46 in the HD group (100 mg every 12 hours). The SD group and the HD group were not signifi cantly diff erent in terms of age, severity of disease, duration of TGC therapy, rate of concomitant other active antibiotic use and of inadequate empirical antimicrobial therapy (IIAT) (P = NS). MDR A. baumannii and K. pneumoniae were the main pathogens isolated. The percentage of germs other than A. baumannii and K. pneumoniae was higher in the SD TGC group (P <0.01). Otherwise infections due to less susceptible germs (TGC MIC value ≥1 μg/ml) were mainly treated with TGC higher doses (P <0.01). No signifi cant diff erences were found in terms of ICU mortality (P = 0.8). The rate of abnormal laboratory measures during TGC treatment was similar between the two groups (P = NS). No patients required TGC discontinuation or dose reduction because of suspected adverse events. In the VAP subpopulation (63 patients: 30 received SD and 33 HD), the clinical cure rate and microbiological eradication percentage were higher when TGC was used at higher doses (57.6% vs. 33.3%; P = 0.08 and 57.1% vs. 30.4%; P = 0.1). Table 1 shows multivariate analysis of clinical cure predictors in the VAP subgroup. Conclusion In critically ill patients, HD TGC use seems to be safe and, combined with other active antibiotics, may increase the rate of MDR germ VAP clinical success. IIAT and the severity degree of patients' clinical condition still remain major determinants of VAP treatment failure. Reference
Introduction Amikacin Inhale (NKTR-061, BAY41-6551) is a drugdevice combination in clinical development for adjunctive treatment of intubated and mechanically ventilated patients with Gram-negative pneumonia. The product uses a proprietary vibrating mesh nebulizer system (PDDS Clinical) with amikacin sulfate formulated for inhalation (3.2 ml of 125 mg/ml amikacin solution) for a 10-day twice-daily course of therapy. It is designed for use with two delivery systems: one system for intubated patients (On-vent; Figure 1 ), and a second Handheld (HH) system for patients who are extubated before completing the course of therapy ( Figure 2 ). We investigated in vitro the amikacin lung dose delivered by PDDS Clinical. Methods An estimated lung dose (ELD) for On-vent setting was measured in vitro after collecting aerosolized amikacin from a fi lter at the end of an endotracheal tube during ventilation. The ELD for the HH device was calculated from the fi ne particle fraction (FPF <5 μm) postmouthpiece, multiplied by the in vitro delivered dose post-mouthpiece. FPF <5 μm refl ects lung deposition observed during phase 2 clinical trials [1] . Eighty-one nebulizers with volume median diameter (VMD) Introduction Recent studies demonstrate that a loading dose of 25 mg/kg (total body weight) of amikacin in septic patients is required to reach a suffi cient peak concentration. This study examines parameters infl uencing the relation between amikacin dose and peak concentration.
Methods In this retrospective study we looked at 47 patients (128 peak levels) between 2003 and 2012. Multivariate linear regression analysis was done for several parameters: administered dose calculated with total body weight, ideal body weight, adjusted body weight, type of intensive care patient, BMI, daily fl uid balance, SOFA score and APACHE score, and patient characteristics were analyzed. Results A linear correlation between dose and amikacin peak level was confi rmed (Figure 1) . A total 54.69% of all amikacin administrations did not result in a therapeutic peak level. The multivariate linear regression analysis showed the best linear correlation with adjusted body weight and SOFA score. The comparison of variables between four patient groups, based on the deviation between measured peak level and predicted peak level (according the linear correlation), showed new variables that may infl uence peak level. Conclusion This confi rms that low doses (<18 mg/kg) of amikacin in intensive care patients seldom result in a therapeutic peak level. The proposed loading dose of 25 mg/kg is good for reaching a therapeutic level, although 29.6% remains subtherapeutic. Due to the linear correlation, more therapeutic levels may be reached with higher doses (25 to 30 mg/kg). New variables need further investigation to explain the high variability in achieved peak level.
Introduction Antibiotic-associated diarrhoea (AAD) occurs in as many as 30% of patients receiving antibiotics, often leading to increased morbidity, prolonged in-hospital stay and additional healthcare resource utilisation. Age, antibiotics and prolonged postoperative ward and ICU stay have been suggested to be independent risk factors. In such patient populations, probiotics may be used to prevent antibioticassociated diarrhoea, yet they are not routinely recommended as a component of perioperative care. The aim of this study was to model the long-term costs associated with AAD and to assess the eff ectiveness of probiotics as a preventive strategy.
We developed a simulation model to determine clinical costs and outcomes attributable to AAD. To assess the cost-eff ectiveness of probiotics, as part of a perioperative regime, we constructed a decision Critical Care 2013, Volume 17 Suppl 2 http://ccforum.com/supplements/17/S2 S30 tree. The model observes long-term costs and outcomes of probiotics as compared with conventional therapy, from a societal perspective. Input parameters, extracted from meta-analysis, clinical trials and national databases, include incidence numbers, costs and qualityadjusted health states for the remaining life (QALYs). Outcomes assessed were overall costs attributable to ADD and the cost-eff ectiveness of probiotics, described as costs/QALY.
Our results indicate an estimated incremental lifetime cost of £13,272.53 per ADD patient, largely driven by increased ICU length of stay and readmission rates. The addition of probiotics to the standard perioperative regime is associated with a small survival benefi t of 1.2 months, yet a cost reduction of £917.3/ADD patient. The main cost was increased duration of ICU stay and readmissions, which contribute to 85% of total expenses. Conclusion AAD is associated with a signifi cant increase in costs from a societal perspective. The provision of probiotics can achieve substantial cost savings and can be recommended as a cost-eff ective regime in the perioperative setting. Preventing ADD off ers a potentially signifi cant reduction of in-hospital costs and resource expenditures. Introduction Novel treatment strategies for invasive candidiasis (IC) are constantly emerging. Nevertheless, diffi culties in diagnosis pose a challenge on their reliability, effi cacy and safety. We have previously developed and approbated in our ICU an algorithm for empirical antimycotic therapy, combining the most signifi cant risk factors for IC with three major clinical criteria for persistent nonbacterial sepsis [1] . On the other hand, preemptive therapy, based on identifi cation of mycotic antigens and/or anti-mycotic antibodies in serum, is regarded as more reliable, even though it is known for its low sensitivity. The aim of the current study was to compare and evaluate the possible outcome benefi t of our protocol implementation versus detection of galactomanan in patient's serum as a trigger for antimycotic treatment initiation. Methods A randomized prospective controlled trial was carried out from September 2010 to September 2012. After the implication of the inclusion and exclusion criteria, patients were submitted to block randomization and stratifi ed on the basis of their initial SAPS II exp score. Antimycotic therapy was started on the day of inclusion in the control group and only with positive galactomanan serum test in the preemptive therapy group. Initial data were gathered on demographics, proven risk factors for IC-related mortality, severity of infl ammatory response and organ dysfunction. Dynamics of SIRS and SOFA values, Candida colonization index, ventilator-free days, length of ICU stay and outcome were followed for each patient. Results A total of 106 patients were enrolled. No statistically signifi cant diff erences in their basal characteristics were found. The subsequent SIRS and SOFA scores showed fi rm dynamics in the control group, although the new organ dysfunction severity was insignifi cantly lower. The length of ICU stay and the number of ventilator-free days were comparable. The in-hospital mortality was 47.1% in the preemptive therapy group versus 31.3% in the control group (P = 0.94). A total of seven adverse reactions were observed among treated patients, yet not associated with higher mortality risk. Conclusion The choice of empirical versus preemptive therapy led to earlier and more stable reduction in the degree of organ dysfunction severity. It showed to be at least not inferior if not equal; in terms of survival benefi t and expediency of treatment. Moreover, galactomanan detection fails to guide the choice of the individual antimycotic, based on the expected Candida spp. Reference Introduction Invasive candidemia is a major cause of increased mortality among ICU patients. Antifungal agents like liposomale amphotericin B and azoles could not accomplish the claim to be fi rst choice in the treatment of invasive fungal infection (IFI) because of side eff ects and eff ectiveness. Especially, cardiothoracic surgery patients as a group of high-risk patients are in a focus for new strategies and agents. A new class of antimycotic agents, the echinocandins, with a low profi le of side eff ects, low interactive potential and high eff ectiveness in the treatment of candidemia, is a powerful option in the treatment of IFI. We report our single-center experience with a modifi ed clinical treatment approach based on clinical score of Leon and using echinocandins as fi rst-line therapy for proven and suspected fungal infection. Methods From May 2011 to October 2012, 2,844 patients were treated on our cardiothoracic ICU. We evaluated 37 cardiothoracic postoperative patients with proven or suspected IFI or prophylaxis ( Figure 1 ). The records were evaluated for cardiothoracic procedures, microbiological and yeast date, cardiothoracic surgery score (CASUS), ICU and clinical data.
Mean age was 67.4 years with 64% male patients. Most patients had combined CABG and valve procedure (n = 20), other groups were HTX and LTX (n = 4), assist therapy (n = 4), TAVI (n = 3) and other procedures. Mean predicted mortality using the logarithmic CASUS score at the onset of IFI was 59%. C. albicans was isolated in 73%, C. glabrata in 21%. Length of antifungal treatment using micafungin in 30 cases was 14 ± 5 days. Eradication of yeast was successful in 79% but mortality of all patients remains high at 36.8% but was lower than predicted in the CASUS score. Mortality was not yeast related.
Conclusion Our described treatment approach shows encouraging results for the treatment of IFI especially in high-risk cardiothoracic patients. with fungi [1] . The relationship between colonization and invasive fungal infection (IFI) in severely ill ICU patients with a VAD support is not described. This study analyzes the incidence and outcome of fungal infection and colonization in VAD patients in bridge to transplantation or in destination therapy. Methods We conducted a retrospective review of all VAD implantations in our surgical ICU between 2007 and 2012. The incidence of fungal colonization, antifungal prophylaxis, bacterial sepsis and the mortality of IFI versus no IFI patients were compared. Results In the study period, 34 patients with severe heart failure or cardiogenic shock were selected for a VAD implantation (nine in destination therapy). The overall mortality rate was 50% during mechanical assistance. Confi rmed (n = 8) and highly suspected (n = 2) IFI occurred during the ICU stay in 29% of patients who were treated with echinocandins, voriconazole and/or liposomal amphotericin B. The isolated fungi were: six Candida albicans, two parapsilosis, one glabrata and one invasive pulmonary aspergillosis. Antifungal prophylaxis with fl uconazole was administered to 18% of patients at mean for 5 days mainly in the more recent implantations. In the no IFI population, 54% (n = 13) had a systemic or VAD bacterial sepsis with a mortality rate about 54%. The mortality without any sepsis was reduced to 18%. Fungal colonization was signifi cantly more present (90% vs. 50%) before IFI in VAD patients. The mortality rate was dramatically higher with IFI (80% vs. 38%) in accordance with the literature [1] . See Table 1 . Conclusion In our center, we observed a high incidence of IFI in ICU patients with VAD that was associated with a mortality rate of 80%. Screening of fungal colonization appears to be very important during the ICU stay for VAD patients. Trials are needed for investigating the use, the drug choice and the timing of antifungal prophylaxis for such high-risk patients. Reference
Introduction Echinocandins are recommended fi rst-line treatment for candidaemia [1] . A cost-eff ectiveness model developed from a UK perspective examined costs and outcomes of antifungal treatment for candidaemia and other forms of invasive candidiasis based on European clinical guidelines [1] . Methods Costs and treatment outcomes with the echinocandin anidula fungin were compared with caspofungin, micafungin, fl uconazole, voriconazole and amphotericin B. The model included non-neutropenic patients aged ≥16 years with confi rmed candidaemia/ another form of invasive candidiasis receiving intravenous fi rst-line treatment [2] . Patients were categorised as a clinical success or failure (patients with persistent/breakthrough infection); frequency data for each outcome were taken from a mixed-treatment comparison [3] . Successfully treated patients switched to oral therapy. Clinical failures switched to a diff erent antifungal class. It was assumed that second-line treatment duration was equivalent to that of fi rst-line treatment and only two lines of therapy were required to treat infection. Other inputs were all-cause 6-week mortality, cost of treatment-related adverse events (AEs) and other medical resource use costs. Life-years were calculated using a published model [4] . Antifungal agent-related AEs were taken from the product label/literature. Resource use was derived from the literature and discussion with clinical experts. Drug acquisition/ administration costs were taken from standard UK costing sources. Results First-line anidulafungin for treatment of candidaemia was cost-eff ective per life-year gained versus fl uconazole (incremental cost-eff ectiveness ratio £813). Anidulafungin was cost saving versus caspofungin and micafungin in terms of life-years gained due to lower ICU costs and a higher rate of survival combined with a higher probability of clinical success. Conclusion Anidulafungin was cost-eff ective compared with fl uconazole for treatment of candidaemia and was cost saving versus other echinocandins in the UK. European guidelines recommend echinocandins as fi rst-line treatments for candidaemia [1] ; this model indicates that anidulafungin marries clinical eff ectiveness and cost-eff ectiveness.
Introduction Invasive fungal infections (IFI) aff ect 1% of ICU patients and are increasing in incidence. IFIs are associated with a poor prognosis, which is further complicated by diffi culties in identifi cation of fungal organisms by traditional culture methods and the emergence of Candida species resistant to triazole therapy [1, 2] . This study aimed to assess the prevalence of IFIs, the organisms responsible and outcomes of patients aff ected. The majority of patients (71%) were treated with echinocandins, whilst of the nine patients who were initially treated with fl ucanazole, six (67%) required therapy escalation to an echinocandin.
The results of our study are consistent with other published data, in that whilst IFI prevalence is low, they are associated with increased morbidity in critically ill patients. This study has led to a change in hospital policy regarding antifungal use in the ICU, with echinocandins being fi rst-line in the pre-emptive treatment of IFI. We keenly await the results of the FIRE study, which will provide important insights to identifi cation of patients at risk of IFIs and optimal drug therapy.
Introduction The aim of this study was to compare self-reported beliefs with actual clinical practice of oxygen therapy in the ICU. Hyperoxia is frequently encountered in ventilated patients and prolonged exposure has repeatedly been shown to induce lung injury and (systemic) toxicity.
Methods An online questionnaire for ICU clinicians was conducted to investigate beliefs and motives regarding oxygen therapy for critically ill patients. Furthermore, arterial blood gas (ABG) samples and corresponding ventilator settings were retrieved to retrospectively assess objective oxygenation between 1 April 2011 and 31 March 2012 in the ICUs of three teaching hospitals in the Netherlands. Results Analyzable questionnaire responses were received from 200 ICU physicians and nurses. The majority of respondents believed that oxygen-induced lung injury is a concern, although barotrauma and volutrauma are generally considered to impose a greater risk in mechanical ventilation. Frequently allowed minimal saturation ranges in the questionnaire were 85 to 95% and 7 to 10 kPa ( Figure 1 ). Selfreported FiO 2 adjustment in hypothetical patient cases with variable saturation levels was moderately impacted by the underlying clinical condition. To study actual clinical practice, a total of 107,888 ABG samples with corresponding ventilator settings, covering 5,565 patient admissions, were retrieved. Analysis showed a median (IQR) PaO 2 of 11.7 kPa (9.9 to 14.3), median FiO 2 was 0.4 (0.4 to 0.5), median PEEP was 5 (5 to 8). A total 63.5% of all PaO 2 registries were higher than previously suggested oxygenation goals (7.3 to 10.7 kPa) [1] . In 56.8% of cases with PaO 2 higher than the target range, neither FiO 2 nor PEEP levels had been lowered when the next ABG sample was taken.
Conclusion Most clinicians acknowledge the detrimental eff ects of prolonged exposure to hyperoxia in the ICU and report a low tolerance for high saturation levels. However, the self-reported intention for conservative oxygen therapy is not consistently expressed in our objective data of actual clinical practice and a large proportion of patients was exposed to high and potentially toxic oxygen levels.
Introduction During mechanical ventilation, oxygenation can be infl uenced by adjusting FiO 2 and positive end-expiratory pressure (PEEP). There have been recommendations for how the FiO 2 and PEEP should be set [1] . However, in a recent audit we found that the compliance of doctors of these recommendations is very low [2] . Conclusion Implementing an FPi ≤7-based algorithm signifi cantly reduced the FiO 2 and increased the PEEP applied in mechanically ventilated within the fi rst 24 hours. Whether this has any impact on earlier weaning due to reaching the weaning criteria of FiO 2 sooner, and as a result shortening the duration of mechanical ventilation, has to be investigated in the future. References system) were applied with the humidifi er to optimize humidication. TypeB was used in three patients and typeV in four patients. The fl ow was started at 10 l/minute. This fl ow rate was titrated upwards to a maximum of 60 l/minute (10, 25, 30, 40, 50, 60 l/minute) and the AGFR was measured. Intratracheal pressure tracing was done over 1 minute. Airway pressure measurement was repeated and the maximal expiratory pressure was measured in mmHg.
The AGFR in the respiratory circuit was almost same in typeB, but there was obvious decrease in the AGFR in typeV (7.1 ± 1.0, 17.7 ± 0.8, 21.9 ± 0.9, 29.9 ± 3.6, 36.9 ± 2.7, 45.0 ± 5.5 l/minute at assumed fl ow, 10, 25, 30, 40, 50, 60 l/minute, respectively). HFNC signifi cantly increased maximal expiratory pressure in both groups, 1.5 ± 2.1, 2.0 ± 1.0, 3.0 ± 2.8, 4.5 ± 3.5 mmHg for typeV and 2.5 ± 0.7, 5.8 ± 2.4, 6.0 ± 2.8, 8.0 ± 2.8 mmHg (maximum 10 mmHg) for typeB, when AGFR was set at 30, 40, 50, 60 l/minute. Higher AGFRs were found to result in larger increase in maximum expiratory pressure. The data indicate that HFNC are associated with an increase in intratracheal expiratory pressure. Because it was diffi cult to determine end-expiratory pressure, we chose maximal expiratory pressure for a substitute. The reason why AGFR in typeV was lower than assumed fl ow may be the resistance generated by NC. The larger increase in expiratory pressure in our study than previously reported may be due to the eff ect of high respiratory resistance of Japanese who have relatively small airway structure compared with western people. Conclusion HFNC are eff ective in providing higher expiratory pressure.
It is important to know the fl ow rate is lower than expected when the Venturi type is used. Results A weaning-induced pulmonary edema was diagnosed in 12 instances (PAOP signifi cantly increased from 15.6 ± 0.6 to 25.8 ± 0.9 in these cases). EVLWI, BNP, plasma protein and hemoglobin concentrations signifi cantly increased in these instances (28.3 ± 5.7%, 20.2 ± 7.8%, 9.6 ± 0.8% and 9.3 ± 1.3%, respectively) while they did not signifi cantly changed in cases without weaning-induced pulmonary edema. The increase of EVLWI ≥8.5% (+1.5 ml/kg), an increase in BNP ≥6.7% (+23 pg/ml), an increase in plasma protein concentration ≥5% and in hemoglobin concentration ≥5% exhibited good areas under the ROC curves to predict weaning-induced pulmonary edema (0.97 ± 0.03, 0.80 ± 0.11, 1.0 ± 0.00 and 0.92 ± 0.05, respectively). These areas under the ROC curves were not statistically diff erent. The baseline values of EVLWI, BNP, plasma protein and hemoglobin concentrations did not predict weaning-induced pulmonary edema. Conclusion The increases in EVLWI, in plasma protein and hemoglobin concentration and in BNP are valuable alternatives to the pulmonary artery catheter for diagnosing weaning-induced pulmonary edema.
The primary aim of this study is to assess the impact of pressure support ventilation (PSV) on the rate of pneumothorax and mortality in critically ill patients with lung injury. The secondary aim is to evaluate pressure-volume (P-V) relationships. Spontaneous modes of ventilation have been associated with lower rates of atelectasis, less muscle atrophy, better airfl ow distribution and importantly lower sedation requirements, which relates to lower mortality. Accordingly, we hypothesized that the use of PSV in patients with moderate/severe lung injury would have rates of pneumothorax and mortality within the standard of care. We further hypothesized that given its spontaneous nature, set pressures (PEEP and PS) but not tidal volume (Vt) would be related to airway pressures. Methods All adult patients admitted to two surgical/medical ICUs subjected to invasive mechanical ventilation (MV) were enrolled. Patients were stratifi ed by Lung Injury Score (LIS) in two groups: <2.5 (LISL); ≥2.5 (LISH). Exclusion criteria included pneumothorax on admission, use of other ventilatory strategies, and inability to trigger ventilation. Patients were ventilated with PSV, and treated only with pro re nata haldol, morphine and clozapine. Airway pressures and Conclusion We demonstrate that PSV in minimally sedated patients with severe lung injury is safe as it is associated with low incidence of barotrauma, atelectasis and mortality, and with Ppl and duration of MV within standard of care. We also demonstrate in PSV that P-V relationships may diff er and that in this setting higher Vt may not be deleterious.
Introduction The aim of this study is to compare two ventilation strategies, the ARDSNet protocol and open lung management, using computer control for 6 hours. The standard therapy for patients with ARDS does typically apply a mechanical ventilator to support breathing. The cost of therapy is high and it requires much attention from physicians to adjust the proper ventilation settings in a timely manner. A closed-loop ventilation concept has therefore been developed and tested with two induced ARDS pigs. Methods The hardware system is composed of a ventilator (Servo 300), a spectrophotometry (CEVOX), a capnography device (CO2SMO+), an electrical impedance tomography device (GOE MF II) and a patient monitor (Sirecust). The software is developed with Labview 7.1. With approval from the ethical committee, two 27 kg pigs were exposed to surfactant depletion with a warm saline washout to induce ARDS (PaO 2 / FiO 2 <200 mmHg). One pig model was ventilated with an automatic ARDSNet protocol and another was automatically ventilated with open lung management. Blood gas analysis (BGA) was carried out every half an hour. Results Artifi cial ventilation using the auto ARDSNet protocol successfully stabilized oxygenation, minimized plateau pressure (<30 cmH 2 O), and controlled the pH value for acidosis and alkalosis management. On the other hand, auto open lung management off ers a distinctive result of ventilation. A signifi cant improvement of oxygenation and lung compliance was observed within a few breaths after the recruitment maneuvers. Both subjects were ventilated at the same tidal volume of 6 ml/kg and the comparative results of automatic ventilation settings and BGA are provided in Table 1 for every 2 hours.
Conclusion The auto open lung management concept gave much better gas exchange than the auto ARDSNet protocol. These preliminary results showed a necessity to evaluate the two diff erent ventilation strategies. Therefore, further experiments with pig models will be implemented in the near future to obtain results with statistical signifi cance and to ensure the safety of automation in a mechanical ventilation system.
Intellivent-ASV has been developed to provide fully closed loop mechanical ventilation using a ventilation controller keeping EtCO 2 and SpO 2 within expert-based ranges. Ventilation of ARDS patients focuses on delivering adequate oxygenation and allowing elimination of CO 2 while protecting the lung. The objectives were to compare Intellivent-ASV with conventional ventilation on safety and effi cacy, and to compare the number of manual adjustments between the two ventilatory modalities.
Methods A randomized, controlled study including all consecutive patients receiving mechanical ventilation for at least 48 hours. Patients were randomly ventilated either with Intellivent-ASV or conventional ventilation, with a S1 (Hamilton, Bonaduz, Switzerland). Parameters were adjusted by the clinician in charge of the patient. Ventilatory and oxygenation parameters were recorded cycle by cycle during 48 hours and blood gases were performed every 6 hours. Results Twenty-four patients with ARDS were included, 10 female, 14 male, median age 58 (46 to 63) years, APACHE II score 22 (17 to 29), PaO 2 /FiO 2 at inclusion 136 (107 to 154). Eleven were ventilated in the conventional group and 13 in the Intellivent-ASV group. The study was stopped for one patient from the Intellivent-ASV group because of a pneumothorax not caused by ventilation. The delivered Vt was slightly higher during Intellivent-ASV (7.9 (7.5 to 8.5) vs. 7.2 (6.8 to 7.8) ml/kg, P = 0.013). The time spent by the various parameters in the suboptimal zone (safety) is the same for the two ventilation modes. The time spent in the optimal zone (effi cacy) is the same for the two ventilation modes, Introduction Ventilator-induced lung injury (VILI) is a well-known side eff ect of mechanical ventilation. The pressures and volumes needed to induce VILI in healthy animals are far greater than pressure and volumes applied in clinical practice [1] . A possible explanation may be the presence of local pressure multipliers (stress raisers). Methods We retrospectively analyzed CT scans of 147 patients with ARDS and CT scans of 100 healthy subjects. A homogeneous lung would have the same gas/tissue ratio in all its regions. If a lung region expands less than the neighbour regions these will be more strained to vicariate the non/less expanding region. We measured the stress raisers by computing the ratio between the gas fraction of the region of interest and the neighbouring regions: if the infl ation would be the same (homogeneity), the ratio will be equal to one; if the infl ation of the surrounding regions would be greater than the region of interest (that is, more strained), the ratio between the two will be greater than one and was taken as a measure of stress raiser. We considered pathological stress raisers as the regions showing infl ation ratio greater than the 95th percentile of the control group (1.61) and defi ned as the extent of the stress raisers the fraction of lung volume above this threshold.
The extent of stress raisers increased with the severity of ARDS (14 ± 5, 18 ± 8, 23 ± 1% of lung parenchyma in mild, moderate and severe ARDS, P <0.0001). The extent of stress raisers correlated with the dead space fraction (r 2 = 0.34, P <0.001), with the fraction of poorly aerated tissue (r 2 = 0.36, P <0.0001) and also has a negative correlation with the fraction of well infl ated tissue (r 2 = 0.47, P <0.0001).
The response to PEEP, passing from 5 to 45 cmH 2 O is minimal (average decrease of stress raiser extent 6 ± 5%) and inter-individual variability is great (in 11 patients, stress raisers increased passing from PEEP 5 to PEEP 45). Stress raisers turn out to be greater in nonsurvivor patients than in survivor patients (17 ± 7 vs. 20 ± 9% of lung volume, P = 0.03). The ART strategy did not increase the risk of barotrauma (relative risk (RR) = 0.78, 95% CI = 0.19 to 3.30) in the fi rst 7 days after randomization or the need to initiate or increase vasopressors or mean arterial pressure <65 mmHg (RR = 1.14, 95% CI = 0.65 to 2.02, P = 0.67) 1 hour after randomization. However, the ART strategy increased the risk for severe acidosis (pH <7.10) 1 hour after randomization (RR = 3.20, 95% CI = 1.12 to 9.20, P = 0.03).
Conclusion ART is feasible. The incidence of adverse events was similar between groups except for severe acidosis 1 hour after randomization. Hence we adjusted the study protocol, increasing the respiratory rate (from 10 to 15/minute) during MSARM.
Introduction Cardiac surgical procedures are associated with a high incidence of postoperative complications, increasing costs and mortality. The purpose of this study is to evaluate prospectively the impact of two protective mechanical ventilation strategies, both using low-tidal volume ventilation (6 ml/kg/ibw) after cardiac surgery. Conclusion The reliability of pressure measurements and also of compliance estimation via the tested catheters is high. Only in two catheters was the fi lling volume a critical point for a precise measurement of pressure or for estimation of compliance. Immediately after unpacking, adhesion of the balloon material might prevent reliable pressure measurement, therefore before the fi rst measurement overfi lling of the balloon and retention of the excess gas seems strongly recommended.
Introduction Low tidal volume (VT) ventilation in intensive care patients without lung injury attenuates the systemic infl ammatory response [1] . The contribution of the specifi c organ infl ammatory responses to the systemic picture remains to be elucidated. We investigated the eff ect of low VT ventilation compared with medium high VT on hepatic, splanchnic and cerebral cytokine responses in an experimental large animal postoperative sepsis model. Methods Twenty pigs, group Protective Ventilation (PV), were ventilated with low VT (6 ml/kg) and PEEP 10 cmH 2 O while 10 pigs, group Control (C), were ventilated with a VT of 10 ml/kg and PEEP 5 cmH 2 O. Catheters were introduced into an artery, the jugular bulb, the hepatic vein and the portal vein. Laparotomy for 2 hours simulated a surgical procedure after which baseline ensued and a continuous endotoxin infusion was started at 0.25 μg/kg/hour for 5 hours. Diff erences were analyzed with ANOVA for repeated measures. Results TNFα levels were higher in the hepatic vein than in the artery, the jugular bulb and the portal vein. IL-6 levels were higher in the artery and the jugular bulb compared with the portal and hepatic veins. IL-10 levels were higher in the portal vein compared with the jugular bulb and hepatic vein. The organ-specifi c IL-10 concentrations were all higher than the arterial concentration. Comparison between the ventilation groups showed that TNFα, IL-6 and IL-10 in the hepatic vein were higher in group C compared with group PV at the end of the experiment. Peak concentrations of TNFα and IL-6 in the portal vein were higher in group C compared with group PV. In this experiment TNFα was mainly generated in the  liver while the results point to signifi cant nonhepatic IL-6 and IL-10 production. Ventilation with low VT and medium-high PEEP attenuated hepatic and splanchnic cytokine production compared with mediumhigh VT and lower PEEP. Reference Introduction Airway pressure release ventilation (APRV) allows spontaneous breathing throughout the ventilation cycle. It increases venous return and cardiac index, which will signifi cantly improve organ perfusion. This is important in septic shock patients to prevent extrathoracic organ system failure secondary to poor perfusion. Benefi ts of APRV with cardiovascular changes are noticed in patients with acute lung injury and acute respiratory distress syndrome. It is not well established whether applying APRV will improve the survival outcome for septic shock patients. The primary outcome is whether the use of APRV in septic shock patients restores hemodynamic stability earlier than the CMV mode. The secondary hypothesis is whether the use of APRV in septic shock patients improves their survival in the ICU. Methods After Institutional Review Board approval, we retrospectively analyzed the clinical data of 129 septic shock patients who received ventilator support between January and December 2011 at a tertiary care hospital. The Cox proportional hazards model was used in adjusting potential confounding factors. The nonparametric Wilcoxon rank sum test was used to assess signifi cant outcome diff erences between groups. Time to event/survival data will be analyzed using Kaplan-Meier methods. These analyses were accomplished using SAS, version 9.3. Results Among the 187 patients, 58 were excluded as per the exclusion criteria: incomplete data (n = 28), do not resuscitate (n = 16), ICU readmission (n = 12) and head injury (n = 4). Finally, 129 patients were included, from these 91 received CMV and 38 received APRV. At the beginning of the study, there were no diff erences between the groups in relation to hemodynamic parameters. Reversal of shock achieved in less than 72 hours was statistically signifi cant between the groups (APRV, n = 16 (42%) and CMV, n = 8 (9%), P = 0.0101). The proportion of patients recovering from septic shock after initiation of ventilator therapy was higher in APRV than the CMV group (72% vs. 49%, respectively, P <0.0001). The mortality rate was signifi cantly higher in CMV (n = 46, 51%) as compared with APRV (n = 11, 29%) (P = 0.022).
Conclusion The use of APRV in septic shock patients restores hemodynamic stability earlier than the CMV mode. There was a signifi cant improvement in ICU survival using APRV over CMV. Early initiation of APRV in ventilated septic shock patients was associated with a decrease in ICU mortality.
Obese patients are at risk of developing atelectasis and acute respiratory distress syndrome (ARDS) [1] . The prone position (PP) may reduce atelectasis, and improves oxygenation and outcome in severe hypoxemic patients in ARDS [2] , but little is known about its eff ect in obese ARDS patients. Introduction Protective mechanical ventilation (MV) in ARDS is based on reduced stretch of pulmonary tissue, sometimes resulting in severe hypoventilation that can be avoided when using high respiratory rate. High-frequency positive-pressure ventilation (HFPPV) has not been fully explored, especially when associated with other strategies aiming to avoid hypercapnia. Methods We induced ARDS in eight pigs by lung lavage with saline plus 3 hours of injurious MV with low PEEP and high driving pressure (DP). We then performed a recruitment maneuver (RM) followed by PEEP titration using the amount of alveolar collapse in electrical impedance tomography (EIT). Then stabilization during 1 hours with tidal volume (VT) at 6 ml/kg, respiratory rate (RR) 35 breaths/minute and PEEP selected with the PEEP-FiO 2 table (ARMA study), which was kept constant during two steps of HFPPV with a RR 60: one without an inspiratory pause (HFPPV-60), and one with a pause of 30% of inspiratory time (HFPPV-60 w/P30%). In another HFPPV step, we used PEEP titrated with EIT after RM (HFPPV-60 w/RM). During each HFPPV step, VT was set to reach a PaCO 2 of 60 ± 3 mmHg. Distribution of regional ventilation was analyzed using EIT. Equilibrium was considered if PaCO 2 was stable (<5% of variation) for >30 minutes.
Results HFPPV allowed reduction in PaCO 2 levels: 81 (77, 94) versus 60 (58, 61), 59 (58, 60), 60 (58, 61) mmHg, besides using lower VT: 5.2 (5.0, 5.9), 5.1 (4.5, 6.0), 4.7 (4.2, 5.7) and 4.8 (4.5, 5.6) ml/kg during stabilization, HFPPV-60, HFPPV-60 w/P30% and HFPPV-60 w/RM, respectively. It had no signifi cant diff erent results comparing HFPPV-60 with and without an inspiratory pause. HFPPV-60 w/RM allowed a better alveolar homogenization and improvement in oxygenation, shunt, dead space and DP compared with the other steps. See Table 1 . Conclusion HFPPV with a conventional mechanical ventilator is able to maintain stable PaCO 2 in clinically acceptable values, allowing reductions in VT. HFPPV-60 w/RM and PEEP titration using EIT allowed further physiologic benefi ts in a severe ARDS model.
High-frequency percussive ventilation (HFPV) is a rescue technique for most severe acute lung injury/acute respiratory distress syndrome (ARDS) patients [1] , especially with smoke inhalation or respiratory burns [2] . This study aimed at characterizing HFPV as delivered by Percussionnaire VDR4® and at evaluating how hypobarism interferes with HFPV, in order to assess its usability at altitude. Methods Using a mechanical test lung mimicking ARDS (compliance 17 ml/cmH 2 O) with two resistance levels (5 and 15 cmH 2 O/l/second) and ventilated with VDR4® in a hypobaric chamber, ascents/descents between 0 and 5,000 and then 0 and 8,000 ft were performed. Adjustable VDR4® parameters were modifi ed one at a time at each altitude. Besides these parameters (cross-measured with standalone hardware), oxygen consumption of the respirator and three calculated parameters were studied: low-frequency tidal volume (Vt, integrated from instantaneous fl ows measured with a Fleisch pneumotachograph), end-inspiratory (PmEI) and end-expiratory (PmEE) mean pressures. PmEI and PmEE in HFPV refl ect plateau pressure and positive end-expiratory pressure in conventional ventilation. The correction of altitude-induced off set with the modifi cation of working pressure was also tested. Results Data displayed by VDR4® overestimated pulmonary pressures by more than 10%, but were reliable for other parameters. During ascent, an off set appeared for all respiratory parameters: Vt increased by 59% and PmEI by 53% between 0 and 8,000 ft. During descent, the off set was reversely directed with a 39% decrease in Vt and a 28% decrease in PmEE between 8,000 and 0 ft. Modifying working pressure adequately corrected PmEI and PmEE, but not Vt. In all cases, manually correcting VDR4® parameters to their 0 ft level also corrected these off sets. Multivariate analysis further established that, adjusting for other parameters, Vt, PmEI and PmEE did practically not depend on altitude. Oxygen consumption of the respirator was high, 25 l/minute at 0 ft, and stable with altitude. It was reduced with percussive rate and with FiO 2 . Conclusion HFPV can be safely used at altitude, provided that VDR4®displayed parameters are used to manually adjust settings in order to avoid exposing patients to volutrauma or barotrauma during ascent, and to major hypoventilation and alveolar collapse during descent. The high oxygen consumption is currently the main limit to its use for longrange aeromedical evacuations.
The application of PEEP is commonly used in acute respiratory distress syndrome (ARDS) and has been shown to improve oxygenation. To identify patients that most benefi t from the application of PEEP, the discrimination of recruiters and nonrecruiters has been postulated by Gattinoni and colleagues [1] . Recently, Dellamonica and colleagues [2] presented a method to predict alveolar recruitment. We hypothesised that the amount of recruitable volume allows the discrimination between ARDS patients and patients with healthy lungs (HL). Methods We recalculated the recruited volume (RV) in 25 patients with ARDS [3] according to the method proposed by Dellamonica and colleagues during an incremental PEEP manoeuvre (PEEP increased until the plateau pressure reached 45 cmH 2 O). RV was calculated as the change in end-expiratory lung volume minus total respiratory system compliance times the PEEP change (RV = ΔEELV -CTOT×ΔPEEP). For comparison, 12 patients with HL undergoing elective surgery in general anaesthesia were measured using the same protocol. Results Both ARDS and HL patients exhibited typical P-V curves and stepwise recruitment ( Figure 1 ). By raising PEEP from 0 to 12 cmH 2 O, ARDS patients recruited 331 ± 195 ml (mean ± SD) and HL patients 435 ± 43 ml. There was a strong correlation (R 2 = 0.88) of the total RV with the end-inspiratory volume at a plateau pressure of 45 cmH 2 O in both groups; that is, recruitment was found to the same extent in both groups ( Figure 2 ). Conclusion The relative contribution of RV to lung volume gain is similar in ARDS and in patients with healthy lungs. Our results question the relevance of recruitability as defi ned by Dellamonica and colleagues as a typical phenomenon of ARDS, but support the baby lung concept, as the recruited volume was closely related to the size of the lung.
Introduction Venovenous extracorporeal membrane oxygenation (VV-ECMO) for respiratory failure in the ICU is used in a variety of clinical situations and has been demonstrated to signifi cantly improve survival without disability in adult respiratory distress syndrome [1] . ECMO has been presented as a risk factor for bloodstream infection although recently published data do not support this view or the use of antibiotic prophylaxis [2] . We aimed to examine VV-ECMO as a risk factor for nosocomial bloodstream infection. A larger study is needed to confi rm such fi ndings and to assess the need for specifi c intervention, namely routine antibiotic prophylaxis.
Introduction aPTT is a common tool for anticoagulation monitor ing during extracorporeal membrane oxygenation (ECMO). Thromboelasto graphy (TEG) is another available option in this setting. Methods A prospective observational study on 12 consecutive patients during venovenous ECMO. Anticoagulation was provided Critical Care 2013, Volume 17 Suppl 2 http://ccforum.com/supplements/17/S2 S47 with unfractioned heparin titrated to an aPTT ratio target of 1.5 to 2. Kaolin-activated TEG (K-TEG) was contemporarily measured but did not guide heparin infusion. Baseline K-TEG reaction time (R) >20 minutes is accepted for anticoagulation but when it exceeds 90 minutes anticoagulation may be too great [1] . Results Mean ECMO duration was 9 ± 4 days. A total of 152 K-TEGs were collected. Comparison between aPTT and K-TEG R is reported in Table 1 .
Four patients (33%) had hemorrhagic complications. Neither aPTT nor K-TEG R were signifi cantly diff erent in patients with hemorrhagic events compared with patients without hemorrhagic events but the latter received a signifi cantly lower total heparin dose (P = 0.0097). Conclusion Anticoagulation was excessive in more than one-half of the samples according to TEG monitoring, while negligible based on aPTT. Reference Introduction The usefulness of extracorporeal membrane oxygenation (ECMO) is being rediscovered in the wake of the pandemic of H1N1 infl uenza. However, it has been reported that patients who received ECMO often developed virus-associated hemophagocytic syndrome (VAHS), compared with those without ECMO support. Although there is ample evidence that extensive cytokine activation is a key factor in VAHS, ECMO itself could be a potential trigger to exacerbate the pathology by amplifying cytokine activation. In this study, we investigated whether mediators such as cytokines may be produced by ECMO. Methods Patients with severe respiratory failure who were placed on ECMO were enrolled between June and July 2012. This study was approved by the ethics committee. Blood specimens were drawn from the blood circuit at the inlet of the centrifugal pump (before) and outlet of the hollow fi ber oxygenator (after) at a frequency of three to four times per day. Blood IL-1β, IL-2, IL-4, IL-5, IL-6, IL-7, IL-8, IL-10, IL-12(p70), IL-13, IL-17, G-CSF, GM-CSF, IFNγ, MCP-1, MIP-1β, and TNFα were measured globally using a multiplex cytokine bead array system (Bio-Plex; Bio-Rad, Tokyo, Japan). HMGB1 was measured using an ELISA kit (Shino-Test, Tokyo, Japan). Results Two patients with interstitial pneumonia were studied. The ECMO system consisted of a Rotafl ow Centrifugal Pump (Maquet Japan, Tokyo, Japan), a Biocube TNC coating 6000 (NIPRO, Osaka, Japan), and a percutaneous cardiopulmonary support system (Capiox EBS; Terumo, Tokyo, Japan). The blood fl ow rate was 2.0 ± 4.0 l/minute. A total of 34 blood sets were collected. In most cases, blood levels of IL-1β, IL-2, IL-4, IL-5, IL-12(p70), IL-13, IL-17, GM-CSF, IFNγ, and TNFα were below the detection limit and did not increase during ECMO. The other mediators were detected at the inlet (before), but no signifi cant increase was observed at the outlet (after) (HMGB1, P = 0.33; IL-6, P = 0.12; IL-7, P = Introduction During severe exacerbation of chronic obstructive pulmonary disease (COPD) tachypnea, as a consequence of respiratory acidosis, and airfl ow limitation, due to small airway obstruction, lead to lung hyperinfl ation, respiratory distress and gas exchange impairment. Invasive mechanical ventilation could worsen lung hyperinfl ation and produce a vicious circle. We investigated whether increasing extracorporeal carbon dioxide removal (ECCO 2 Cl) could reduce the respiratory rate (RR), so prolonging time for lung emptying and allowing resolution of hyperinfl ation. Methods Six patients with COPD exacerbation with respiratory acidosis (PaCO 2 83 ± 27 mmHg, pH 7.19 ± 0.1) and tachypnea (RR 39 ± 5) despite maximal non-invasive ventilation underwent venovenous extracorporeal membrane oxygenation (VV-ECMO). All patients were awake and spontaneously breathing an adequate air-oxygen mixture to correct hypoxemia (PaO 2 72 ± 27 mmHg). While keeping the blood fl ow stable (2.9 ± 0.5 l/minute), we changed the gas fl ow of the artifi cial lung to modify the extracorporeal CO 2 clearance as a percentage of total patient CO 2 production (% ECCO 2 Cl/total VCO 2 ) and we observed the variations of RR. We recorded RR at three levels of gas fl ow in each patient ( Figure 1) .
In all patients RR decreased with the increase of extracorporeal CO 2 removal and a negative correlation was found between RR and ECCO 2 Cl/total VCO 2 (r 2 = 0.42, P <0.01). In all patients we were able to obtain a reduction of RR below 15 (28 ± 4 vs. 8 ± 4, RR at low gas fl ow vs. RR at maximal gas fl ow, P <0.001). The selected maximal gas fl ow was variable between diff erent patients (6.7 ± 2 l/minute), corresponding to diff erent levels of ECCO 2 Cl/total VCO 2 (83 ± 17%, range 53 to 100%) and RR response (8 ± 4, range 5 to 14). Conclusion In patients with COPD exacerbation, who failed noninvasive ventilation, VV-ECMO allows one to maintain spontaneous breathing. Titration of extracorporeal CO 2 removal leads to control RR. This approach could interrupt the vicious circle of dynamic hyperinfl ation and allow the defl ation of lung parenchyma. Table 1 presents the main results. The CO 2 removal by membrane oxygenator ranged from 56 to 37 ml/minute. All patients survived to the treatment and 7/10 were weaned from the ventilator at the end of ECCO 2 removal. Only one oxygenator was used for every patient without clotting of the circuit or any major bleeding problem. 
We have previously shown, in an ex vivo porcine model, that lung elastance calculated as the PEEP change divided by lung volume increase (ΔPEEP/ΔEELV) is closely correlated to conventionally measured lung elastance using oesophageal pressure [1] . In this study we hypothesize that the successive change in lung volume during a PEEP-step manoeuvre could be predicted from ΔPEEP and lung elastance as ΔPEEP/EL. The objective of the study was to validate this hypothesis in patients with acute respiratory failure (ARF). Methods Thirteen patients with ARF were studied during an incremental PEEP trial, 0-4-8-12-16 cmH 2 O. ΔEELV was determined as the change in expiratory tidal volume following each PEEP step. Conventional calculation of lung elastance was obtained from tidal variation in airway pressure minus tidal variation in oesophageal pressure divided by tidal volume. Position of the oesophageal catheter was verifi ed according to Baydur [2] . The measured change in end-expiratory lung volume during the PEEP-step manoeuvre using spirometry was compared with the end-expiratory lung volume change calculated from EL and stepwise changes in PEEP as ΔPEEP/EL. Results There was a close correlation between the measured build-up of end-expiratory lung volume during a PEEP-step manoeuvre and ΔPEEP/EL where EL was conventionally determined using oesophageal pressure measurements (see Figure 1 ). Conclusion Esophageal pressure measurements are diffi cult to perform [3] and rarely used in routine clinical practice. Our fi ndings indicate that a change in PEEP together with measurements of the resulting change in end-expiratory volume by spirometry in the ventilator could be used to determine lung elastance separately, the relation between lung and chest wall elastance as well as the transpulmonary pressure. References Introduction Long-term use of mechanical ventilators may lead to ventilator-induced diaphragmatic dysfunction (VIDD) and increase the duration of weaning from MV [1] . It was hypothesized that stimulating the diaphragm during MV may prevent VIDD and may lead to early weaning [2] . In this study, the feasibility of generating coordinated contraction of both diaphragms was investigated using a novel transvenous diaphragmatic pacing system. Methods Two juvenile pigs were anesthetized with propofol (150 to 250 μg/kg/minute) and ventilated (VENT) with an assist control mode MV (Nellcor Puritan Bennett 840). Using fl uoroscopy, a novel multipolar neurostimulation catheter (Inspirx RL PICC53; Respithera, Bloomington, MN, USA) was threaded into the left internal jugular vein and advanced to the junction of right atrium and the superior vena cava using a modifi ed Seldinger technique. The successful capture of the right and left phrenic nerves was confi rmed by fl uoroscopic visualization. Peak airway pressures (PAWP) and blood gases were determined after 10 minutes MV (MV), MV and stimulation applied together (MV+STIM) and stimulation only (STIM).
No animal-ventilator dyssynchrony during stimulation (MV+STIM) was noted while peak airway pressures were reduced. During STIM there was no discernible paradoxical movement of the diaphragm. In addition, PCO 2 and PO 2 confi rmed that adequate ventilation and oxygenation can be provided by the system, while PAWP could be reduced (Table 1) . Introduction Retrospective studies suggest that cardiac troponin levels are often elevated in patients with acute exacerbation of chronic obstructive pulmonary disease (AECOPD) indicating a poor survival. Novel high-sensitivity cardiac troponin (hs-cTnT) assays have better analytical precision than standard troponin (cTnT) assays. We elaborated a prospective cohort study to investigate the prognostic value of this novel biomarker in patients with AECOPD. Methods Fifty-six patients (mean age 64 years, 68% male) with the fi nal diagnosis of AECOPD were enrolled. Those who were diagnosed with acute coronary syndromes were excluded. We measured cardiac troponin T with a standard fourth-generation assay and a highsensitivity assay. Clinical, electrocardiographic and echocardiographic data were collected at admission and the primary prognostic endpoint was death during 30 days of follow-up. Introduction British Thoracic Society guidelines on communityacquired pneumonia (CAP) advocate ICU referral for patients with CURB65 score of 4 and 5. A recently developed scoring system, SMART-COP, designed to identify patients at need of intensive respiratory or vasopressor support (IRVS), has been validated in a variety of settings. It predicts the need for ICU admission (defi ned as need for IRVS) with greater accuracy than CURB65, but is not used routinely in our UK institution.
Methods We retrospectively analysed critical care admissions of patients with a diagnosis of CAP in a UK district general hospital -ICNARC-coded diagnoses of pneumonia (bacterial, viral, no organisms isolated) over a 7-month period (August 2011 to January 2012). We ascertained the CURB65 and SMART-COP scores on referral to the ICU and matched them in relation to the need for IRVS, length of inotropic and ventilatory support and ICU length of stay. Results Our search revealed 28 potential matches. Five patients were excluded (not CAP) and the notes for seven patients were not available for analysis. We analysed the notes of 16 patients matching our criteria. In this small sample, there was a strong association between increasing SMART-COP score and the need for IRVS (correlation coeffi cient r = 0.96). There was also a strong correlation with longer inotropic support (r = 0.85) and longer ventilatory support (r = 0.96) with increasing SMART-COP scores but a weaker correlation with length of ICU stay (r = 0.49). Moreover, none of the patients admitted to the ICU had CURB65 score higher than 3 at the time of ICU referral.
Conclusion In our small sample, higher SMART-COP score was associated with increased likelihood of IRVS. This suggests that a further study with a larger sample size should be performed to investigate whether SMART-COP is an improvement on CURB65 in predicting the need for IRVS in UK intensive care patients.
Introduction Streptococcal pneumonia remains the most common cause of community-acquired pneumonia (CAP), bacterial meningitis and bacteremia. Severe pneumonia caused by streptococcal pneumonia frequently exists in the emergency room or ICU. We performed this study to evaluate the eff ect of steroid therapy for severe streptococcal pneumonia patients with mechanical ventilation retrospectively. Methods We enrolled 13 adults of streptococcal pneumonia patients who required mechanical ventilation. Seven of 13 patients (S group) were administered with steroid (hydrocortisone 200 to 300 mg/day), and the remaining six patients received no steroid therapy (NS group).
As the conventional therapies, mechanical ventilation was commenced when a patient's PaO 2 /FiO 2 showed less than 200 or they clinically complained of being short of breath. All patients received appropriate fl uid therapies, vasoactive agents and blood transfusion according to the protocol of early goal-directed therapy in the Surviving Sepsis Campaign Guidelines 2008, and also were treated with antibiotics, immunoglobulins (5 g/day for 3 days) and sivelestat sodium hydrate (4.8 mg/kg/day for 7 days).
The APACHE scores in the S group and NS group were 27 ± 10 and 23 ± 4, Sequential Organ Failure Assessment scores were 8 ± 4 and 7 ± 3, respectively. These scores showed no signifi cant diff erence between the groups. Procalcitonin (PCT) in the S and NS groups was 20.7 ± 21.7 and 45.0 ± 47.7 ng/ml, respectively, and there was no signifi cant diff erence between the groups. PCT declined signifi cantly in both groups. PaO 2 /FiO 2 of the NS group was signifi cantly higher than the S group on ICU admission and 4 days after admission, but no signifi cant diff erence on 7 days after ICU admission. IL-6 of the NS group declined signifi cantly after ICU admission, and the S group also tended to decline. Conclusion Steroid therapy for severe streptococcal pneumonia patients with mechanical ventilation may have a potential to maintain oxygenation of the lung, but no signifi cant eff ects on changes of infl ammatory markers (IL-6, CRP). Introduction Electrical impedance tomography (EIT) is a non-invasive and nonradiating imaging technique, which can be used to visualize ventilation distribution of the lungs and could distinguish between the dependent (dorsal) and nondependent (ventral) parts.
Methods The aim of this study was to observe ventilation distribution between dependent and nondependent lung regions, for the individual patient, during three diff erent levels of support during pressure support (PS) and neurally adjusted ventilatory assist (NAVA) ventilation. Ten mechanically ventilated patients in the ICU were included.
The ratio for dependent/nondependent distribution of ventilation is signifi cantly higher at lower support levels compared with higher support levels in both PS and NAVA. However, during NAVA there was signifi cantly less impedance loss between the diff erent levels of assist compared with PS. Tidal volumes decreased when decreasing assist levels during PS whereas not during NAVA ventilation. The electrical activity of the diaphragm decreased in both PS and NAVA with higher levels of assist. Three patients showed an increase in dependent tidal impedance variation (TIV) after lowering the assist level from 15 to 10 cmH 2 O. This increase in TIV did not occur during NAVA ventilation. Conclusion There is more ventilation in the dependent part of the lung, compared with the nondependent part, at lower levels of assist. This could indicate that at higher support levels the contribution of the diaphragm is reduced. During NAVA ventilation, there is an autoregulation in which the patient is adjusting his tidal ventilation to maintain homogeneous ventilation distribution.
in status asthmaticus. Our purpose was to analyze BiPAP use and outcomes for children with status asthmaticus and obesity in our PED. Methods Patients placed on BiPAP in the PED for status asthmaticus from 1 January 2010 to 31 August 2012 were included in the analysis. Subjects were divided into moderate and severe exacerbations and then further subdivided into the following growth curve-based weight subgroups: <90 percentile, 90 to 97 percentile and >97 percentile. Subjects received standard asthma therapies in addition to BiPAP. Data were obtained at the bedside by the respiratory therapist or collected retrospectively by study investigators. Data were stored and analyzed using a RedCap database.
Results Three hundred and fi fty-nine subjects were analyzed. Table 1 shows the time on BiPAP per visit. Children whose weight was >97 percentile revealed trends towards longer treatment times on BiPAP compared with the other two groups. We explored the feasibility, reliability and physiological signifi cance of diaphragm thickening on ultrasound. Methods Five healthy subjects participated. We monitored inspiratory fl ow, volume, esophageal and gastric pressures, and diaphragm electrical activity (by esophageal and surface electromyography) while subjects performed a series of inspiratory maneuvers: tidal breathing, threshold-loaded breathing, a Muller maneuver, and inspiration to various lung volumes above functional residual capacity. At the end of each inspiratory eff ort, subjects were instructed to close the glottis and relax the respiratory muscles (so as to maintain lung volume while eliminating diaphragm activation). Sonographic images of diaphragm thickening during these maneuvers were obtained using M-mode with a 13 MHz linear array probe placed in the right ninth, 10th, or 11th intercostal space between the middle and anterior axillary lines. Results Diaphragm thickening in the zone of apposition was readily visualized by ultrasound in all fi ve subjects. Mean end-expiratory diaphragm thickness was 2.1 mm (SD = 0.3 mm). During tidal breathing, the diaphragm thickened by a mean of 35% (SD = 31%). The Bland-Altman coeffi cient of reproducibility was 0.5 mm; approximately 50% of measurement variability arose from caliper positioning on the ultrasound machine; diaphragm thickness measurements changed as the probe was placed in diff erent intercostal interspaces. Diaphragm inspiratory thickening increased signifi cantly with increasing inspiratory eff ort but also varied with lung volume independent of eff ort. At inspiratory volumes below 40% of inspiratory capacity, lung volume change contributed minimally to diaphragm thickening. Conclusion Visualizing diaphragm thickening in the zone of apposition by ultrasound provides a feasible non-invasive technique for monitoring diaphragm activation in healthy subjects. Diaphragm thickening primarily refl ects muscular eff ort rather than altered muscle conformation induced by changes in lung volume, especially at lower inspiratory volumes.
The theoretical advantages of monitoring the electrical activity of the diaphragm (EAdi) and neural triggering of support breaths (NAVA-Maquet) have not yet been shown to translate into signifi cant clinical benefi t [1] . Here we assess the eff ect of EAdi monitoring, in patients at risk of prolonged weaning, on outcomes. Introduction Emergency endotracheal intubation results in accidental oesophageal intubation in up to 17% of patients often with disastrous consequences. We have previously published a highly specifi c and sensitive novel method to detect endotracheal intubation based on diff erences in ventilation pressure waveforms in the oesophagus and in the trachea in patients with healthy lungs [1] . A detection algorithm, based on diff erences in compliance/elasticity between the lung and the oesophagus, generated a D-value indicating tracheal intubation if D >0.5 and oesophageal intubation if D <0.5. The aim of the current study was to validate the algorithm in patients with lung disease. Methods After obtaining institutional approval, 20 intubated and ventilated ICU patients were included. Inclusion criteria were controlled mechanical ventilation and at least mild to moderate lung injury according to a Murray lung injury score >0.1. A connecting piece was placed between the endotracheal tube and the ventilation bag. This piece comprised a thin air-fi lled catheter inserted through the tube lumen at 1 cm from the distal end, and a second catheter located at the proximal end of the tube. We performed three consecutive manual bag ventilations while recording the pressure curves through both catheters. For each ventilation, a D-value was calculated. Results Mean age (SD) of the patients was 60 (16) years, 60% were male. The mean (SD) Murray score was 1.4 (0.6). Pathologies included pulmonary oedema, pneumonia, atelectasis and traumatic lung injury. All 60 D-values are represented in Figure 1 . The median (IQR, range) D-value was 38 (16 to 74, 0.8 to 1,272). Our algorithm therefore confi rmed a high sensitivity to detect correct endotracheal intubation also in patients with lung disease. Under the hypothesis that oesophageal compliance does not increase signifi cantly in patients with lung disease, the specifi city of our algorithm will not be aff ected. 
The aim was to compare two novel endotracheal tubes (ETT), Mallinckrodt TaperGuard (TG, tapered polyvinyl chloride (PVC) cuff ) and KimVent Microcuff (MC, cylindrical polyurethrane cuff ), with conventional Portex (PT, globular PVC cuff ) in leakages across cuff s (microaspiration) under simulated clinical situations. It has been shown that globular PVC cuff s protect poorly against leakages due to microchannels formed from infolding of redundant cuff material [1] . We hypothesized that TG and MC better prevent microaspiration, which is a major mechanism of ventilator-associated pneumonia (VAP 
The most common cause of ventilator-associated pneumonia (VAP) is aspiration of oral secretion through the endotracheal tube (ET). Subglottic suction drainage (SSD) has been recommended as a safety measure against aspiration due to its high eff ectiveness. Currently, two types of cuff shape -spindle and tapered -are predominant in high-volume, low-pressure (HVLP) ETs with SSD. However, the shape most suitable for preventing dripping onto the subglottis has not been determined. The purpose of this study was to determine whether an ET with tapered-type cuff can reduce the incidence of VAP. Methods After approval from the appropriate ethics committee, we conducted a single-institutional prospective randomized clinical trial on the eff ectiveness of using an ET with a diff erent cuff type. Introduction National Audit Project 4 (NAP4) highlighted the need to improve airway management in ICUs and key recommendations were the continuous use of end-tidal carbon dioxide (ETCO 2 ) monitoring, pre-intubation checklists and diffi cult airway trolleys [1] . This complete cycle audit aimed to quantify the current state of airway management on our ICU and the eff ectiveness of implementing the NAP4 recommendations.
Methods Data collection was carried out prospectively for both phases and included documentation of intubation, use of ETCO 2 and the incidence of serious adverse events (SAEs). The contents of the intubation boxes were compared against the Diffi cult Airway Society (DAS) guidelines [2] . The re-audit was carried out 6 months following the introduction of a pre-intubation checklist, a documentation sticker, a diffi cult airway trolley and standardization of the basic bedside airway boxes with a checklist of contents. A training program in airway management for all ICU staff was also introduced. Micro-CT scan (SkyScan 1176; Bruker, Belgium) was performed using a resolution of 35 μm. Axial sections of the 20 cm above the cuff were reconstructed, and the volume of secretions was assessed by a density criterion. Microbiological cultures of the ETT lavage fl uid were then obtained. Patient's demographics and clinical data were recorded. In a diff erent set of bench experiments, we injected 1 ml water-based polymer into new ETTs of diff erent sizes. We measured resistance to airfl ow before and after using an ETT cleaning device (Airway Medix Closed Suction System; Biovo Technologies, Tel Aviv, Israel). We also obtained resistance values of intact ETTs as controls.
The studied ETTs remained in place for a median of 7 days (IQR range 4 to 15). The amount of secretions assessed by CT scan was 0.293 ± 0.290 ml (range 0.032 to 0.777 ml). Secretion volumes were not related to patient severity at admission (SAPS 2, P/F ratio) or days of intubation; an inverse correlation with patient's age was present (P = 0.032, R 2 = 0.46). Bacterial growth was present in 9/11 (82%) ETT fl uids cultures and Candida spp. showed an elevated prevalence (6/11, 55%). In the bench tests, the cleaning device reduced resistance to airfl ow (diff erence before and after cleaning 5.5 (95% CI = 8.9 to 1.6) cmH 2 O/l/second, P = 0.006). After cleaning, resistance resulted higher than intact ETTs, although with a clinically negligible diff erence (diff erence 0.3 (95% CI = 0.2 to 0.6 cmH 2 O/l/second), P = 0.032). Conclusion Micro-CT scan is a feasible and promising technique to assess secretions volume in ETTs after extubation. The use of an ETT cleaning device decreases resistance to airfl ow in bench tests; the eff ectiveness of such a device in the clinical setting could be properly assessed by post-extubation CT scan. [1, 2] . The objective was to develop and validate a simplifi ed score for identifying patients with DI in the ICU and to report related complications. Methods Data collected in a prospective multicenter-study from 1,000 consecutive intubations from 42 ICUs were used to develop a simplifi ed score of DI, which was then validated externally in 400 consecutive intubation procedures from 18 other ICUs and internally by bootstrap on 1,000 iterations.
In multivariate analysis, the main predictors of DI (incidence = 11.3%) were related to the patient (Mallampati score III or IV, obstructive sleep apnea syndrome, reduced mobility of cervical spine, limited mouth opening), to pathology (severe hypoxia, coma) and to the operator (non-anesthesiologist). From the β-parameter, a sevenitem simplifi ed score (MACOCHA score; Introduction In mechanically ventilated neonates the fl ow-dependent resistance of the endotracheal tube (ETT) causes a noticeable pressure diff erence between airway and tracheal pressure [1] . This may potentially lead to retardation of the passive driven expiration and dynamic lung infl ation consecutively but more importantly increases . The aim of this study was to evaluate the correlation between NT-proBNP and CCE and the potential usefulness of such variables during the weaning process from MV. Methods Twenty-two long-term (>48 hours) mechanically ventilated patients capable of performing a weaning trial of spontaneous breathing (SBT) were enrolled in the study. Inclusion criteria were: age >18 years and equipment with a standard arterial catheter line. Exclusion criteria were: neuromuscular disease, tracheotomy, renal failure, and traumatic brain injury. During the weaning process, NT-proBNP plasma levels, CCE, and standard hemodynamic and ventilatory data were collected 30 minutes before extubation (T1), 2 hours (T2) and 12 hours later (T3). After removal of tracheal tube, patients with a history of heart failure received continuous positive airway pressure (CPAP group). Patients with normal cardiac function were maintained with spontaneous breathing (SB group).
Results Sixty-six paired NT-proBNP and CCE values were obtained. Patients in the SB group and in the CPAP group were 10 and 12, respectively. In both groups there was a trend towards an increase in NT-proBNP values after extubation, an opposite trend was observed regarding CCE values (P <0.05). NT-proBNP levels showed an increase after extubation (T2, T3) compared with T1; conversely, CCE showed an inverse trend. Overall, a negative correlation was found between NT-proBNP and CCE values (R = -0.81, P <0.001). Signifi cant inverse correlations were found between NT-proBNP and CCE at T1, T2, and T3 (R = -0.91, -0.75 and -0.73 respectively; P < 0.001). The overall correlation between NT-proBNP and CCE was -0.74 in the SB group and -0.86 in the CPAP group. Standard hemodynamic and ventilatory data did not show signifi cant changes during the study. Conclusion NT-proBNP correlated well with CCE. The latter seems to be an additional attractive index of cardiovascular state that, in association with NT-proBNP changes, may provide information about cardiac function on a beat-by-beat basis during weaning process from MV.
Comparison of outcomes between early and late tracheostomy for critically ill patients K Suzuki 1 , S Kusunoki 1 , T Yamanoue 1 , K Tanigawa Introduction Tracheostomy is one of the more commonly performed procedures in critically ill patients requiring long-term mechanical ventilation. However, the optimal timing or method of performing tracheostomies in this population remains to be established. In the present study, we compared outcomes of early and late tracheostomy in critically adult patients with diff erent clinical conditions. Methods All patients needing tracheostomy in the Critical Care Medical Center of Hiroshima Prefectural Hospital from January 2009 to December 2011 were surveyed. Patients with tracheostomy who were not indicated for mechanical ventilation were excluded from the subjects. Early tracheostomy (ET) was defi ned as <10 days after tracheal intubation and late tracheostomy (LT) was defi ned as ≥10 days after intubation. We compared patient characteristics, type of tracheostomy procedure, length of weaning from ventilator and outcomes between the groups. Data are shown as the mean ± SD, with unpaired t test and Mann-Whitney U test used for statistical analyses. Statistical signifi cance was accepted at P <0.05. Results One hundred patients were surveyed. The ET and LT groups included 49 and 51 patients, respectively. Tracheostomy was performed using a percutaneous procedure in 48 patients (ET: 25, LT: 23) and a surgical procedure in 52 patients (ET: 24, LT: 28). Sixty-two patients (ET: 34, LT: 28) survived to discharge and 16 patients died in the ICU (ET: 7, LT: 9). Fifty-six patients (ET: 31, LT: 25) were weaned from ventilator support and tracheostomy cannula was removed in 20 patients (ET: 11, LT: 9). There were no signifi cant diff erences in type of tracheostomy procedure, period from tracheostomy until ICU and hospital discharge, rate of patients who could be weaned from ventilator and removed tracheostomy cannula, and ICU and hospital mortality between the groups. The length of mechanical ventilation and the time to removal of tracheostomy cannula were signifi cantly shorter in the ET group (5 ± 7 vs. 26 ± 41 and 29 ± 24 vs. 94 ± 83 days, respectively). Conclusion In this retrospective study, early tracheostomy reduced the length of weaning after tracheostomy and the time to removal of tracheostomy cannula, while there were no diff erences in the length of ICU stay and patient outcome. In critically ill adult patients who require mechanical ventilation, a tracheostomy performed at an earlier stage may shorten the duration of artifi cial ventilation. A further randomized clinical trial is essential to determine the eff ectiveness and safety of early tracheostomy. Reference S60 variability in the course of blood vessels in the pre-tracheal area. A 5% risk of clinically relevant bleeding was recently reported for patients undergoing PDT [1] . We conducted a systematic review of reports evaluating clinical outcomes following use of ultrasound scanning (US) for PDT. Methods Two investigators performed a search of the literature using the following databases: CENTRAL, Embase, MEDLINE and SCOPUS.
The following eligibility criteria were used: population including adults >16 years managed in the ICU; use of ultrasound to guide decisionmaking pre-PDT or guide PDT performance; report of clinically relevant outcome measures. Nonrandomised controlled trials were classifi ed according to Cochrane Non-Randomised Study Methods Group criteria [2] and evaluated for risk of bias.
Results An initial search identifi ed 2,043 reports, of which 10 studies met eligibility criteria: eight case series, one randomised controlled trial (RCT) and one prospective cohort study, incorporating 488 patients. Two studies specifi cally reported data on patients with obesity (n = 29 patients) and one study reported data for a group of patients with spinal cord fi xation (n = 6). US was used to guide decision to perform PDT or surgical tracheostomy in fi ve studies, with decision to perform surgical tracheostomy ranging from 0 to 27% of cases. US was used to guide insertion point in seven studies, and used real-time in four studies. Times to perform US-guided PDT were reported in four studies (ranging from 8 to 12 minutes). No studies compared time taken with or without US. Data on complications of procedure were reported in nine studies. Minor bleeding was reported for eight cases (1.6% overall). Prolonged bleeding was reported in two cases (0.4%). There were no episodes of catastrophic bleeding among 488 cases. High risk of bias was identifi ed in fi ve studies in terms of patient selection. An intervention protocol was not defi ned in three reports. No attempt was made at blinding any aspect of the 10 studies. Conclusion Use of US guidance could theoretically help minimise risk of haemorrhagic complications during PDT and perhaps reduce time taken to perform PDT. However, there is currently inadequate evidence from controlled cohort studies or RCTs to suggest that routine use for PDT in selected or unselected groups improves clinically relevant outcome measure.
Introduction Failed airway situations are potentially catastrophic events and require a correct approach with appropriate tools. Recently, Ventrain has been presented as a manual device for emergency ventilation through a small-bore cannula, which can provide expiratory assistance by applying the Venturi eff ect. Methods We used the SimulARTI Human Patient Simulator to evaluate Ventrain. Initially, we studied the eff ectiveness and security in ventilating and oxygenating the patient. In a second phase, the Ventrain performance was compared with what is considered to be the present gold standard (Quicktrach II, Portex Mini-Trach II Seldinger Kit, Melker Emergency Cricothyrotomy Catheter Set). Seven anesthesiologists performed an emergency transcricoid ventilation with each device in the same setting.
Results Ventrain provided an average tidal volume of 334 ml and an average minute volume of 2.4 l in the considered situation, with a modifi cation of PAO 2 from 32 to 702 mmHg and of PACO 2 from 54.5 to 38.8 mmHg. In the second phase, the time needed to obtain an eff ective oxygenation with Ventrain was found to be shorter than other devices (median diff erence; vs. Minitrach -60 seconds; vs. Melker -35 seconds; vs. Quicktrach -25 seconds) ( Figure 1 ); the ability to remove CO 2 resulted bigger (average diff erence: vs. Minitrach -11.9;
vs. Melker -0.3; vs. Quicktrach -5.9) ( Figure 2 ) and moreover the users judged it more favorably. Conclusion In this manikin study, Ventrain seemed to be able to appropriately oxygenate and ventilate a patient in a CICV situation. When compared with the best available choices, it has shown not to be inferior. Introduction Eff ective delivery of aerosolized bronchodilators for patients with asthma is crucial for adequate therapy in critical care and emergent settings. Often administered with pressure-metered dose inhalers (pMDIs), bronchodilator delivery depends on the correct patient technique during administration [1] and the ability to measure treatment response, which are diffi cult to monitor at the point of care and particularly so in resource-poor settings where standard inhospital monitoring is unavailable [2] . Methods A point-of-care device for airfl ow measurement during bronchodilator delivery was designed and tested for use in resourcelimited settings. The handheld device was constructed from a clinical aerosol delivery tube with a bidirectional sensor for pressure diff erential detection about the aerosol element ( Figure 1 ). The custom low-cost Introduction Protocol-based care of the tracheostomised patient is important, as adverse events confer a high rate of mortality. Little is known regarding the existence of formal evidence-based guidelines on tracheostomy care. The aim of this study was to perform a systematic review for evidence-based guidelines on adult tracheostomy care. Methods A systematic search of PubMed, MEDLINE, guideline clearinghouses, centres of evidence-based practice, and professional societies' guidelines relating to care of adult patients with a tracheostomy was performed by two reviewers. In addition, a Google search of publicly available tracheostomy care guidelines was performed. Search terms: (tracheostom* OR tracheotom*) AND (protocol* OR guideline* OR standard* OR management OR consensus OR algorithm*). Filters: English language, human, from 1 January 1990 to date, adult patients. Guideline appraisal criteria: the quality of guidelines retrieved was assessed using the Appraisal of Guidelines Research and Evaluation II (AGREE II) instrument [1] .
The search results are summarised in Table 1 . A total of 80 guidelines were identifi ed. Five were found to satisfy the AGREE II criteria and only three related to the entire spectrum of tracheostomy management. The majority was informal and was not published or evidence based. Conclusion Five evidence-based guidelines on adult tracheostomy management were identifi ed. This may represent a paucity of evidence on the subject, suggesting that further clinical trials on the topic are needed to contribute to the evidence base. This also highlights the need for international consensus on the topic, to reduce duplication of eff orts, standardise practice, and improve outcomes. [1] concluded that the majority of airway-related signifi cant complications in ICUs resulted from displaced or blocked tracheostomies and recommended together with the Intensive Care Society and the National Tracheostomy Safety Project that each ICU in the UK should have an emergency airway management plan and guidelines [2] . The aim of this survey was to establish whether such guidelines exist and are familiar to those working within the ICUs of the East of England (EoE), their ease of availability in an emergency and the degree of emergency tracheostomy training within the region. Methods Data collection was via a telephone survey of 11 ICUs in the EoE training region during July 2012 with one senior ICU nurse and one ICU trainee questioned per hospital. Questions related to the existence and accessibility of guidelines for tracheostomy emergencies, and to the respondent's degree of emergency tracheostomy training and their perceived availability of formal training. Results All 11 ICUs questioned perform and manage tracheostomies. Of 22 respondents, 10 knew of guidelines covering all of the emergencies described above and their location. Four respondents thought that these guidelines were accessible in an emergency setting, one-half of which were on computer systems requiring a login and search function. With regards to emergency management, 19 respondents felt competent in a tracheostomy emergency; almost exclusively through experience and in-house teaching. No respondents were aware of any formal emergency tracheostomy management courses. Conclusion Despite national guidance within the UK this survey highlights that implementation and awareness of emergency tracheostomy guidelines in ICUs in the EoE region is poor, and when present they are not readily accessible in an emergency. Emergency training has largely been informal and the availability of formal training courses has not been recognised. In order to improve patient safety there is a need to ensure that emergency tracheostomy management including guidelines, equipment and formalised tracheostomy emergency training are adopted and embraced universally. References Introduction A fatal incident related to a blocked tracheostomy tube prompted a review in our Trust. To provide safe tracheostomy care, changes in staffi ng, education and operational policies were recommended. Training of potential fi rst responders to tracheostomy or laryngectomy emergencies remains outstanding. We aim to quantify the training defi cit. Tracheostomies are common in critical care but these patients require ongoing management of an artifi cial airway on discharge to the ward and even the community. In 2010 our critical care unit cared for 108 tracheostomy patients, of which 30 were transferred to the wards. The 4th National Audit Project highlighted complications including hypoxic brain injury and death [1] and the National Patient Safety Agency recognised a number of avoidable aspects [2] . Existing guidelines for management of these patients including emergencies are not widely known. Methods An anonymous online survey was sent to all trainees who may respond to a tracheostomy emergency in our organisation. Trainees in anaesthesia/critical care, general medicine, general surgery, ENT, thoracics and A&E were approached. All completed forms were included.
We achieved a response rate of 39% (65/168). Respondents comprised: 33% anaesthesia/critical care, 47% medicine and 14% surgery. Over one-half (36/65) had managed tracheostomy/laryngectomy emergencies, with 42% (15/36) of these incidents occurring on the wards and one in an outpatient clinic. Only 20% (13/65) had received any formal training on management of a blocked/misplaced tracheostomy tube and only 18% (12/65) were aware of any guidelines. One-third of responders lacked confi dence in management of these emergencies and 88% felt they would benefi t from formal training including simulation. Conclusion The population of patients with exteriorised tracheas is increasing and represents a high-risk group. Management of airway emergencies in these patients is not part of standard life-support courses. According to our trainees, these scenarios are relatively common and a signifi cant proportion of fi rst responders are poorly equipped to deal with them. Our Trust will be including specifi c training on the emergency management of neck breathers as part of in-house resuscitation training. We would contend that national resuscitation courses should consider doing the same. Introduction Usually percutaneous tracheostomy is accomplished via the tracheal tube. Some severe complications during percutaneous dilatational tracheostomy (PDT) may be related to poor visualization of tracheal structures. The alternative implies extubation and reinsertion of a laryngeal mask (LMA). An accidental extubation as well as an injuring of the vocal cords (because of the infl ated cuff during dislocation) appears impossible in this method. Subjectively, the bronchoscopic view obtained via a LMA seems to be better than that obtained with an endotracheal tube (ET) [1, 2] . Methods In this prospective observational study, the bedside PDT was performed using the Ciaglia Blue Dolphin method in 150 critically ill patients. The patient's tracheal tube was exchanged for a LMA Fastrach™ before undertaking PDT. The insertion of the LMA, the quality of ventilation, the blood gas values, the view of the tracheal puncture site, and the view of the balloon dilatation were rated as follows: very good (1), good (2), barely acceptable (3), poor (4), and very poor (5) .
Results PDTs with LMA were successful in 99.3% of the patients (n = 149). The ratings were 1 or 2 in 96% of cases with regards to ventilation and to blood gas analysis, in 96.6% for identifi cation of relevant structures and tracheal puncture site, and in 93.3% for the view inside the trachea during PDT. A rating of 5 was assigned to one patient requiring tracheal reintubation for inadequate ventilation. There were no damages to the bronchoscope or reports of gastric aspiration. Conclusion The Blue Dolphin PDT using a LMA showed defi nite advantages regarding inspection of dilation process. This method improves visualization of the trachea and larynx during a video-assisted procedure and prevents the diffi culties associated with the use of an ET such as cuff puncture, tube transection by the needle, accidental extubation, and bronchoscope lesions. The LMA results as an eff ective and successful ventilatory device during PDT. This may be especially relevant in cases of diffi cult patient anatomy where improved structural visualization optimizes operating conditions. The intensivist performing PDT should be scrupulous when deciding which method to use. In our ICU the Blue Dolphin PDT with LMA has become the procedure of choice. Introduction Acute cor pulmonale (ACP) is associated with increased mortality in patients ventilated for acute respiratory distress syndrome (ARDS). Interventional lung assist (iLA) allows a lung-protective ventilatory strategy, whilst allowing CO 2 removal, but requires adequate right ventricular (RV) function. RV restriction (including presystolic pulmonary A wave) [1] is not routinely assessed in ARDS.
Methods A prospective analysis of retrospectively collected data in patients with echo during iLA was performed. Data included epidemiologic and ventilatory factors, LV/RV function, evidence of RV restriction and pulmonary hemodynamics. Data are shown as mean ± SD/median (interquartile range).
Results Thirty-two patients (45 ± 17 years), 22 male (68%), SOFA score 11.15 ± 2.38 were included. Pulmonary hypertension (PHT) was 53%, and hospital mortality 43%. Mortality was not associated with age, days on iLA, length of ICU stay, inotropic support, nitric oxide or level of ventilatory support, but was associated with pressor requirement (P = 0.005), a worse PaO 2 :FiO 2 ratio (9.4 (7.8 to 12.6) vs. 15.2 (10.7 to 23.9), P = 0.009) and higher pulmonary artery pressures (56.5 mmHg (50 to 60) vs. 44.5 (40.5 to 51.2), P = 0.02). No echo features of ACP were found, with no signifi cant diff erence between RV systolic function, pulmonary acceleration time and pulmonary velocity time integral between survivors and nonsurvivors. The incidence of RV restriction was high (43%), and independent of PHT, RV systolic function and level of respiratory support, but correlated with CO 2 levels (restrictive 7.1 kPa (7.4 to 8.0) vs. 6.1 (5.8 to 6.8), P = 0.03). See Figure 1 .
Conclusion Typical echo features of ACP were not seen in this study, possibly because of the protective ventilatory strategies allowed by use of iLA. The incidence of RV restriction may refl ect more subtle abnormalities of RV function. Further studies are required to elucidate RV pathophysiology in critically ill adult patients with ARDS. Reference
Introduction Global left ventricular electromechanical dyssynchrony (GLVD) is uncoordinated LV contraction that reduces the extent of intrinsic energy transfer from the myocardium to the circulation leading to a reduction in peak LV pressure rise, prolonged total isovolumic time (t-IVT) and fall in stroke volume [1] . This potentially important parameter is not routinely assessed in critically ill cardiothoracic patients.
Methods A prospective analysis of retrospectively collected data in cardiothoracic ICU patients who underwent echocardiography was performed. In addition to epidemiological factors, echo data included comprehensive assessment of LV/RV systolic and diastolic function including Doppler analysis of isovolumic contraction/ relaxation, ejection time (ET) and fi lling time (FT). t-IVT was calculated as (60 -(total ET + total FT)) and the Tei Index as (ICT + IRT) / ET. t-IVT >14 second/minute and Tei index >0.48 were used to defi ne GLVD [2] . Data are shown as mean ± SD/median (interquartile range). Results A total of 103 patients (63.5 ± 18.4 years), 65 male (63%), APACHE II score (14.6 ± 7.4) were included. The prevalence of GLVD was high (24/103, 22%) and associated with signifi cantly increased mortality, 7.5% vs. 25% (P = 0.02). There was no diff erence in requirement for cardiorespiratory support between the two populations, but there were signifi cant diff erences (no GLVD vs. GLVD) in requirement for , P = 0.43), mitral regurgitation (40.5% vs. 62.5%, P = 0.06), or any other measures of LV systolic or diastolic function between the two groups. There was good correlation between the two methods used to assess dyssynchrony (LV t-IVT:LV Tei index correlation coeffi cient = 0.80, P <0.001).
Conclusion GLVD that limits cardiac output is common in the cardiothoracic ICU, and signifi cantly related to mortality. When diagnosed, the underlying cause should be sought and treatment instigated to minimize the t-iVT (pacing optimization/revascularization/ inotrope titration/volaemia optimization). References Introduction Correction of coagulopathy before central venous catheter (CVC) insertion is a common practice; however, when ultrasound guidance is used this is controversial as mechanical complications are rare. Studies in oncology patients suggest that CVC placement without prior correction of coagulopathy is safe but no studies are available for critically ill patients and guidelines do not give recommendations [1, 2] . We do not routinely correct coagulopathy, even if severe, when ultrasound guidance is used and the purpose of this retrospective study was to evaluate the safety of this practice.
Methods Data for all ultrasound-guided interventions, including complications, are prospectively collected in our department for audit purposes; in this study we involved only CVC insertions in the ICU between February 2011 and November 2012. Electronic medical and laboratory records and paper-based nursing charts were retrospectively studied for all interventions, specifi cally looking for blood results, coagulation abnormalities and intervention-related complications.
In the study period, ultrasound guidance was employed for a total of 291 central line insertions in 220 ICU patients. Coagulopathy was detected in 127 cases at the time of CVC placement (43.6%). On the day of CVC insertion, coagulation abnormalities were corrected in 20 cases (15.7%); 33 out of 50 patients with severe coagulopathy (66.0%) and 74 out of 77 patients with coagulopathy of moderate severity (96.1%) had no correction at all. Correction was started only after CVC insertion for reasons unrelated to CVC placement in a further eight and two patients with severe and less severe coagulopathy (16.0% and 2.6%), respectively. No bleeding complications were observed. Conclusion In patients undergoing CVC insertion in our ICU, coagulopathy is common. We observed uncomplicated CVC placement in all 41 patients with severe uncorrected coagulopathy and in a further 76 patients with coagulopathy of moderate severity. When combined with other studies, our data suggest that ultrasound-guided CVC placement without routine correction of coagulation abnormalities may be safe in the ICU.
Introduction Early bleeding from the exit site after CVC or PICC placement is a very common event that causes diffi culties in the patient's care and logistical problems. In our experience, the rate of signifi cant local bleeding after placement of PICCs without reverse tapering may be as high as 40% at 1 hour and 15% at 24 hours, while the rate of bleeding after placement of a large-bore dialysis catheter is above 50% at 1 hour. Methods The aim of this pilot study was to verify the effi cacy of a cyanoacrylate glue in reducing the risk of early bleeding at the exit site after CVC or PICC placement. We studied a group of adult patients consecutively undergoing placement of polyurethane CVCs or PICCs without reverse tapering in a non-intensive ward of our hospital. All lines were inserted according to the same protocol, which included 2% chlorhexidine antisepsis, maximal sterile barriers, ultrasound guidance, EKG guidance and securement with sutureless device. Two minutes after placement of the glue, the exit site was covered with a temporary gauze dressing, which was replaced by transparent membrane at 24 hours. All patients were assessed at 1 hour and at 24 hours. Results In 65 consecutive patients (45 PICCs, 11 dialysis catheters and nine CVCs), there was no signifi cant local bleeding at 1 hour or at 24 hours after catheter placement. No local adverse reaction occurred.
No damage to the polyurethane of the catheters was detected. Conclusion Glue is an inexpensive and highly eff ective tool for avoiding the risk of early bleeding of the exit site after catheter placement. We also suggest that in the next future the glue might prove to have benefi cial collateral eff ects on the risk of extraluminal contamination (by reducing the entrance of bacteria in the space between the catheter and the skin), as well as on the risk of dislocation (by increasing the stability of the catheter inside the skin breach). Introduction About 10 years ago the use of chest radiographs as the golden standard to ensure correct positioning of central venous catheters (CVC) was questioned. The frequent use of CVCs was also challenged. We decided to retrospectively evaluate our routines in a large surgical unit in a Swedish university hospital.
Methods All X-rays were centrally registered. Chest X-ray performed in our unit is almost entirely used to confi rm CVC positioning. The Certofi x CVC set for the Seldinger technique in combination with Certodyn -Universaladapter (B Braun, Germany) is now used as the routine equipment and the right jugular vein is our standard approach.
In 2002 the total number of X-rays performed in patients at our unit was 2,306, which corresponds to the approximate number of inserted CVCs at that time, since a confi rmatory X-ray was routine. X-rays were rarely performed on other indications in our unit. X-ray costs were at that time approximately €300,000 (~€130/each). The year after, 1,726 chest X-rays were performed, refl ecting both the use of intracardiac confi rmation of correct CVC position and also a reduced use of CVCs. This trend has continued over time. In 2011 approximately 600 CVCs were inserted at our unit. X-rays were performed in about 20% of these cases. The cost for a chest X-ray is today ~€200, meaning that X-ray costs were approximately €24,000. We have not experienced any medical problems when intracardiac ECG was used for positioning confi rmation. On the contrary, aspiration of venous blood without apparent p-waves in a patient with sinus rhythm may suggest improper placement of the CVC; for example, the right brachial vein. Conclusion If we had continued to use CVCs at the same frequency as we did 10 years ago, and used X-ray confi rmation in practically all cases, we would have paid approximately €460,000 annually. Reduced use of CVCs, in combination with intracardiac confi rmation of CVC positioning, has not only allowed us to reduce costs associated with CVC insertion by more than €400,000, corresponding to a reduction rate of more than 90%, but also decreased the patient's exposure to X-ray irradiation.
Introduction In cases of arrhythmia, the beat-to-beat variation of arterial pressure (AP) may impair the accuracy of automated cuff measurements. Indeed, this oscillometric device relies on the detection of arterial wall oscillations. Our aim was to determine, in ICU patients, whether brachial cuff measurements are really less reliable during arrhythmia than during regular rhythm.
Methods Patients with arrhythmia and carrying an intra-arterial catheter were prospectively and consecutively included in this multicenter study. After each arrhythmic inclusion, a regular rhythm patient was included. A second inclusion was possible in case of change in the cardiac rhythm. Three pairs of invasive and brachial cuff (Philips® MP70 monitor) measurements of mean arterial pressure (MAP) were respectively averaged. Some patients underwent a second set of measurements, after a cardiovascular intervention (passive leg raising, volume expansion, initiation of/increase in catecholamine infusion) allowing the assessment of MAP changes. Introduction Signifi cant changes in haemodynamics occur after brain stem death (BSD) and there is evidence that yield of transplantable organs may be decreased in donors who remain preload responsive prior to donation [1] , suggesting that optimisation of the cardiac output (CO) may be benefi cial in potential organ donors. We describe current UK practice with regard to CO monitoring in this group. Methods We reviewed a database of 287 brain-stem-dead potential organ donors collected by specialist nurses in organ donation (SN-OD) over a 6-month period (30 April 2011 to 31 October 2011) across multiple UK centres. The database contained data on donor management in the period from initial SN-OD review to immediately prior to transfer to the operating theatre. We analysed data on CO monitoring and vasopressor/inotrope use. Where information was missing/not recorded in the dataset, the treatment referred to was interpreted as not given/not done.
Fifty-three patients (18.5%) had evidence of CO monitoring. LiDCO was the most popular method ( Figure 1 ). A total of 264 (94%) patients received treatment with vasopressors and/or inotropes. CO data were utilised in a variety of ways ( Figure 2 ). Conclusion The majority of potential donors require vasopressors and/or inotropes post BSD, but it seems only a minority currently have their CO monitored. There is variation in how CO data are utilised to direct haemodynamic management. We welcome the development of standardised bundle-driven donor management. Reference 
The indocyanine green plasma disappearance rate (ICG-PDR) is a dynamic liver function test that can be non-invasively measured by pulse densitometry. ICG-PDR is associated with mortality and other markers of outcome. Due to predominant use of ICG-PDR in the ICU setting, the normal range is based on scarce data available outside the ICU and given with 18 to 25%/minute. Methods To prospectively re-evaluate the normal range and to analyze the potential impact of biometric data on ICG-PDR, we measured ICG-PDR (i.v. injection of 0.25 mg/kg ICG; LiMON, Pulsion, Munich, Introduction Mixed venous oxygen saturation (SVO 2 ) represents a well-recognized parameter of oxygen delivery (DO 2 )-consumption (VO 2 ) mismatch and its use has been advocated in critically ill patients in order to guide hemodynamic resuscitation [1] and oxygen delivery optimization. Nevertheless, the pulmonary artery catheter (PAC) is not readily available and its use is not devoid of risks. Furthermore, its use has been decreasing in recent years in surgical and cardiac surgical patients as the benefi t of guiding therapy with this device is unclear [2] [3] [4] . Central venous oxygen saturation (ScVO 2 ) has been suggested as an alternative to SVO 2 monitoring due to its feasibility in several settings. Unfortunately concerns arise from its capability to correlate with SVO 2 , the relationship being infl uenced by several factors, such as hemodynamic impairment and pathological process. Hemodynamic instability and shock often complicate cardiac surgery, and the SVO 2 -ScVO 2 relationship has not been specifi cally investigated in this setting. The aim of this study is to compare SVO 2 and ScVO 2 values in patients with cardiogenic shock after cardiac surgery. Methods A prospective observational study was designed and conducted. Inclusion criteria were: patients who had underwent elective or urgent/emergent cardiac surgery, with cardiac index (CI) <2.5 l/minute/m 2 estimated by means of a PAC, left ventricle ejection fraction (LVEF) <40%, lactate >2 mmol/l, age >18 years. A central venous catheter (CVC) and a PAC were inserted for each patient before surgery in the same right internal jugular vein in accordance with standard procedure. Proper position of the PAC was confi rmed with pressure tracings and chest X-ray. Mixed and central venous blood samples were collected from the distal ports of the PAC and CVC respectively 30 minutes after ICU admission, and every 6 hours for a total of three samples in a 24-hour period for each patient. All blood samples were analyzed by a co-oximeter (Radiometer ABL800 fl ex; Radiometer, Copenhagen, Denmark). Statistical analysis was performed by Stats Direct (Ver.2.5.8, Cheshire, UK) and GraphPad (Vers. Prism 4.0; San Diego, CA, USA). All data were tested for normal distribution with the Kolmogorov-Smirnov test. Statistical analysis was performed by linear regression analysis. The agreement between absolute values of ScvO 2 and SvO 2 were assessed by the mean bias and 95% limits of agreement (LOA) ((mean bias ± 1.96)×standard deviation) according to the method described by Bland and Altman [5] . Results A total of 20 patients were enrolled. In 18 out of 20 cases all three blood samples were collected. In two patients only two blood samples were drawn as they exited the inclusion criteria. Linear regression analysis between the two variables resulted in an r 2 of 0.708. Bland-Altman analysis ( Figure 1 ) for the pooled measurements of SvO 2 and ScvO 2 showed a mean bias and LOA of 6.82% (SD of bias 5.3) and -3.71 to +17.3% respectively. Conclusion ScVO 2 has been advocated as an attractive and simple indicator of DO 2 -VO 2 mismatch [6] . Its role as a surrogate of the wellestablished SVO 2 has been investigated in several settings, and it has been purposed in the hemodynamic resuscitation of critically ill septic patients [1] . Nevertheless, the SVO 2 -ScVO 2 relationship can be infl uenced by several factors due to ScVO 2 dependency from global blood fl ow redistribution that can occur during hemodynamic impairments. It has been shown previously that in healthy people ScVO 2 values tend to underestimate SVO 2 values, due to the higher oxygen content from inferior vena cava [7] . During circulatory shock, not homogeneous oxygen extraction and regional blood fl ow Methods We assessed the benefi t these TEE data provided in the assessment of fi ve domains: hypovolemia, right ventricular dysfunction, left ventricular dysfunction, sepsis, and valvular abnormality. Bedside practitioners listed their diagnoses before and after seeing primary TEE images perform by trained physicians. We used a 0 to 5 Likert scale to assess diff erential diagnosis before and after the TEE, comparing changes using a paired t test. Results All requests for TEE were to access hemodynamic instability. A total of 18 patients were screened and nine were eligible, in which 16 total TEE studies were performed. There were no complications with TEE and all patients tolerated the long-term placement of the probe well. Of the fi ve diagnostic domains studied, right ventricular failure was the most commonly underdiagnosed contributor to the hemodynamic instability among patients prior to TEE (P = 0.054) (Figures 1 and 2 ). Introduction Echocardiography is increasingly utilized by inten sive care physicians in everyday practice. Standardization of echocardiographic studies and reporting, quality assurance and medicolegal requirements necessitate establishment of a dedicated system within the critical care setting. We describe the process of setting up a critical care echocardiography (CCE) laboratory based on our experience from three separate ICUs. Methods A retrospective review and analysis of the process involved in establishment of echocardiography laboratories within ICUs.
Results Creating a CCE service involves a number of stages and takes several years to achieve. Major components include staffi ng, equipment, quality control, study archiving and networking capability. For staffi ng the objective is to identify and recruit staff with adequate training and expertise in CCE, providing 24/7 specialist cover in addition to supporting and training junior medical and nursing staff .
There is further a need to acquire funding for high-quality ultrasound machines and related hardware as well as long-term DICOM-based archiving and reporting systems. This should be based on projections of annual volumes of echo studies and corresponding digital storage. Networking connectivity is highly desirable, including obligatory back-up solutions and site allocations. A business case incorporating all the above should precede any development as identifi able funding sources and administrative approval are essential. The implementation stage requires the presence of a project leader who can organize the trialing of scanners, archiving, reporting and research systems, ensure compatibility with existing hospital and cardiology networks, and who can assist in individualizing archiving and reporting software refl ecting institutional and ICU specifi cs. Coordination with the IT department is very important. Clear contractual vendor obligations for service, maintenance and future upgrades of hardware and software need to be specifi ed. Training and credentialing of staff is best achieved within a systematic framework that includes ongoing competency review, education and QA programs. Partnership with cardiology may benefi t both groups. Major pitfalls are associated with poor initial training, lack of expertise and leadership, and bad vendor contracts. Conclusion Establishment of a CCE laboratory requires careful planning, and allocation of adequate human and fi nancial resources. Many potential problems can be identifi ed and prevented in advance. Strong expert leadership plays an important role. Introduction Contrast-enhanced ultrasonography (CEUS) is a dynamic digital ultrasound-based imaging technique, which allows quantifi cation of the microvascularisation up to the capillary vessels. As a novel method for assessment of tissue perfusion it is ideally designed for use in the ICU. CEUS is cost-eff ective and safe and can be repeatedly performed at the bedside without radiation and nephrotoxicity. Critical Care 2013, Volume 17 Suppl 2 http://ccforum.com/supplements/17/S2
Methods The frequency of CEUS use in the multidisciplinary surgical ICU was retrospectively evaluated for the period from 1 September 2011 to 1 September 2012. Furthermore, contributions of this novel method to the management of critically ill ICU patients as well as its accuracy were assessed. Results In total, 33 CEUS studies were performed in critically ill ICU patients. The most frequent indications included: assessment of the liver perfusion, assessment of the pancreas and kidney perfusion after pancreas and kidney transplantation, assessment of the renal perfusion in acute kidney injury (AKI), assessment of active bleeding and assessment of the bowel perfusion. In all studies, the correct diagnosis was achieved and the transport of critically ill patients to the radiology department for further diagnostic procedures as well as application of iodinated contrast agents was avoided. In 16 cases signifi cant new fi ndings were detected. Twelve of them were missed by conventional standard Doppler ultrasound prior to CEUS. In assessment of seven cases with AKI, impaired or delayed perfusion and microcirculation of the kidney was observed in six patients. In three patients urgent surgical intervention was performed because of CEUS results. In three cases active bleeding was excluded at the bedside due to absence of contrast agent extravasation into hematoma (thigh and perihepatic) or into abdominal cavity, without need for complementary CT imaging or angiography. In one case the regular perfusion of intestinal anastomosis was confi rmed with no need for surgical exploration. None of patients undergoing CEUS manifested any adverse reactions or developed any complications associated with the imaging technique. Conclusion Contrast-enhanced ultrasonography clearly improves visualization of the perfusion in various tissues. It is very likely to be superior to standard Doppler ultrasound, and is safe and well tolerated in critically ill patients. Promising indications for the use of CEUS in the ICU may be the assessment of kidney microcirculation and assessment of liver perfusion in liver transplant and liver trauma patients. Introduction Even though invasive hemodynamic devices are usually used for assessment of septic shock victims, they cannot evaluate the heart function. LV dysfunction as well as right heart syndrome are not uncommon in sepsis and critical patients. Intensive care ultrasound discloses these data and leads to appropriate treatment. Methods The study was a prospective cross-sectional study. The measurement was performed within 24 hours of ICU admission. We excluded patients with history of COPD and pulmonary hypertension from any diseases. Only good-quality images acquired from subjects were included for analysis. The primary objective was to disclose how the hemodynamic changed in septic patients by ICU-US. Introduction Thermodilution (TD) is considered a gold standard for measurement of cardiac index (CI) in critically ill patients. The aim of this study is to compare intermittent bolus TD CI with intermittent automatic calibration CI (AutoCI) and two continuous CIs obtained by pulse contour analysis with PiCCO 2 (PiCCI) and Pulsiofl ex (PuCCI). Methods Interim results of an ongoing prospective multicentre study in 53 patients. Age 58.7 ± 15.4, SAPS II score 51.4 ± 14.7 and SOFA score 10 ± 3.2. All patients underwent PiCCO monitoring via a femoral line whilst a radial line was kept in place during four 8-hour time periods (in the fi rst two periods, the Pulsiofl ex was connected to a radial line; in the last two it was connected to a femoral line). In the fi rst and third periods, the Pulsiofl ex was calibrated with TDCI, for the second and fourth periods Pulsiofl ex was calibrated with AutoCI. Simultaneous PiCCI and PuCCI measurements were obtained every 2 hours while simultaneous TDCI and AutoCI were obtained every 8 hours. We also looked at the eff ects of 40 interventions.
In total, 940 CCI and 382 TDCI values were obtained: 940 paired PiCCI and PuCCI; 358 paired AutoCI-TDCI measurements. TDCI values ranged from 1.5 to 6.9 l/minute/m 2 (mean 3.6 ± 1.1), AutoCI from 1.8 to 7.2 (3.6 ± 0.9), PiCCI from 1.0 to 7.1 (3.5 ± 1.1) and PuCCI from 1.3 to 7.6 (3.6 ± 1). Pearson's correlation coeffi cient comparing mean PuCCI and PiCCI values per patient had an R 2 of 0.79. Comparison between AutoCI and TDCI had an R 2 of 0.51. Changes in AutoCI correlated well with changes in TDCI (R 2 = 0.44, concordance coeffi cient = 95.7), as did changes in PuCCI versus changes in PiCCI (R 2 = 0.99, CC = 93.4%). Changes in PiCCI and PuCCI induced by an intervention correlated well with each other (R 2 = 0.86, CC = 100%). The percentage error (PE) obtained by Bland and Altman analysis and R 2 for the diff erent comparisons are presented in Table 1 .
The preliminary results indicate that in unstable critically ill patients, CI can be reliably monitored with Pulsiofl ex technology via a femoral line. Pulsiofl ex was also able to keep track of changes in CI.
Interim results of an ongoing study on the use of non-invasive hemodynamic monitoring with Nexfi n in critically ill patients Introduction Perioperative goal-directed therapy (pGDT) can substantially improve the outcome of high-risk surgical patients [1] . But the approach needs an initial investment and increases the staff workload. Economic factors might participate in the weak adherence to the pGDT concept. Some model studies support pGDT cost-eff ectiveness, but real economic data based on a recent clinical trial are lacking. We performed an economic analysis of hemodynamic optimization using the stroke volume variation trial [2] in order to elucidate this issue. Methods The hospital care invoices of all 120 patients included in the trial were retrospectively extracted. Due to the nature of the data we have adopted the healthcare payer's perspective. We performed a comparison of induced costs between the Vigileo (n = 60) and Control (n = 60) groups and constructed a cost tree using the study group and complications occurrence as distributive parameters. The incremental cost-eff ectiveness ratio per complication avoided was calculated and, fi nally, diff erent reimbursing categories were assessed as potential cost drivers.
Results A decreased rate (18 vs. 35 patients) and number of complications (34 vs. 78) were observed in the original trials Vigileo group. The mean cost of intervened patient was lower (€2,877 ± 2,336 vs. €3,371 ± 3,238; P = 0.38). According to the cost-tree analysis, patients with complications (n = 53; 44%) consumed signifi cantly more resources (€235,623; 63%). A gain of €634 per avoided complications confi rms that the lower complications rate was the most important cost driver. Both the clinical care for patients costs (€505 vs. 912; P = 0.04) and ward stay costs (€244 vs. 402; P = 0.03) were decreased by the intervention. On the contrary, the intervention increased anaesthesia costs (€880 vs. 688; P = 0.001).
Conclusion Intraoperative fl uid optimization with the use of stroke volume variation and the Vigileo/FloTrac system showed not only a substantial improvement of morbidity, but was also associated with an economic benefi t. This observed benefi t highly exceed the increased monitoring costs in our trial.
Introduction Hemodynamic monitoring is important in high-risk surgical patients in order to detect and correct circulatory instability, thereby improving outcome [1] . The extravascular lung water index (EVLWI) refl ects pulmonary edema [2] . The new EV1000/VolumeView (Edwards Lifesciences) can accurately measure EVLWI corrected for the actual volume of lung parenchyma (EVLWIc). The aim of our study is to prove a stronger correlation between EVLWIc and PaO 2 /FiO 2 compared with EVLWI in patients undergoing pulmonary resection. Methods A prospective observational study. Seven patients with lung cancer undergoing pulmonary resection were monitored using the EV1000 plathform. EVLWI was assessed by thermodilution at the following time points: after intubation (t1); during single-lung ventilation (t2); after lung resection (t3); after ICU admission (t4); 12 hours (t5) and 18 hours after ICU admission (t6). EVLWIc values were also collected at t3 and t4. PaO 2 /FiO 2 was measured at the same time points.
Results No signifi cant correlation was found between EVLWI and PaO 2 / FiO 2 (r = -0.3124, P >0.05), while a signifi cant correlation was seen between EVLWIc and PaO 2 /FiO 2 (r = -0.528, P =0.009; Figure 1 ).
Conclusion Despite the small sample size, this study shows that in patients undergoing pulmonary resection the EVLWIc is more strongly correlated to PaO 2 /FiO 2 than EVLWI. Therefore, the EV1000 may be a valuable tool for more reliable hemodynamic monitoring in this subgroup of patients. References or extracardiac arteriopathy) were allocated to GDT or conventional hemodynamic therapy. We excluded patients with endocarditis, previous use of dobutamine, need for IABP, high dose of vasopressors and emergency surgery. The GDT protocol involved hemodynamic resuscitation aimed at a target of a cardiac index >3 l/minute/m 2 through a three-step approach: fl uid therapy of 250 ml lactated Ringer's solution, dobutamine infusion up to a dose of 20 μg/kg/minute, and red blood cell transfusion to reach a hematocrit level above 28%. Results Twenty patients from the GDT group were compared with 20 control patients. Both groups were comparable concerning baseline characteristics and severity scores, except for a higher prevalence of hypertension and heart failure in the GDT group. Intraoperative data showed no diff erence regarding length of extracorporeal circulation, fl uid balance, transfusion or inotropic requirement. Patients from the GDT group were given more fl uids within the fi rst 8 hours as compared with the conventional group (1,250 ml vs. 500 ml, P <0.001). GDT patients showed a median ICU stay of 3 days (95% CI: 3 to 4) compared with 5 days in control patients (95% CI: 4 to 7). Moreover, hospital stay was less prolonged in GDT patients (10 days vs. 14 days, P = 0.043 Methods Sixteen patients were divided into two groups: one group was treated with a restrictive approach (≤8 ml/kg/hour), and the other with a liberal approach (> 8 ml/kg/hour). Patients were randomly allocated using sealed envelopes. During the thoracic part of the surgical procedure, all patients received one-lung ventilation (OLV).
In the group treated with a restrictive volume approach, patients received fl uids at the rate of 7.0 ± 1.0 ml/kg/hour. PaO 2 /FiO 2 was 288 ± 14 after intubation and 270 ± 22 before extubation. In the group treated with a liberal volume approach, fl uids were replaced at 11.0 ± 2.0 ml/kg/hour. PaO 2 /FiO 2 was 259 ± 24 after intubation and 223 ± 43 before extubation. Surgery combined with OLV was found to signifi cantly aff ect the PaO 2 /FiO 2 value (ANOVA, F 1,14 = 15.85a, P = 0.001, partial η 2 = 0.531). The average PaO 2 /FiO 2 level was signifi cantly higher in the restrictive-replacement group than in the liberal-replacement group (ANOVA, F 1,14 = 9.66, P = 0.008, partial η 2 = 0.408). There was no interaction between the groups (ANOVA, F 1,14 = 1.7a, P = 0.215, partial η 2 =0.108). Mean length of stay in the ICU was similar between the restrictive-replacement group (5.2 ± 2.3 days) and the liberalreplacement group (6.3 ± 1.6 days) (ANOVA, F 1,14 = 0.814a, P = 0.382, partial η 2 = 0.055).
Conclusion Results from this small sample indicate that esophageal carcinoma surgery by itself had a detrimental eff ect on the PaO 2 /FiO 2 value, which restriction of perioperative volume did not signifi cantly aff ect. Volume restriction also did not aff ect length of stay in the ICU. 
We hypothesized that goal-directed therapy (GDT) is not associated with an increased risk of cardiac complications in high-risk, noncardiac surgical patients. Patients with limited cardiopulmonary reserve are at risk of mortality and morbidity after major surgery [1] . Augmentation of the oxygen delivery index (DO 2 I) with a combination of intravenous fl uids and inotropes (GDT) has been shown to reduce the postoperative mortality and morbidity in high-risk patients [2] . However, concerns regarding cardiac complications associated with fl uid challenges and inotropes used to augment cardiac output may deter clinicians from instituting early GDT in the very patients who are more likely to benefi t. Methods Systematic search of MEDLINE, Embase and CENTRAL databases for randomized controlled trials of GDT in high-risk surgical patients. Studies including cardiac surgery, trauma, and pediatric surgery were excluded to minimize heterogeneity. We reviewed the rates of all cardiac complications, arrhythmias, acute myocardial ischemia, and acute pulmonary edema. Meta-analyses were performed and forest plots drawn using RevMan software. Data are presented as odd ratios (ORs) (95% CIs), and P values. and compared with those calculated with the echocardiographic standard formulation (stroke volume = cross-sectional area×velocity time integral; COECHO = SV×heart rate). In every patient CO was measured twice: at baseline (T1) and after volume loading (500 ml lactate Ringer solution) (T2). Agreements between COVIG, COMC, and COECHO were evaluated by means of simple linear regression (r 2 ) and Bland-Altman analysis.
Results Twenty patients were enrolled in the study. Values of r 2 , bias and limit of agreement at T1 and T2 are summarized in Table 1 . CO values ranged from 3.9 and 8.6 l/minute (echo), from 3.4 to 9.9 (Vigileo) and from 4 to 8.3 (MostCare); the Pearson's and Bland-Altman methods showed poor agreement between COECHO and COVIG, demonstrating a tendency to overestimation (see Figure 1 ). The percentage of error (PE) was 51.7% at T1 and 49.3% at T2. On the contrary, MostCare measures showed good agreement with echocardiography (see Table 1 ) with a PE of 22.4% at T1 and of 17% at T2. Conclusion Vigileo did not prove to be a substitute to the reference system; pre-loaded data, necessary for vascular impedance estimation, may be one of the main limitations that made Vigileo measurements less accurate than the MostCare ones. On the contrary, MostCare, an uncalibrated totally independent system, was shown to properly estimate the vascular impedance in these hemodynamically stable patients. Further comparisons in unstable conditions are needed to confi rm our observations. References Previous studies have found an association between severity of acute infl ammatory states and increased arterial stiff ness but it is not known whether non-invasive pulse waveform analysis could predict development of multiple organ failure in septic patients. The purpose of this study was to evaluate the photoplethysmographic brachial artery pulse wave transit time and augmentation index and their changes in response to induced forearm ischemia in septic ICU patients and correlate these indices to the development of subsequent end organ damage.
Methods A prospective observational study in patients with sepsis within 24 hours of admission. Severity of sepsis was assessed with APACHE II score (median 18.5) and SOFA score (median 7.5). Threeminute signal recording was done concurrently from the brachial artery at the elbow and the radial artery at the wrist with an originally designed photoplethysmograph at rest and after 5 minutes of induced forearm ischemia. Recordings were analyzed to obtain the pulse wave transit time and augmentation index at rest and 60 seconds after induced ischemia. The SOFA score was recalculated at 48 hours post recording.
Results We studied 14 consecutive general ICU patients. There was a negative linear relationship between the pulse wave transit time (median 22.6 ms) at rest and increase in SOFA score in 48 hours (P = 0.02, r = 0.96). The postischemic pulse wave transit time increased in all patients (median 25.7 ms) but no association was found between the proportion of increase and subsequent change in SOFA. Correlation between rest (median 7.6) and postischemic (median 7.2) augmentation index and 48-hour SOFA scores was not statistically signifi cant (r = 0.57, P = 0.46). Conclusion This study indicates that in early sepsis pulse waveform characteristics could predict the risk of developing end organ failure. The pulse wave transit time is more robust than the augmentation index and could be easier to use in patients with poor perfusion. Vascular reactivity indices do not seem to have predictive value in this context. Reference 
In clinical practice, blood volumes (BV) are typically measured by thermodilution. Recently, contrast-enhanced ultrasound (CEUS) has been proposed as an alternative minimally invasive approach for BV assessment [1] . This method measures BV using a single peripheral injection of a small bolus of ultrasound contrast agent (UCA) detected by an ultrasound scanner. By measuring the acoustic backscatter, two indicator dilution curves (IDCs) can be derived from two diff erent sites in the circulatory system. IDC analysis permits deriving the mean transit time (MTT) the bolus takes to travel between the injection site and two measurement sites. Assessment of the BV between these sites is obtained by multiplying the diff erence in MTT (ΔMTT) by the blood fl ow. In this study, we compare diff erent volumes in an in vitro set-up by CEUS with true set-up volumes and thermodilution acquired volumes. Methods The in vitro set-up consisted of a centrifugal pump, a network of tubes with variable volumes, an electromagnetic fl owmeter to measure and adjust the generated fl ow, heating devices to maintain constant temperature (37°C), two thermistors for thermodilution measurement, an ultrasound transducer and a pressure stabilizer. A small bolus of UCA diluted in cold saline (1 mg SonoVue® in 20 ml saline at 4°C) was injected into the system. The cold UCA passage through a fi rst and a second region of interest (ROI) was measured simultaneously with the ultrasound transducer and the thermistors. The measurements were performed at diff erent fl ows and volumes. BVs were estimated using the two diff erent approaches, namely CEUS and thermodilution. The IDCs were processed and fi tted separately with a dedicated model to estimate the ΔMTT of the cold UCA bolus between the two ROIs and the two thermistors. All the measurements were repeated three times. Results A linear relation between BVs estimated by the two techniques was observed with a correlation coeffi cient of 0.94. The bias of CEUS with respect to the true volumes was -40.1 ml; the bias of thermodilution was 84.3 ml. The most prominent diff erences between the two techniques were observed in case of high volume and low fl ow, possibly due to diff erent transport kinetics between UCAs and heat.
The use of cardiac output monitoring has been shown to be benefi cial in the setting of perioperative medicine and critical illness [1, 2] . More recently, its application in the setting of major trauma has been described [3] . Here, we describe our preliminary experience of embedding bioreactance fl ow monitoring within the major trauma primary survey of severely injured patients and the subsequent eff ect on patient management. Methods Institutional ethical approval was obtained. Intubated major trauma patients were sequentially enrolled. Exclusions included major thoracic burns and children. Bioreactance fl ow monitoring (NICOM; Cheetah) was applied at the same time as ECG leads and the calibration step performed during handover from the prehospital team. Time to availability of oxygen delivery data was recorded and trauma team members surveyed regarding for perceived benefi ts and concerns from this monitoring. The infl uence of fl ow monitoring on fl uid resuscitation, time to CT and defi nitive disposal (to OR/ICU) was measured and compared with a control population matched for injury severity score, age and sex.
Results Cardiac index was available at mean 10.6 minutes (median 9 minutes; SD 3.9), fl uid responsiveness at mean 35.9 minutes (median 35; SD 11.3) and oxygen delivery calculation at mean 25.3 minutes (median 25; SD 7.7). Passive leg raise was not performed in 63% of patients due to concerns about pelvic or brain injury. Volume of fl uid infused (mean 738 vs. 925 ml; P = 0.124), time to CT (mean 57.4 vs. 68.8 minutes; P = 0.08), and time to defi nitive disposal (mean 124.9 vs. 146.1 minute; P = 0.069) were all reduced in the fl ow monitored group, although not signifi cantly diff erent when compared with a matched control group (Mann-Whitney U rank sum). Eighty-four percent of trauma team members surveyed felt the fl ow monitoring data to be useful, and only 11% felt it may impair clinical management. Conclusion Cardiac index, fl uid responsiveness and oxygen delivery data can be obtained inform a primary survey. Rather than introducing delays, the use of fl ow monitoring was associated with a trend towards decreased time to imaging; less fl uid use pre-damage control point and reduced time to defi nitive disposal. Further research is required to confi rm benefi ts and mechanism. References Introduction Pulse pressure variation (PPV) is a dynamic indicator of fl uid responsiveness, which is known to have a low sensibility and specifi city in patients ventilated in pressure support (PS) [1] . We aim to investigate patient-ventilator asynchrony as a potential source of hemodynamic interference in PS.
Methods We performed a prospective study including PS ventilated patients who met inclusion criteria for fl uid depletion [1] . Patients who showed an asynchrony index (AI) exceeding 10% were included in the asynchrony group (AG). The remaining patients were included in the synchrony group (SG) [2] . Beat-to-beat hemodynamic variables were recorded through PRAM (Mostcare; Vytech Health srl, Padova, Italy). PPV cutoff of 13% was used to identify fl uid responders/nonresponders. A fl uid challenge of 500 ml normal saline was given in 5 minutes. An increase of 15% of cardiac index after 10 minutes indicated fl uid responsiveness. Results So far, eights patients showed an AI >10% while 16 did not.
Overall sensitivity was 28.6% versus 50% in SG; overall specifi city was 76.5% versus 91.7% in AG. Overall Cohen's k was 33.3% versus 61.2% in AG (see Figure 1 ). However, because none of the responders in the AG group was detected by PPV, statistical analysis was not feasible within this subgroup. 
The mini-fl uid challenge is a widely used strategy to manage fl uid loading in the ICU and OR. Although it might be a rational strategy, data on the mini-fl uid challenge and its reliability are very limited. We investigated the value of changes in pulse contour cardiac output as a result of a mini-fl uid challenge of 50 and 100 ml to predict fl uid loading responsiveness. Methods We measured the eff ects after the administration of 50, 100 and 500 ml bolus colloid infusions on CO (Modelfl ow (COm) and LiDCO (COli)), CVP and MAP in 21 patients on mechanical ventilation after elective cardiothoracic surgery. From the data we analysed the smallest volume that was predictive for the eff ects of 500 ml on cardiac output. Results COli and COm increased after 50, 100 and 500 ml fl uid loading.
Best results are observed for changes in COm after 100 ml fl uid loading (area under the ROC 0.86, 95% CI between 0.65 and 1.00). A change in Modelfl ow CO of at least 4.3% has a sensitivity of 67% and a specifi city of 100% after 100 ml fl uid loading. Sensitivity is 60% and specifi city 83% for a similar cutoff in CO measured with the LiDCO device after 100 ml fl uid loading. In our patient population, MAP and COli did not predict responsiveness with more accuracy than mathematical chance. See Figure 1 .
Conclusion Changes in pulse contour CO can be used in a mini-fl uid challenge to assess fl uid responsiveness in our postcardiac surgery patients.
Introduction Fluid responsiveness is defi ned based on an arbitrary increase of cardiac output (CO) or stroke volume (SV) of 10 to 15%. We hypothesise that the variation of heart effi ciency (Eh) and the slope (S) defi ned by the relative increase of CO over the relative increase of mean fi lling pressure (Pmsa) can be used as alternative defi nitions of fl uid responsiveness. Introduction Fluid overload is associated with poor outcome in the critically ill. Thus, an accurate predictor of a positive haemodynamic response (increase in stroke volume) to fl uid challenge is vital.
Methods We studied the predictive value (positive response defi ned as change in stroke volume >15% after 10 ml/kg fl uid bolus) of a range of haemodynamic variables: static (CVP, active circulating volume, central blood volume, total end diastolic volume), dynamic (systolic pressure variation, stroke volume variation) and contactility (dp/dt), in a group of 100 ventilated children (median weight 10 kg). Variables were measured using transpulmonary ultrasound dilution and PRAM (an arterial pulse contour method).
We performed 168 paired measurements (pre-fl uid and postfl uid challenge), with a SV response rate of 45%. Overall predictive values were poor, but slightly better for static versus dynamic variables (Table 1) . When SV response was analysed as a continuous variable, the two predictive multivariable variables were change in TEDVI and baseline dp/dt (r 2 = 0.30, both P <0.001). Conclusion The predictive ability for typical static and dynamic haemodynamic variables, when taken in isolation, is poor. However, improved prediction is seen when baseline contractility is taken into account. pressure (MAP)-guided fl uid therapy on microcirculatory perfusion in patients undergoing abdominal surgery. Methods Patients undergoing elective abdominal surgery were randomized into a PPV/CI-guided group (n = 11) or a MAP-guided (n = 12) group. PPV, CI and MAP were measured using the non-invasive fi nger arterial blood pressure measurement device ccNexfi n (Edwards Lifesciences BMEYE, Amsterdam, the Netherlands). Tidal volumes were ≥8 ml/kg with PEEP ≥8 mmHg. In both groups, MAP of 70 mmHg was maintained. In the PPV/CI group, an intraoperative algorithm was used keeping the PPV under 12% and CI above 2.5 l/minute/ m 2 using fl uid therapy and dobutamine and noradrenaline infusion, respectively. Sublingual microvascular perfusion was measured after anesthesia induction, and every subsequent hour using sidestream dark-fi eld imaging (Microscan; Microvision Medical, Amsterdam, the Netherlands). The perfused small vessel density (PVD) values were offl ine quantifi ed.
The fi rst hour during surgery, the PPV/CI-guided group tended to receive more fl uids than the MAP-guided group (1,014 ± 501ml vs. 629 ± 463 ml; P = 0.07). At this time point, the PVD was slightly lower in the PPV/CI-guided group (16.7 ± 3.1 mm/mm 2 ) when compared with the MAP-guided group (17.9 ± 3.9 mm/mm 2 ; P = 0.41). In both groups the PVD remained stable during the fi rst 2 hours of surgery. However, 2 hours after the start of surgery, the PVD in the PPV/CI group restored and tended to be higher than in the MAP-guided group (21.1 ± 1.9 vs. 18.1 ± 3.4 mm/mm 2 ; P = 0.09). After 1 hour of surgery, the administered fl uid volume correlated inversely with PVD (r = -0.59, P = 0.011).
Conclusion Goal-directed fl uid management resulted in a higher administered fl uid volume in the beginning of surgery, and this was associated with a slightly reduced microcirculatory perfusion when compared with MAP-guided fl uid management. Microcirculatory perfusion tended to improve as surgery progressed in the goal-directed fl uid therapy group. Our fi ndings suggest that goal-directed and MAPguided fl uid management are associated with distinct patterns in fl uid resuscitation, which may be of consequence for microvascular perfusion. Introduction Previous studies demonstrate that loss of glycocalyx integrity is associated with impaired microvascular function. We investigated whether glycocalyx dimensions are reduced in patients undergoing cardiac surgery with or without cardiopulmonary bypass (CPB), and are paralleled by loss of microcirculatory perfusion using in vivo microcirculation measurements. Methods Patients undergoing on-pump surgery with nonpulsatile (n = 11) or pulsatile (n = 13) CPB or off -pump surgery (n = 13) underwent sublingual sidestream dark-fi eld imaging at baseline, during coronary grafting and upon ICU admission to assess perfused microvascular vessel density. Glycocalyx integrity was evaluated using the GlycoCheck Measurement Software, and expressed as the perfused boundary region (PBR). An increase in PBR represents deeper penetration of erythrocytes into the glycocalyx, and is indicative for compromised glycocalyx thickness. Introduction Cold exposure can be adapted for exercise or therapeutic purposes, but its impact on microcirculation in healthy humans has not been well defi ned. We hypothesize that whole body cold stress may impair microcirculation. Methods Seven volunteers were recruited for the water immersion procedure. During the cooling protocol the volunteers every 20 minutes of immersion were asked to step out from the bath and rest for 10 minutes in a room environment and then return to the water bath for the next 20 minutes of immersion. This head-out immersion procedure in bath water at 14°C continued until the rectal temperature was dropped to 35.5°C or the time of 180 minutes was terminated. Maximum cold water immersion time was 120 minutes. Before, at the end of whole body cooling and 1 hour after cooling was ended, systemic hemodynamics and direct in vivo observation of the sublingual microcirculation were obtained with sidestream dark-fi eld imaging. Assessment of microcirculatory parameters of convective oxygen transport (microvascular fl ow index (MFI), proportion of perfused vessels (PPV)), and diff usion distance (perfused vessel density (PVD) and total vessel density (TVD)) was done using a semiquantitative method.
Results During cooling and 1 hour after cooling was ended, a signifi cant increase in cardiac output (P = 0.028 and P = 0.043) was observed, but there were no changes in heart rate or mean arterial pressure in comparison with baseline variables. There were no signifi cant changes in PPV, MFI, PVD and TVD of small vessels in comparison with baseline variables during all observational time. Conclusion Defined cold exposure had no effect on the microcirculation.
Introduction Vasodilation and increased skin blood fl ow (also sweating) are infl uential in heat dissipation during heat exposure and exercise. It is unclear how heat stress infl uences microcirculation. Side dark-fi eld imaging visualizes the blood fl ow at the capillary level and helps to assess perfusion heterogeneity. Clinical and experimental data show that the sublingual region is clinically relevant for detecting microcirculatory alterations and more represents central microcirculation than cutaneous perfusion. We hypothesize that whole body heat stress may increase capillary density. Methods Eight healthy men with no history of cold and/or heat injury were recruited to this study. Passive body heating was performed by continuous immersion up to the waist in the water bath at 44°C and continued until rectal temperature reached 39.5°C. Before, at the end of whole body heating and 1 hour after heating was ended, systemic hemodynamics and direct in vivo observation of the sublingual microcirculation were obtained with sidestream dark-fi eld imaging. Assessment of microcirculatory parameters of convective oxygen transport (microvascular fl ow index (MFI), proportion of perfused vessels (PPV)), and diff usion distance (perfused vessel density (PVD) and total vessel density (TVD)) was done using a semiquantitative method. Vessels were separated into large (mostly venules) and small (mostly capillaries) using a diameter cutoff value of 20 μm. Results Whole body heating resulted in signifi cantly increased heart rate (P = 0.012) and cardiac output (P = 0.046) in comparison with baseline variables. One hour after heating was ended, the heart rate Introduction Serial measurements of lactate over time may be a better prognosticator than a single lactate concentration [1] . Early lactateguided therapy also reduces ICU length of stay and ICU and hospital mortality [2] . This study aims to assess the prognostic value of the lactate clearance (LC) in the fi rst 24 hours in surgical patients.
Methods In a prospective cohort during 1 year, we followed consecutively enrolled patients admitted immediately postoperative to the surgical ICU of Hospital Santa Luzia, Brasília, Brazil. Patients were assigned to two groups: LC >10% and LC ≤10%. The primary outcome measure was mortality at 7 and 28 days. The secondary outcome included hospital and ICU length of stay (LOS). Results A total of 417 patients were followed. In total, 50.4% were male and 83% underwent elective surgery. The mean age was 59 ± 16, APACHE II score 8 ± 5, SAPS 2 26 ± 11. The mortality at 7 days was 0.95% (n = 4) and the mortality at 28 days was 2.15% (n = 9), respectively. Hospital mortality was 4.79% (n = 20). Sixty-one percent (n = 255) of the patients had LC >10% versus 39% (n = 162) with LC ≤10%. Those who had LC ≤10% were older (62 ± 16 vs. 57 ± 17, P = 0.00) and had greater APACHE II score (9 ± 6 vs. 7 ± 4, P = 0.00) and SAPS 2 (28 ± 12 vs. 25 ± 10, P = 0.02). There was no diff erence in ICU LOS (5 ± 12 vs. 4 ± 9 days, P = 0.54) and hospital LOS (10 ± 15 vs. 9 ± 11 days, P = 0.48). Initial lactate levels were lower in the group with LC ≤10% (1.1 ± 0.9 vs. 1.9 ± 1.6, P = 0.00); however, mean lactate was higher in 24 hours (2.0 ± 1.8 vs. 1.0 ± 0.7, P = 0.00). All of the patients who died in the fi rst 7 days had LC ≤10% (2.46%, n = 4, P = 0.02); this group also had a higher mortality at 28 days (4.32%, n = 7 vs. 0.78%, n = 2; P = 0.03). The relative risk for mortality LC ≤10% in 7 and 28 days was 1.02 (95% CI: 1.00 to 1.05) and 5.07 (95% CI: 1.17 to 27.09), respectively. Signifi cant diff erence was observed in the Kaplan-Meier survival curves for 7 and 28 days (P = 0.01 and 0.02, respectively). The sensibility of LC ≤10% was 100% (95% CI: 51 to 100%) for 7-day mortality and 78% (95% CI: 45 to 94%) for 28-day mortality. The specifi city was 62% (95% CI: 57 to 66%) for 7-day mortality and 62% (95% CI: 57 to 66%) for 28-day mortality. Conclusion Despite initial lactate levels, lactate clearance ≤10% proved to be a good predictor of mortality in 7 and 28 days in surgical patients admitted in the postoperative period to the ICU. References
Introduction The use of peripheral perfusion objective parameters to anticipate successful resuscitation in septic shock has been recently investigated [1] . The mottling score, a perfusion parameter used for decades, has been proposed to correlate with septic shock survival [2] , and was tested in this study as a clinical tool in predicting mortality. Methods A prospective observational study was conducted, with patients consecutively admitted to a tertiary hospital ICU in Brasília, Brazil. From July 2011 to May 2012, all patients diagnosed with septic shock were enrolled. Demographic data, diagnoses, shock origin and severity scores were recorded. After initial resuscitation, the score was registered in the fi rst 3 days by the same observer, considering the score on the lower limb without an arterial catheter, or the worst between the lower limbs, and the worst in the 3 days. Exclusion criteria were terminal illness with no intervention decision and incomplete Methods Pigs (20 to 30 kg) were randomized into one of the groups: Sham (n = 2), HS (n = 9), LR (3× volume bled; n = 9) or TERLI (2 mg bolus; n = 9). HS induced to target MAP of 40 mmHg was maintained for 30 minutes. Brain tissue oxygen pressure (PbtO 2 ), intracranial pressure (ICP), cerebral perfusion pressure (CPP), haemodynamics and blood gas analyses were assessed prior to HS (baseline) up to 120 minutes after treatment. Tissue markers of brain oedema (aquaporin-4 (AQP4) and Na-K-Cl cotransporter-1 (NKCC1)), apoptosis (pre-apoptotic protein (Bax)) and oxidative stress (thiobarbituric acid reactive substances (TBARS)) were also measured.
Results Sham animals had no signifi cant changes in the variables assessed. HS resulted in a signifi cant decrease in CPP (mean varied from 36 to 39 mmHg), PbtO 2 (from 23.6 to 26.6 mmHg), ICP (from 1 to 2 mmHg) and haemodynamics (MAP from 38 to 40 mmHg; CI from 1.8 to 2.1 l/minute/m 2 ), and a signifi cant increase in blood lactate (from 6.7 to 8.9 mmol/l) and cerebral AQP4 (mean ± SE; 167 ± 54% of sham), NKCC1 (237 ± 47% of sham), Bax (167 ± 44% of sham) and TBARS. Fluid resuscitation was followed by an increase in ICP (from 7 to 9 mmHg) and a decrease in CPP (from 41 to 52 mmHg), with an increased expression of cerebral AQP4 (210 ± 56% of sham), NKCC1 (163 ± 32% of sham) and Bax (137 ± 24% of sham Introduction Shock induces mitochondrial damage, which can lead to tissue injury and infl ammation. Resuscitative adjuncts to limit mitochondrial injury may be eff ective to reduce tissue injury and protect against the sequelae of hemorrhagic shock (HS). Others and we have demonstrated the protective eff ects of inhaled carbon monoxide (CO) or nebulized sodium nitrite (NaNO 2 ) in models of HS. Our aim was to test the hypothesis that CO and NaNO 2 protect against hemorrhagic shock-induced tissue injury/infl ammation by limiting mitochondrial damage and preventing bioenergetic failure. Methods Twenty anesthetized female Yorkshire pigs were subjected to severe hemorrhage until unable to compensate or 90 minutes, and were then resuscitated with volume/pressors. Muscle and platelet samples were obtained at baseline (BL) and 2 hours after resuscitation (EndObs). Animals were randomized to: standard of care (HSR, n = 5); HSR+CO (CO; 250 ppm×30 minutes, n = 6); or HSR+NaNO 2 (NaNO 2 ; 11 mg in PBS×30 minutes, n = 6), and sham (n = 3). CO or NaNO 2 were initiated ~30 minutes before resuscitation. Primary endpoints were changes in muscle and platelet mitochondrial respiration between BL and EndObs, quantifi ed by muscle respiratory control ratio (RCR, traditional respirometry), and by the change in proton-leak respiration (PLR) and mitochondrial reserve capacity in platelets. Secondary endpoint was mortality at EndObs. Results Skeletal muscle RCR decreased in the HSR group (P = 0.04) but not in sham. Decrease in RCR was primarily due to decreased ADPdependent respiration, without change in state 4 respiration. HSR also resulted in platelet mitochondrial dysfunction as demonstrated by increased PLR and decreased reserve capacity. This correlated with increased platelet activation (%CD62P+ by fl ow cytometry) in HSR. CO or NaNO 2 treatment prevented these deleterious changes in both muscle and platelet mitochondrial respiration, as well as limited HSR-induced platelet activation. CO treatment also improved reserve capacity compared with baseline. Mortality was higher in HSR than in CO or NaNO 2 (80 vs. 33 and 33%, respectively). Conclusion In severe HS, mitochondrial injury in platelets and muscle was limited by CO or NaNO 2 . Although not powered for a secondary endpoint, mortality was double in HSR versus adjunctive therapies. This suggests that CO and NaNO 2 may protect mitochondrial function by maintaining ATP-coupled respiration and reserve capacity, and that this may confer a survival advantage. However, further investigations are required. Introduction Norepinephrine has been widely used in septic shock. However, its eff ect remains controversial. We conduct a systematic review and meta-analysis to compare the eff ect between norepinephrine and other vasopressors. Methods The PubMed, Embase, and Cochrane Library databases from database inception until October 2012 were searched. We selected randomized controlled trials in adults with septic shock and compared norepinephrine with other vasopressors. The quality of each study included was assessed with Jadad score. After assessing for heterogeneity of treatment eff ect across trials using the I 2 statistic, we used a fi xed eff ect model (P ≥0.1) or random-eff ects model (P <0.1) and expressed results as the risk ratio (RR) for dichotomous outcomes or the standardized mean diff erence (SMD) for continuous data with 95% CI. Results Eighteen trials (n = 2,715) met inclusion criteria, which compared norepinephrine with fi ve diff erent vasopressors (dopamine, vasopressin, epinephrine, terlipressin and phenylephrine). The mean Jadad score was 3.11. Overall, there was no diff erence in mortality in the comparisons between norepinephrine and vasopressin, epinephrine, terlipressin and phenylephrine (P >0.05, respectively). However, norepinephrine had a trend in decreasing mortality compared with dopamine (RR, 0.84; 95% CI, 0.68 to 1.02; P = 0.08). There were a decreased heart rate (HR) (SMD, -2.10; 95% CI, -3.95 to -0.25; P = 0.03), cardiac index (SMD, -0.73; 95% CI, -1.14 to -0.03; P = 0.004) and an increased systemic vascular resistance index (SVRI) (SMD, 1.03; 95% CI, 0.61 to 1.45; P <0.0001) with the treatment of norepinephrine compared with dopamine. Conclusion There is not suffi cient evidence to prove that norepinephrine is superior to vasopressin, epinephrine, terlipressin and phenylephrine in terms of mortality. However, norepinephrine is associated with a decreased HR, cardiac index and an increased SVRI, and appears to have a greater eff ect on decreasing mortality compared with dopamine.
Introduction Vasoplegic syndrome is a common complication after cardiac surgery, with negative impact on patient outcomes and hospital costs. Pathogenesis of vasodilatory phenomenon after cardiac surgery remains a matter of controversy. Loss of vascular tone can be partly explained by the depletion of neurohypophyseal arginine vasopressin stores. Vasopressin is commonly used as an adjunct to catecholamines to support blood pressure in refractory septic shock, but its eff ect on vasoplegic shock is unknown. We hypothesized that the use of vasopressin would be more eff ective on treatment of shock after cardiac surgery than norepinephrine, decreasing the composite endpoint of mortality and severe morbidity. Methods In this prospective and randomized, double-blind trial, we assigned patients who had vasoplegic shock to receive either vasopressin (0.01 to 0.06 U/minute) or norepinephrine (0.01 to 1 μg/ kg/minute) in addition to open-label vasopressors. All vasopressor infusions were titrated and tapered according to protocols to maintain a target blood pressure. The primary endpoint was major morbidity according to STS (30-day mortality, mechanical ventilation >48 hours, mediastinitis, surgical re-exploration, stroke, acute renal failure). Secondary outcomes were time on mechanical ventilation, ICU and hospital stay, new infection, the time to attainment of hemodynamic stability, occurrence of adverse events and safety. Results A total of 300 patients underwent randomization, were infused with the study drug (148 patients received vasopressin, and 152 norepinephrine), and were included in the analysis. Patients who received vasopressin had a lower rate of morbidity (23.5% vs. 34%, P = 0.001) as compared with the norepinephrine group. The 30-day mortality rate was 6.1% in the norepinephrine group and 4.6% in the vasopressin group (P = 0.570). There were no signifi cant diff erences in the overall rates of serious adverse events (7.4% and 6.6%, respectively; P = 0.772). Results Patients in the two groups were statistically comparable with respect to sex (P = 0.31) and age (P = 0.53). The causes of the syndrome of Tako-tsubo were: subarachnoid hemorrhage (six patients) after coronary artery bypass graft (four patients), and polytrauma (two patients). All patients had low cardiac output. In the levosimendan group the ejection fraction at entrance was 25 ± 6%, after 12 hours 36 ± 10%, and 47 ± 5% after 24 hours. In the control group the ejection fraction at entrance was 24 ± 7%, after 12 hours 28 ± 6% and after 24 hours 33 ± 4%. Comparing the two groups we reached statistical signifi cance, P = 0.026. Conclusion Comparing the two groups, we noticed that both started from a low cardiac output. However, in the group who used the drug therapy based on levosimendan we saw a return of systolic function of the left ventricle to near-normal levels within 24 hours, while in the control group there remains a dysfunction in systolic function. We have shown the drug therapy based on levosimendan contributes to improving the systolic function of the left ventricle compared with treatment with dobutamine despite the initial cardiac stunning. Reference Introduction In the critically ill, the incidence of raised cardiac troponin T (cTnT) levels is high. Although the mechanisms of myocardial injury are not well understood, raised cTnT levels are associated with increased mortality. The aim of our study was to determine the incidence, prevalence and outcome of silent myocardial injury as determined by raised cTnT levels and concomitant ECG changes in critically ill patients admitted for noncardiac reasons. Methods ECGs were taken and cTnT was measured daily during the fi rst week and on alternate days during the second week until discharge from the ICU or death. After completion of the study, all cTnT levels and ECGs were analysed independently and patients were classifi ed into four groups: defi nite MI (cTnT ≥15 ng/l and defi nite ECG changes of MI), possible MI (cTnT ≥15 ng/l and ischaemic changes on ECG), troponin rise alone (cTnT ≥15 ng/l with no ischaemic ECG changes), or normal. All medical notes were reviewed independently by two ICU clinicians. Results A total of 144 patients were included in the analysis (42% female; mean age 61.9 (SD 16.9); mean APACHE II score 19.4). In total, 121 patients (84%) had at least one cTnT level ≥15 ng/l during their stay in the ICU. Twenty patients (14%) fulfi lled criteria for a defi nite MI, of whom 65% were septic and 50% were on noradrenaline at the time (ICU and hospital mortality: 25% and 30%, respectively). Thirty-nine patients (27%) had a possible MI, of whom 69% were septic and on noradrenaline (ICU and hospital mortality: 31% and 41%, respectively). Sixty-two patients (43%) had a raised troponin without ECG, of whom 69% were septic and 50.7% were on noradrenaline (ICU and hospital mortality: 23% and 31%, respectively). Twenty-three patients had normal cTnT results and serial ECGs, of whom 61% had sepsis. ICU and hospital mortality was 4%. Only 25% of defi nite MIs and 18% of possible MIs were recognised by the clinical teams at the time.
Conclusion Eighty-four per cent of critically ill patients had a raised cTnT level at some stage during their stay in the ICU. More than 40% of patients fulfi lled criteria for a possible or defi nite MI, of whom only 20% were recognised clinically. ICU and hospital outcome were signifi cantly worse in patients with a cTnT rise. The proportion of patients with sepsis was similar between the patients with a defi nite, possible or no MI. 
The GRACE risk score for predicting death within 6 months of hospital discharge was validated and can be used in patients with ACS. It would be perfect in the future to include the GRACE risk score in the medical records of this type of patients. Also it would be very interesting to validate this in a multicentric study. Figure 1 ). Patients in Group 2 had more prolonged length of stay in the ICU and in hospital than patients in Group 1.
After recovery from septic shock we notice a huge accumulated fl uid balance. A more positive fl uid balance was associated with a more prolonged length of stay in the ICU and in the hospital. UGIB patient needs an intervention or not. However, the intervention which the GBS mentions includes not only endoscopy but also blood transfusion. Therefore, we cannot determine whether a UGIB patient needs urgent endoscopy or just blood transfusion by GBS alone. We hypothesized that high lactate clearance (CLac) would decrease the likelihood of sustained UGIB. Methods This is a retrospective study. UGIB patients, who visited the emergency department (ED) of the National Center for Global Health and Medicine from April 2011 to March 2012 and received urgent endoscopy in the ED, were enrolled. We collected for each patient the GBS, the blood lactate value on arrival in the ED, the blood lactate value after bolus administration of 20 to 40 ml/kg Ringer's acetate (initial fl uid therapy) and the report of urgent endoscopy. We classifi ed the severity of UGIB according to GBS. A score ≤12 was classifi ed as moderate, and a score ≥13 was classifi ed as severe. CLac was defi ned as the percentage decrease in blood lactate from the time of arrival in the ED to the time when an initial fl uid therapy was fi nished. CLac <50% was defi ned as low, and CLac ≥50% was defi ned as high. Whether a patient had sustained bleeding or not was determined based on the report of urgent endoscopy. The relationship between CLac and sustained bleeding was examined by Fisher's exact test, and P <0.05 was considered statistically signifi cant.
Results Seventy-nine patients were enrolled. Fifty-one patients were with moderate UGIB, and 28 patients were with severe UGIB. As indicated in Tables 1 and 2 , there was a signifi cant relationship between CLac and sustained bleeding in moderate UGIB (P = 0.02). On the other hand, there was no signifi cant relationship between CLac and sustained bleeding in severe UGIB (P = 0.58). Introduction The aim of our study was to assess the muscular glucose by microdialysis and its association with mortality in septic shock patients.
We conducted a preliminary prospective study. We included septic shock patients hemodynamically optimized according to international recommendations. A microdialysis catheter was inserted in the femoral quadriceps. Interstitial fl uid samples were collected every 6 hours for 5 days. The determination of muscular glucose was performed by the CMA 600 analyzer (CMA/Microdialysis AB, Sweden). We also performed a dosage of concomitant blood glucose. The study population was divided into two groups according to hospital mortality. Statistic analysis: Mann-Whitney test and chi-squared test: comparisons between groups. Quantitative variables were expressed as mean ± standard deviation or median (interquartile range) as appropriate.
Results We included 12 patients with septic shock. The mortality rate was 50%. Demographics were comparable between groups except for age (66 ± 9 vs. 41 ± 12, dead patients vs. survivors, respectively; P = 0.002). Pneumonia was the major cause of septic shock (10 patients).
We analysed 167 blood samples and 166 muscular glucose samples. We found a positive association between muscular glucose, blood glucose and mortality. Tissue glucose was signifi cantly higher among dead patients compared with survivors at the 54th hour. Comparing all data, muscular glucose (P = 0.02) and blood glucose (P = 0.007) were signifi cantly higher in dead patients (Table 1) . Conclusion Our data suggest that muscular glucose assessed by microdialysis and blood glucose are associated with mortality in septic shock patients. Therefore, muscular glucose may refl ect the metabolic alterations and microcirculatory dysfunction induced by septic shock. Methods The audit had the Trust Audit Committee's approval. The existing protocol was used as the benchmark. Patients were studied prospectively to assess compliance with the local bowel protocol, incidence of constipation and relationship to weaning from respiratory support and feeding. All HDU and all mechanically ventilated ICU patients who stayed on the ward for more than 3 days were included, except for patients after bowel surgery and patients with encephalopathy.
Results Among the 24 HDU and 21 ICU patients audited in the Royal Liverpool University Hospital, 67% and 57% respectively were constipated. Laxatives were prescribed when patients had not opened their bowels for 3 days in 25% HDU and 75% ICU cases. Taking into consideration that the median age, APACHE II score and length of stay for constipated and nonconstipated patients were similar, the relationship to feeding and respiratory support were assessed. Introduction It was noted on our unit that dislodgement of nasogastric tubes occurred commonly. This can lead to an increased risk of aspiration, interruptions in nutritional support, skin breakdown and radiographic exposure [1] . It is recommended that the position of nasogastric tubes should be confi rmed by aspiration and pH testing, with radiographic confi rmation used only when this is not possible [2] . Methods We performed a retrospective review of chest X-ray (CXR) requests for the 3-month period June to August 2012 using the trust radiology information system. The proportion of CXR requests for confi rmation of position and patient demographics were measured with an estimation of the fi nancial cost performed. Results There were 541 patients admitted to the critical care area in the study period. In total, 207 out of 2,340 (8.8%) CXRs performed were for confi rmation of position. Repeated X-rays were required in some patients (see Table 1 ); these patients were older and tended to have a longer length of stay. A mobile CXR costs £25 in our trust, if one CXR is accepted per patient with a nasogastric tube; there was an excess of 160 images with a cost of £4,000 in the 3-month period. Conclusion An excess of CXRs were performed for confi rmation of nasogastric tube in our patient population. The recommended methods for position confi rmation were reinforced amongst medical staff . The high number of repeated imaging for some patients indicates that dislodgement of tubes was also a problem. We propose that nasogastric tubes should be bridled after fi rst dislodgement or at tracheostomy insertion to minimise dislodgement in the future.
Methods Mechanically ventilated, not enterally fed ICU patients (n = 9) were recruited from an interdisciplinary ICU. Healthy, overnight-fasted volunteers (n = 6) served as reference. A primed constant i.v. infusion of 2 H-labeled phenylalanine (Phe) and tyrosine was used to quantify whole-body protein metabolism. Patients remained on parenteral nutrition (PN) as clinically indicated; controls received PN starting 2.5 hours before starting enteral feeding. Intrinsically 13 C-Phe-labeled casein was infused for 6 hours by nasogastric tube at 0.75 g protein/ hour, together with maltodextrin at 2.73 g/hour. Protein breakdown, synthesis, net balance, and Phe splanchnic extraction were calculated before and at the end of the enteral feeding period, using equations for steady-state whole-body protein kinetics. Comparisons were made by Wilcoxon matched pairs and Mann-Whitney U tests; values are reported as mean ± SD. Results Protein net balance was lower in patients than in the reference group at baseline (-1.8 ± 1.7 vs. 0.6 ± 0.6 mg/kg BW/hour, P = 0.003), and after enteral feeding (-1.1 ± 1.5 vs. 0.6 ± 0.6 mg/kg BW/hour, P = 0.049). Recovery of labelled Phe from enteral feeding into the systemic circulation was higher in the reference group as compared with patients (20.3 + 11.2% vs. 7.0 + 4.8%, P = 0.005). Enteral feeding did not aff ect protein metabolism in the reference group. In patients, protein breakdown became slightly lower during enteral feeding (10.6 ± 3.3 vs. 11.2 ± 3.3 mg/kg BW/hour, P = 0.086) and protein net balance became slightly higher (-1.1 ± 1.5 vs. -1.8 ± 1.7 mg/kg BW/ hour, P = 0.086).
Conclusion Intrinsically isotope-labelled casein can be used to quantify dietary contribution to protein metabolism in critically ill patients. Hypocaloric enteral feeding marginally improved protein balance in these patients. The low recovery of enterally administered labelled amino acid underlines the need to quantify uptake from the gastrointestinal tract when protein turnover measurements are performed in critically ill patients on enteral nutrition. Methods This small-scale study of 25 NGT placements during a 5-week period collated data supplied by questionnaire by healthcare workers responsible for NGT placements.
Results Analysis of Adverse Incident Reports identifi ed no never events of misplaced NGTs within the previous 10 years. This audit revealed that the commonest type of NGT was a radio-opaque tube with stylet (corfl o) (92% of placements), with occasional use of the electromagnetic placement system (cortrak) (8% of placements). Sizes 10 (40%) and 12 (56%) were most common. Tube placement was confi rmed by: X-ray (72%); pH of aspirates (35%); electromagnetic tube placement (one patient). The time taken from decision to place NGT to use varied (range 15 to 510 minutes). Little distinction was seen in the time taken to use and NGT confi rmed by aspirate alone (205 minutes) or by X-ray (220 minutes), although the shortest interval was seen in electromagnetic NGT placement (15 minutes). The cost of NGTs confi rmed by aspirate alone was low (approximately £10.00), higher with X-ray confi rmation/electromagnetic placement (approximately £45.00).
Conclusion Despite the small dataset the results demonstrate a concerning delay in the application of enteral feeding and/or drug administration. Whilst reassuring in the steps taken to avoid never events, this study demonstrates that there may be delays in time-critical administration of enteral medicine or optimal nutritional practices. This study reveals a signifi cant problem with aspirating gastric contents for pH testing, necessitating a large number of X-ray position confi rmations. Even if the frequency and volume of gastric aspiration were greater, there is a belief that pH testing may not be suffi ciently accurate (since many factors alter patients' gastric pH). It is possible that new technologies such as electromagnetic NGT placement may allow faster/equally safe practices. Further study including cost/benefi t analysis will be needed to confi rm this. Reference . Eighteen readings were from newly placed NG tubes and 150 readings from old NG tubes. Fiftythree per cent of routine pH readings were falsely high; that is, pH 6 or above despite the NG tube being in the stomach (Figure 1 ). Twentyeight per cent of newly placed NG tubes had falsely high pH readings ( Figure 2 ). Conclusion In this population of ICU patients, routine/daily checks of NG pH aspirate appear to be limited. This is almost certainly due to the use of continuous NG feed together with PPIs. The usefulness of pH testing in newly placed NG tubes, however, appears more reliable. Introduction Sepsis is the most common cause of death in ICUs [1] . Destruction of intestinal barrier function and increased translocation of bacteria to systemic blood fl ow can lead to sepsis [2] . Probiotics may have benefi cial eff ects in improvement of critically ill patients by modulating intestinal barrier and reduction of infl ammation [3] . The aim of this trial was to determine the eff ect of probiotic on infl ammatory biomarkers and mortality rate of sepsis in critically ill patients in the ICU. Methods This double-blind, randomized controlled trial was conducted on 40 critically ill patients admitted to the ICU. They were randomly assigned to receive placebo or probiotic for 7 days. The APACHE score, Sequential Organ Failure Assessment (SOFA) and systemic concentrations of IL-6, procalcitonin (PCT) and protein C were measured before initiation of the study and on days 4 and 7. Also, 28day mortality was evaluated for each patient.
Results IL-6 and PCT levels decreased and protein C levels increased signifi cantly in probiotic group over the treatment period (P <0.001).
There was a signifi cant diff erence in IL-6, PCT and protein C levels of the 7th day between two groups (P = 0.001, 0.006 and <0.001, respectively). Compared with controls, probiotic was eff ective in improving APACHE and SOFA scores in 7 days (P <0.001). There was signifi cant diff erence between the probiotic and control group in the 28-day mortality rate (20% vs. 55% respectively, P = 0.048). Conclusion Probiotics reduce infl ammation and mortality rate in critically ill patients and might be considered as an adjunctive therapy to sepsis.
Introduction The aim of this study is to establish whether diff erent types of sepsis have an impact on selenium levels. Selenium is an essential trace element involved in antioxidant and immunological reactions. Selenium levels have been shown to be low in patients with systemic infl ammatory response syndrome and sepsis. Selenium replacement has been recommended in patients with sepsis [1, 2] . Greater than 5 days of supplementation may also help to prevent the development of new infections on ICUs [3] . Methods This is a prospective survey where selenium levels were collected from patients admitted with septic shock to a tertiary ICU, for 6 months from October 2010 to March 2011. Results Selenium levels were measured in 31 patients with septic shock. Abdominal and chest sepsis were the main sources of infection. Those with an abdominal source of sepsis had the lowest levels, as shown in Table 1 . All septic shock patients who had selenium levels taken within the fi rst 10 days of admission had subnormal levels (<0.8 mg/dt), and after 10 days had levels within the normal range, as shown in Figure 1 . Introduction Glutamine regulates many biological functions in preserving the cell, acts as a key respiratory fuel and nitrogen donor for rapidly dividing cells, and modulates the expression of many genes associated with metabolism, cell defences and repair, and cytokine production. In severe thoracic trauma, glutamine supplementation is essential because the body consumes more than it produces and glutamine eff ects become dependent on its route of delivery.
Methods Fifty-two patients 19 to 78 years old with surgery for severe thoracic trauma were assessed in two groups: Group A received 0.3 to 0.5 g/kg/day i.v. glutamine + 20 g enteral glutamine for 7 days, supplementation to enteral nutrition; Group B receive only i.v. glutamine supplementation to enteral nutrition 0.3 to 0.5 g/kg/day for 7 days. Weaning time, the duration of p.o. ileus, incidence and time to resolution of VAP, glycemic level and the percentage decrease of CRP at 96 hours were assessed in both groups. Results Weaning time and the duration of p.o. ileus were signifi cantly lower in Group A; although the incidence of VAP is similar in both groups, the time of VAP resolution is lower, the glycemic control is better in Group A. The percentage of CRP decrease is higher in Group A. See Figure 1 .
Conclusion Glutamine becomes an essential amino acid in severe thoracic trauma and when the patients are fed other than TPN (enteral, oral); although hard evidence is lacking, both administration routes may be effi cient as soon as possible. Results Total cholesterol (TC) and low-density lipoprotein-cholesterol (LDL-C) levels were less changed signifi cantly in the low ratio group (3 ± 18 vs. 16 ± 23 mg/dl, P = 0.027 for TC, 4 ± 12 vs. 12 ± 18 mg/dl, P = 0.026 for LDL-C) compared with the high ratio group in postoperative patients. Other laboratory parameters and adverse events did not show statistically signifi cant diff erences between the groups. See Table 1 . Introduction The optimal feeding of critically ill patients treated in the ICU is controversial. Present guidelines for protein feeding are based on weak evidence obtained with suboptimal methods. Whole body protein kinetics is an attractive technique to assess optimal protein intake by measuring the eff ect of protein feeding strategies on protein synthesis rates, protein degradation rates and protein balance. Here protein kinetics were measured in critically ill neurosurgical patients during hypocaloric and normocaloric parenteral nutrition. Methods Neurosurgical patients on mechanical ventilation (n = 16) were studied. Energy expenditure was measured with indirect calorimetry. After that, the patients were randomized to receiving 24 hours of 50% of measured energy expenditure followed by 24 hours of 100% or 100% before 50%. Whole body protein kinetics were measured during the last half hour of the feeding periods using stable isotope-labeled phenylalanine as a tracer. During a continuous infusion of labeled phenylalanine and tyrosine, plasma samples were obtained and later analyzed for the content of the labeled amino acids using mass spectrometry. Protein kinetics were calculated using standard steady-state kinetics. In addition, amino acid concentrations were analyzed by HPLC. Student's t test was used for statistical analyses.
The patients received 0.5 ± 0.1 and 1.1 ± 0.2 g amino acids/ kg/day (P <0.001) on the days with 50 and 100% of measured energy expenditure respectively. Energy expenditures were 23.4 ± 2.7 and 24.5 ± 2.3 kcal/kg/day (P = 0.05) on the 50 and 100% days respectively. Plasma amino acids concentrations were 2.8 ± 0.5 and 2.9 ± 0.4 mM (P = 0.085) on the 2 days respectively. Whole body protein synthesis was 12% lower when 50% of energy expenditure was given, 11.7 ± 3.0 versus 13.3 ± 2.2 mg/kg/hour (P = 0.025), whilst protein degradation was unaltered 13.6 ± 3.5 versus 14.0 ± 2.6 mg/kg/hour (P = 0.56). Also protein oxidation was unaltered 3.0 ± 2.1 versus 2.9 ± 1.4 mg/kg/hour (P = 0.85). This resulted in a 60% higher whole body protein balance with the normocaloric nutrition, -1.9 ± 2.1 versus -0.7 ± 1.3 mg/kg/ hour (P = 0.014). Conclusion The protein kinetics measurements and the protocol used were useful to assess the effi cacy of nutritional support in critically ill patients. In the critically ill neurosurgical patients treated in the ICU, hypocaloric feeding was associated with a more negative protein balance, while the amino acid oxidation was not diff erent.
controlled trial (EPaNIC: clinicaltrials.gov: NCT00512122) [2] showed that withholding parenteral nutrition during the fi rst week of ICU stay whereby tolerating substantial caloric defi cit (late PN) accelerated recovery and shortened weaning time as compared with early parenteral substitution for defi cient enteral feeding (early PN). We examined the impact of late PN, as compared with early PN, on incidence and recovery of ICUAW.
Methods A preplanned subanalysis of adult patients included in the EPaNIC trial. The study was performed between October 2008 and November 2010 and included those patients who required intensive care for ≥8 days as well as a computer-generated, admission categorymatched, random sample of short-stay ICU patients, the latter to correct for possible bias evoked by earlier ICU discharge in one of the two study groups. Assessors blinded for treatment allocation evaluated muscle strength clinically three times weekly from awakening onward and performed nerve conduction studies and electromyography (NCS and EMG) weekly. The primary outcome was the incidence of ICUAW, diagnosed clinically by the Medical Research Council (MRC) sum score (<48/60) [3] at fi rst evaluation. Secondary outcomes included ICUAW at worst and last MRC evaluation, recovery from ICUAW and incidence of abnormal fi ndings on NCS and EMG. All analyses were performed on the total dataset and on a for-baseline characteristics propensity score-matched sample to correct for possible imbalances between the groups. [1] . Plasma total bilirubin was quantifi ed in all patients daily while in the ICU. Liver enzymes ALT, AST, GGT and ALP were quantifi ed twice weekly in all patients while in the ICU. In a random predefi ned subset of patients, circulating bile salts were also quantifi ed with MS-HPLC at baseline and on day 3, day 5 and the last day in the ICU (n = 280). Gallbladder sludge was evaluated by ultrasound on ICU day 5 by blinded assessors (n = 776).
Results From day 1 after randomization until the end of the 7-day intervention window, plasma bilirubin was higher in the late PN than in the early PN group (all P <0.001). In the late PN group, as soon as PN was started on day 8, plasma bilirubin also fell and the two groups became comparable. Maximum levels of GGT, ALP and ALT during the ICU stay were higher in the early PN group (all P <0.01). Compared with baseline, the circulating glycine and taurine conjugated primary bile salts were elevated on day 3, day 5 and last day of the ICU stay (P <0.01 for all). However, there was no diff erence between the two groups. More patients in the early PN than in the late PN group had gallbladder sludge on day 5 (45% vs. 37%; P = 0.04). Conclusion Tolerating substantial caloric defi cit by withholding PN until day 8 of critical illness increased circulating levels of bilirubin but reduced the occurrence of gallbladder sludge and lowered GGT, ALP and ALT levels. These results suggest that hyperbilirubinemia during critical illness dies not necessarily refl ect cholestasis and instead may be an adaptive response. Additional analyses on a propensity scorematched patient population are ongoing. Reference the duration of renal replacement therapy (RRT) [1] . The impact of the intervention on early markers of catabolism has not been investigated.
Methods We studied the impact of early versus late PN on daily markers of catabolism in the ICU in the total study population and in propensity score-matched subgroups of long-stay patients. In addition, we calculated the net incorporation rate of the extra amino acids supplied by early PN. Results Plasma urea, the urea/creatinine ratio and nitrogen excretion increased over time in the ICU. Early PN further increased these markers of catabolism, from the fi rst day of amino acid infusion onward, and only marginally improved the nitrogen balance. Also in the group that received PN only after the fi rst week in the ICU, ureagenesis was increased by infusing amino acids. Over the fi rst 2 weeks, approximately two-thirds of the extra amino acids supplied by early PN were net wasted in urea. The above fi ndings were confi rmed in propensity scorematched subgroups of long-stay patients. The higher urea levels with early PN, rather than the kidney function as such, may have driven the observed longer duration of RRT, as supported by multiple regression analysis.
Conclusion The extra amino acids supplied by early PN appeared ineffi cient to reverse the negative nitrogen balance, not because of insuffi cient amino acid delivery, but rather because of insuffi cient incorporation with, instead, increased degradation into urea. The substantial catabolism of the extra amino acids, leading to pronounced urea generation, may have prolonged the duration of RRT in the early PN group. Introduction Muscle weakness of critical illness is associated with prolonged dependency on ventilatory support and delayed rehabilitation. Muscle wasting related to poor nutrition has long been considered a major determinant, whereas the importance of myofi ber integrity only recently emerged [1] [2] [3] [4] . We hypothesized that nutrient restriction early during illness aggravates atrophy while preserving myofi ber integrity by activating the crucial cellular quality control pathway autophagy. The latter could be important to preserve muscle function.
Methods Critically ill patients (n = 122) were randomized to early (early-PN) or late (late-PN) initiation of parenteral nutrition to complete failing enteral nutrition, while maintaining normoglycemia (80 to 110 mg/ dl) with insulin, in the EPaNIC study [5] . Vastus lateralis biopsies were harvested after 1 week and compared with matched controls (n = 20).
Results As compared with controls, muscle from critically ill patients showed reduced myofi ber density, a shift to smaller (especially type I) myofi bers, lower myosin and actin mRNA, upregulated mRNA of the ubiquitin ligases muscle-ring-fi nger-1 and atrogin-1, a small increase in the autophagosome formation marker LC3-II/LC3-I, and increases in the autophagic substrates ubiquitin and p62 (all P ≤0.006). Late-PN, resulting in a larger caloric defi cit than early-PN, had no substantial impact on atrophy markers. In contrast, late-PN increased LC3-II/LC3-I (P = 0.02), which coincided with less accumulation of ubiquitinated proteins/aggregates (P = 0.05). Fewer patients on late-PN developed muscle weakness as compared with early-PN (42% vs. 68%, P = 0.05).
In multivariable analysis, a lower LC3-II/LC3-I ratio (P = 0.05) and higher myofi ber density (P = 0.04) were independently associated with muscle weakness.
Conclusion Early-PN did not counteract muscle atrophy whereas it suppressed autophagy and aggravated weakness. Statistically, muscle weakness was not explained by atrophy or wasting but rather by impaired autophagy and preservation of muscle density. Thus, tolerating nutrient restriction early during critical illness may preserve myofi ber integrity by activating autophagy.
Introduction Closure of an acute hospitals emergency department (ED) has important ramifi cations for those centres expected to take up the resultant workload. The continued reconfi guration of emergency care is likely to produce an increasing number of these scenarios. Little evidence is available to support planning of such initiatives and thus the implications are diffi cult to anticipate. This study aims to demonstrate one hospital's experience of the rationalisation of emergency care and its eff ect on workload.
Methods This retrospective study was conducted in a large teaching hospital. Activity data were analysed for a 12-month period following the closure of a neighbouring ED. The results were subsequently compared against the year prior to closure. Attendance, triage data, admission rates and waiting times were compared across the two periods, as were workload data for all grades of physician. The chisquared test was used to examine diff erences between groups. Results In the period studied, the gross attendance fi gure increased by 20,480 (33.72%), whilst the admission rate rose from 22 to 27%. Following closure of the neighbouring ED, the proportion of highacuity patients attending our institution increased dramatically, with the proportion of category one and two patients (Manchester Triage Scale) increasing by 8.33% (P = 0.076) and 18.80% (P <0.001), respectively. Likewise, ambulance arrivals increased out of proportion to the total increase in attendances (P = 0.016). Admissions from the ED to the ICU increased by 63.04%. Consultants workloads now include 50% more category 1 and 2 patients (P = 0.001).
Conclusion Reconfi guration of emergency care can have dramatic implications for existing services; these may not always be anticipated. Rationalisation of ED's may result in a concentration of high-acuity patients accompanied by a downturn in the numbers of patients whose presentations are amenable to care delivered in other settings. This abrupt change in case mix requires a re-examination of existing workforces and their seniority.
Overcrowding estimation in the emergency department: is the simplest score the best? Introduction Emergency department (ED) overcrowding is a major international problem with a negative impact on both patient care and providers. Among validated methods of measurement, emergency physicians have to choose between simple and complex scores [1, 2] . The aim of the present study was to compare the complex National Emergency Department Overcrowding Scale (NEDOCS) with the simple occupancy rate (OR) determination. We further evaluated the correlation between these scores and a qualitative assessment of crowding.
Methods The study was conducted in two academic hospitals and one county hospital in Liège, Chênée and Verviers; each with an ED census of over 40,000 patient visits per year. Samplings occurred over a 2-week period in January 2011, with fi ve sampling times each day.
Results ED staff considered overcrowding as a major concern in the three EDs. Median OR ranged from 68 to 100, while the NEDOCS ranged from 64.5 to 76.3. We found a signifi cant correlation between Introduction It is evident that accident and emergency departments are overloaded with patients, which results in delays in healthcare provision [1] . A large proportion of patients consist of patients with minor illness that can be seen by a healthcare provider in a primary care setting. The aim of the study was to determine the characteristics of patients using GP walk-in services, patients' satisfaction and the eff ect on emergency department (ED) services.
Methods The survey was conducted in Sheffi eld and Rotherham walk-in centres over 3 weeks during September and October 2011. A self-reported, validated questionnaire was used to conduct survey on the patients presenting at these centres. We estimated that a sample size of around 400 patients from each centre was required to achieve statistically robust results. A post-visit, short questionnaire was also sent to those who agreed for the second questionnaire and provided contact details. ED data were also obtained from April 2008 to March 2010, 1 year before and 1 year after the opening of the GP walk-in centre. Data were entered and analysed in PASW Statistics 18. Ethical approval of the study was obtained from the NHS ethical review committee.
Results A total of 1,030 patients participated in the survey (Rotherham 501; Sheffi eld 529). The mean age of the participants was 32.1 years at Sheffi eld and 30.88 years at Rotherham. A higher proportion of users were female, around 59% at both centres. Most of the patients rated high for convenience of the centre opening hours and location (above 85%, apart from the location of Sheffi eld centre, which was rated high by around 72% of the research participants). Overall 93% patients were satisfi ed with the service at Rotherham centre and around 86% at the Sheffi eld centre. Based on the estimation of the monthly counts of patients attending ED and the GP walk-in centre, around 14% monthly reduction in minor attendances at ED was expected. However, ED routine data did not show any signifi cant reduction in minor attendances as a result of the opening of the GP walk-in centre.
Conclusion These walk-in centres have been shown to increase accessibility to healthcare service through longer opening hours and walk-in facility. Although the eff ect on the reduction of patients' load at the ED is not visible as these centres cover a fraction of the population, the centre has a potential to divert patients from the ED. Reference 
Overcrowding in emergency departments (EDs) is a widely known problem. It causes problems and delays in the ED and has a negative impact on patient safety [1] . The aim of this study was to analyse whether a reform of emergency care can reduce patient fl ow into the ED.
Methods A substantial reform of emergency care took place in the province of Kanta-Häme in Southern Finland. Three separate out-ofhours services in primary healthcare (PHC) and one ED in the hospital were combined into one large ED in April 2007. Basic principles of the new ED were: the ED is only for those patients who are seriously ill or injured, and need immediate care; PHC (healthcare centres) take care of acute ordinary illnesses and nonserious injuries during offi ce hours. To achieve these principles a regional fi ve-scale triage system was planned and implemented. The information plan was established.
Citizens were systematically informed about the principles of the new ED by mail, articles in the newspapers and interviews in the radio and television. The ED's Internet pages were planned and established. The number of patient visits (Hämeenlinna region) was analyzed 2 years before and after establishing the new ED.
Results During the 2-year period before the establishment of the new ED the mean number of GP patient visits was 1,845 ± 43/month. During the 2-year period after the reform the number was diminished to 1,364 ± 21/month. This change was not associated with the increase of the patient visits taken care of by specialists and hospital residents. See Figure 1 .
Conclusion An extensive reform of the emergency services can notably reduce patient fl ow into the ED. Reference 
Abdominal pain in adolescent females has undergone recent changes with regards to its management under various specialities. The authors report a single-centre audit looking at the correct investigation and management of 12-year-old to 16-year-old girls with abdominal pain in the emergency department setting. Methods A single-centre audit and retrospective analysis of patients took place using case notes and computerised records. Documentation was analysed using statistical analysis and minimum standards were set and reviewed.
Results After exclusion criteria 62 females between the ages of 12 and 16 presented to the paediatric emergency department in Leicester with abdominal pain as the predominant admission symptom during a 12-month period. Documentation of the gynaecological history was poor (menstrual history 47%, sexual history 14%, contraception 8%), as was the performance of basic investigations (urine dipsticks 65%, pregnancy test 42%). Documentation was analysed with regard to discharge diagnosis. Ultrasound investigation was performed on seven of the patients but only once admitted to various specialities. No ultrasound was undertaken upon admission. Conclusion Improvement in documentation of minimum standards for these patients is needed. A multidisciplinary care pathway could improve outcome. Consideration should be given to whether early ultrasound investigation is appropriate and there is a further need for investigation as to whether this would improve longer term outcomes. Introduction BiPAP utilization for the treatment of severe refractory status asthmaticus patients has become an accepted therapy but is not well described for moderate exacerbations. We sought to analyze outcomes from our BiPAP quality database for children presenting in status asthmaticus at varying levels of severity. Methods PED status asthmaticus patients requiring BiPAP from 1 January 2010 to 31 August 2012 had a bedside interview and documentation of information at the time therapies were given. Incomplete data were collected retrospectively. All data were stored and analyzed using a RedCap database. Subjects were stratifi ed into severity groups based on asthma score at the time of BiPAP placement. Results There were 206 subjects in the moderate severity group and 197 in the severe group. Table 1 shows the groups were well matched and compares other pertinent data. Children with severe presentations were placed on BiPAP sooner (P <0.001) and remained on BiPAP longer (P <0.001). The moderate group had a longer wait until BiPAP placement. Tables 2 and 3 demonstrate higher initial BiPAP (IPAP/EPAP) settings with increasing age and severity. Figure 1 trends initiation and termination asthma scores stratifi ed by severity at BiPAP 
We present a case series of toxicity due to a novel substance in the UK: Eric-3. Novel drugs of abuse are becoming more common throughout the world, and they represent particular diffi culties in their acute management. A recent report from the European Monitoring Centre for Drugs and Drug Addiction and Europol has reported 49 new psychoactive substances reported via its early warning system. Methods This was a retrospective case-note review over a 6-month period. Patients were included if their presentation was due to recent ingestion of Eric-3. Physiological data, symptoms, outcome and destination of the patient from the emergency department were collected. Postmortem toxicological analysis was obtained for one of the two patients who died. Results Forty-one attendances were identifi ed from 18 patients. Two patients died and fi ve were admitted to the ICU. Heart rate and temperature on arrival tended to be above normal (mean heart rate was 112 bpm, with an SD of 18; mean temperature was 37.45°C with an SD of 0.95). In total, 63.4% of attendances included agitation and 34.1% choreiform movements. α-Methyltryptamine and 3-/4-fl uoroephedrine were found in the blood of one of the patients who died. Conclusion In this outbreak in the UK, Eric-3 gave symptoms similar to other stimulants known as legal highs, including death. It may have been a novel substance, 3-/4-fl uoroephedrine. This underlines the need for prospective data collection and early national and international information sharing.
Introduction Thallium is an odorless, tasteless, heavy metal that has been often used for intentional poisonings. In severe patients, thallium poisoning produces neuromuscular symptoms such as extreme pain and muscle weakness. Methods Five case reports. Results All patients worked at a pharmaceutical factory. They joined a tea party held at their workplace at the end of April 2011. The fi ve patients drank tea from a teapot someone had put thallium in. A few days later, they complained of femoral numbness and pain caused by pressure. About a week later, three of fi ve patients had profound hair loss. Three weeks after the party, they came to our ER. We thought that their symptoms might be caused by some chemicals. We searched the keywords: 'lower extremity pain' , 'hair loss' and 'poison' in the Internet. As a result, thallium, mercury, lead, and so forth, were suspicious metals. In those metals, thallium was most likely because it was used in their factory. We immediately examined the blood concentration of several metals and ordered iron(III)hexacyanoferrate(II) that is known as the antidote for thallium poisoning. Only thallium was positive in the blood metal concentration test. Three patients consented to oral administration of an antidote. Two patients rejected administration because their symptoms were mild and getting better. All symptoms of all patients gradually disappeared by August. We also followed up the course of blood concentration of thallium. The concentration in three patients who took the antidote was reduced more rapidly than the two patients who did not take it. Conclusion All patients recovered without any sequelae. Three patients' hair started to grow 3 months from ingestion of thallium, and after half a year their hair was restored to their former state. We had diffi culty ordering iron(III)hexacyanoferrate(II) because this is also known as an antidote for cesium. On 11 March 2011 a megathrust earthquake and tsunami hit Japan and the giant tsunami gave rise to an accident at a nuclear power generation plant. Because the rumor of radioactive substances including cesium might be spread was the talk in the city near the nuclear power plant, the authorities put the antidote under heavy supervision. We could also collect the data for the course of thallium concentration. Thallium concentration of the patients who had an antidote was reduced more rapidly but these patients had a loose stool, thought to be a side eff ect of this antidote. Reference 17.5 ± SD 3.0 9.5 ± SD 1.8 drugs aff ects the central nervous and cardiovascular systems, resulting in severe arrhythmia and death. Heart rate variability (HRV) analysis is a non-invasive assessment method that allows evaluation of the cardiac autonomic (sympathetic and parasympathetic) activity. The aim of this study was to evaluate HRV in children requiring ICU stay due to TCA poisoning.
Methods Twenty children with isolated TCA poisoning aged between 3 and 16 years who were hospitalized in the pediatric ICU, between March 2009 and July 2010, and 20 healthy children as a control group were enrolled. Clinical and electrocardiographic (ECG) fi ndings were noted in the TCA poisoning group. In both groups, 24-hour time domain HRV analysis (SDNN, SDANN, SDNNi, RMSDD, NN50, and pNN50) was performed. We also recorded frequency domain analysis results at the fi rst 5 minutes and the last 5 minutes of the 24-hour record (VLF, nLF, nHF, LF/HF ratio).
The average level of TCA in the study group was 1,116 ± 635 and TCA levels were positively correlated with the duration of QRS interval (P <0.01). In time-domain nonspectral evaluation, SDNN (P <0.001), SDNN (P <0.05), RMSDD (P <0.01), and pNN50 (P <0.01) were found signifi cantly lower in the TCA intoxication group compared with the control group, while NN50 (P <0.01) was signifi cantly higher in value. The spectral analysis (frequency domain) of data recorded at fi rst 5 minutes after intensive care admission showed that the values of the nLF (P <0.05) and LF/HF ratio (P = 0.001) were signifi cantly higher in the TCA intoxication group than the controls, while nHF (P = 0.001) values were signifi cantly lower. The frequency domain spectral analysis of data recorded at the last 5 minutes showed a lower nHF (P = 0.001) in the TCA intoxication group than the controls, and the LF/HF ratio was signifi cantly higher (P <0.05) in the intoxication group. SDNN (P <0.001), RMSDD (P <0.01), SDNNi (P <0.01), and pNN50 (P <0.01) levels were higher in patients with positive ECG fi ndings than those without positive ECG fi ndings. The LF/HF ratio was higher in seven children with seizures (P <0.001). Conclusion Existing fi ndings give us an idea about HRV's value to determine arrhythmia and predict convulsion risk in TCA poisonings. HRV can be used as a non-invasive method in determining the treatment and prognosis of TCA poisoning. Results HMMD receives an average of 30 cases of stroke monthly, and thrombolysis did not occur before the implementation of the TM Project, because of the lack of neurologists available to conduce the cases. After implementation of the TM Program, six cases of ischemic stroke were thrombolyzed with alteplase; only one case (16%) progressed to death from septic shock, and one case (16%) presented symptomatic intracranial hemorrhage. Conclusion Thrombolysis in ischemic stroke reduces 30% the risk of disability and 18% the mortality rate. This procedure has been only feasible to be done in the community setting because of the implementation of the TM Project, which permits the presence of a real time consultation with a specialized neurological team from a tertiary center. analyses, and then returned home. In total, 25.50% of patients were hospitalized in a medical or surgical department, and 12.42% in the short-term hospitalised unit of the emergency department (stay duration <24 hours). Some 5.10% of patients worsened and were oriented in the ICU. A total 3.77% of patients in a cardiac ICU. In total, 73.84% of patients had stay duration less than 6 hours in the ED, 24.45% <24 hours. Forty percent of patients supported by fi remen and 54% supported by private ambulance left the hospital after a single medical consultation.
Conclusion Nearly 70% of patients calling the French emergency medical dispatching centre are sent to hospital. Those transportations are supported for two-thirds of cases by a private ambulance or fi remen ambulance. One out of two patients only receive a simple medical consultation in the ED, and go back home. This may concur to the defi ciency of using general medicine in town. They prefer using emergency services for free. Only one patient out of four was hospitalized more than 24 hours.
Introduction Early onset eff ective care in the emergency department (ED) has been reported to have a great infl uence on the intensive care patients' morbidity and mortality [1] . Little is known about the infl uence of the reorganisation of the ED on patient intake to the ICUs. The aim of this study was to analyse monthly intake of patients from the ED to the cardiac care unit (CCU) and ICU before and after the reform of emergency services. Methods In Kanta-Häme Central Hospital, a new ED started on 1 April 2007. Four older emergency rooms were combined into one bigger emergency department and an observation ward was introduced with continuous follow-up of vital signs. This study is a retrospective analysis of the patient intake to the CCU and ICU 1 year before and after the reorganisation. Using as data the Finnish Intensive Care Quality Consortium (Intensium, Finland) database and the cardiac database of the hospital, patient transfer from ED to the ICU and CCU was collected and analysed. Monthly pre/post comparisons were carried out statistically by a nonparametric Wilcoxon signed-rank test. 
The total decrease in monthly patient infl ow from ED to the ICU and CCU was 30.1% (P = 0.003); that is, from the mean of 47.7 ± 8.2 to 33.3 ± 8.3 patients (Figure 1) <0.001) . The result is longer overall hospitalization of patients having WI (P < 0.001) and a higher number of surgeries (P <0.02). After the ER, 54% of patients with WI were hospitalized in the ICU (86% of them after surgery) but only 26% of patients involved in a CA (71% after surgery). As many patients with WI as involved in a CA (40%) were admitted to the ward (89% of patients with WI after surgery but only 63% of patients with injuries due to a CA). Thirty-three per cent of patients involved in a CA returned home and one was transferred, whereas only three patients with WI returned home after being in the ER, three patients were transferred and one died in the operating room. Observed paediatric mortality in our medical treatment facility was 2.9% (10 children out of 341): three children died of WI, three due to a CA and one of septic shock due to a medical cause. Conclusion War injuries are more prone to cause polytrauma than CA. According to the PTS, ISS, NISS, TRISS and ASCOT, children experiencing WI have higher severity scores and predicted mortality rate than others, stay longer in the hospital and have more surgeries. 
Our research indicated that disaster medicine should be established systematically or it is necessary to compile a compendium of disaster medicine from a broad perspective or from a bird's-eye and long-term view. The Japanese version was tentatively completed with 22 volumes as of the fi nancial year 2005, of which nearly three-quarters are written in Japanese. Although this worked partly during the aboveshown catastrophe in Japan 2011, several problems are left to be solved; that is, the insuffi cient operation system of the Japan DMAT or Disaster Medical Assistant Team that seemed to have caused a large number of preventable deaths. Conclusion The large number of casualties during a major disaster is a global problem, even in the developed countries. When the role of the intensivist is reviewed, many roles were verifi ed to be important; that is, as a leader of a medical team or triage offi cer as well as a professional in the fi eld of specifi c intensive care. However, there are many problems to be solved in the fi elds of disaster medicine. In order to solve the diversifi cation or the various medical problems, it is necessary to compile or systematize a disaster medicine of the world version. The concept of the compendium and our process of trial are shown in relation to intensive care. There are distinct diff erences in the pathophysiology between medical cardiac arrests and TCA. Traumatic pathologies associated with an improved chance of successful resuscitation include hypoxia, tension pneumothorax and cardiac tamponade [1] . The authors believe a separate algorithm is required for the management of out-of-hospital TCA attended to by a highly trained physician and paramedic team. Methods A suggested algorithm for TCA was developed based on the Greater Sydney Area Helicopter Emergency Medical Service's standard operating procedures and current available evidence. Results An algorithm for the general management of TCA can be seen in Figure 1 . In TCA, priority should be given to catastrophic haemorrhage control (tourniquets, direct pressure, haemostatic agents, pelvic and long bone splintage) and volume resuscitation. Simultaneous oxygena tion optimisation should occur with proactive exclusion of tension pneumothoraces with bilateral open thoracostomies. Cardiac ultrasound (US) should be used to help exclude cardiac tamponade and assist in prognostication. The US presence of true cardiac standstill versus low pressure state/pseudo-PEA, and an ETCO 2 <1.3 kPa carries a grave prognosis in TCA. Given the high incidence of hypovolaemia, hypoxia and obstructive shock prior to TCA, the role of adrenaline and chest compressions are limited. Figure 2 shows a suggested algorithm for the management of penetrating TCA requiring prehospital thoracotomy.
Conclusion The suggested algorithm is designed for a highly trained physician-led prehospital team and aims to maximise the number of neurologically intact survivors in out-of-hospital TCA. Little is known about the benefi t of physician winching in addition to a highly trained paramedic. We analysed the mission profi les and interventions performed during rescues involving the winching of a physician in the Greater Sydney Area HEMS (GSA-HEMS). Methods All winch missions involving a physician from August 2009 to January 2012 were identifi ed from the prospectively completed GSA-HEMS electronic database. A structured case-sheet review for a predetermined list of demographic data and physician-only interventions (POI) was conducted.
We identified 130 missions involving the winching of a physician, of which 120 case sheets were available for analysis. The majority of patients were traumatically injured (90%) and male (85%) with a median age of 37 years. Seven patients were pronounced life extinct on the scene. A total of 63 POI were performed on 48 patients. Administration of advanced analgesia was the most common POI making up 68.3% of interventions. Patients with abnormal RTSc 2 scores were more likely to receive a POI when compared with those with normal RTSc 2 (P = 0.03). The performance of POI had no effect on median scene times (45 vs. 43 minutes; P = 0.51). See Tables 1  and 2 . Conclusion Our high POI rate of 40% coupled with long rescue times and the occasional severe injuries supports the argument for winching doctors. Not doing so would deny a signifi cant proportion of patients time-critical interventions, advanced analgesia and procedural sedation. 
The aim was to assess the content and state of repair of equipment carried for transfer of critical care patients to other hospitals. By chance, several items of date-expired stock were identifi ed in the transfer kit whilst moving a patient to a tertiary centre. This raised the possibility of a more extensive problem with the equipment bags. Due to the geographical location of our district general hospital we undertake around 70 transfers of critical care patients to other hospitals per year (16% by air) and it is clearly important that our equipment is well maintained for these journeys. Methods We maintain two identical sets of equipment (syringes, fl uid, airway management items, and so forth) and drug bags to take on transfers; one equipment and one drug bag taken on each trip. The contents of all four bags were checked and itemised. By careful consideration of the aims of the bags (to provide emergency equipment and drugs for managing one patient during an en-route emergency) a new inventory was devised. Excess items were removed to lighten the bags and improve accessibility to the essential items. Expired stock was removed. A daily checking procedure and tamper-proof seals on the bags were instigated and the bags were reassessed 12 months later. Results A total of 13.9% of drug items and 29.2% of equipment items had expired or would do so within 30 days of the initial assessment. The combined weight of one equipment and one drug bag was reduced from 14 to 9 kg (36% reduction) by introducing the new inventory. At reassessment in November 2012, only 10 items of equipment (3.2%) were expired or near to expiry and there were no expired drug items (4.1% near to expiry). In total, 0.3 kg (26 small items) of extraneous equipment had been added through over-restocking and was removed. Conclusion These bags are designed for a clinician to manage a patient when an emergency arises during transfer of a critical care patient. By the introduction of simple measures, the risks posed by expired items or cluttered equipment bags have almost been eradicated. Signifi cant weight savings have been made; this off ers improved ergonomics for staff and is also an important consideration for aeromedical operations. Our department was surprised to discover the extent of decline of our equipment and it may be that other departments would fi nd themselves in a similar position. The anaesthetic registrars who routinely escort the transfer patients have a vested interest to maintain this equipment and this has secured their buy-in to the new checking procedure with clear results. Conclusion Prehospital hyperoxemia did not infl uence the functional neurological outcome. One of the reasons for this fi nding could be the short arrival time to the trauma center where repeated analyses of arterial blood gases were performed. Therefore, correction of fraction of inspired oxygen according to the arterial blood gas analysis shortens the time of hyperoxemia, thus reducing neuronal brain damage.
Introduction Severe burn patients are often noted to have subsequent neurocognitive problems. Experimentally, we have found striking, prolonged elevations of infl ammatory markers in the brain (for example, IL-6) even when the injury occurs in a remote anatomic location. This neuroinfl ammatory response can also be signifi cantly blunted by a single post-burn dose of estrogen. Sonic hedgehog (SHH), an important signaling protein found in the brain, controls and directs diff erentiation of neural stem cells, infl uencing brain regeneration and repair by generating new neurons throughout life. As estrogens not only blunt infl ammation but also exert an infl uence on a variety of stem cells, we hypothesized that 17β-estradiol (E2) might aff ect levels of SHH in the post-burn rat brain.
Methods Male rats (n = 44) were assigned randomly into three groups: controls/no burn (n = 4); burn/placebo (n = 20); and burn/E2 (n = 20). Burned rats received a 40% 3° TBSA dorsal burn, fl uid resuscitation and one dose of E2 or placebo (0.5 mg/kg intraperitoneally) 15 minutes post burn. Eight animals from each of the two burn groups (burn/placebo and burn/E2) were sacrifi ced at 24 hours and at 7 days, respectively (sham group at 7 days only), with four each of the two burn groups sacrifi ced at 45 days. Brain tissue samples were analyzed by ELISA for SHH.
Results Mean levels of SHH levels were signifi cantly elevated within 24 hours as much as 45 days post injury in burned animals receiving the 17β-estradiol (>1,200 pcg/mg) as compared with the placebotreated burned animals (<700 pg/mg) and controls (<300 pcg/mg). See Figure 1 .
Conclusion Early, single-dose estrogen administration following severe burn injury signifi cantly elevated levels of SHH in brain tissue. This fi nding may represent an extremely novel and important pathway for both neuroprotection and neuroregeneration in burn patients.
Introduction Many proposed resuscitative therapies for cardiac arrest and trauma will require the earliest possible intervention and would occur under volatile circumstances, making true informed consent for clinical trials unfeasible. The purpose here was to report our experience using exception to informed consent during the inaugural pilot study of infusing estrogen for acute injury, the so-called RESCUE Shock study. Methods Fifty patients were enrolled in RESCUE Shock in which estrogen or placebo was infused as soon as possible in the emergency department for trauma patients with a low systolic blood pressure (<90 mmHg) at two level I trauma centers. They were all treated with a single-dose estrogen or placebo infusion within 2 hours using exception from informed consent following US federal guidelines. Results Investigator-initiated exception from informed consent studies is feasible, with our FDA IND approval obtained in 31 days, IRB 1 approval in 25 days, and IRB 2 approval in 24 days. Community consultation/notifi cation was successfully accomplished with no one opting out and 47/50 enrolled patients or their legal representatives were notifi ed of participation (one died unidentifi ed, two died with no known contact). The average number of days to verbal notifi cation of patients or advocates was 6.55 days (range 0 to 51 days) as the study team began notifi cation only after the patient or family was able to reasonably understand information about the study. No one decided against continued follow-up. Overall, patients and their families were very enthusiastic about participation and the data safety monitoring board had no safety concerns after reviewing all study data. Conclusion Although delayed notice of participation occurs for many justifi able reasons, the use of exception from informed consent for novel, time-sensitive resuscitation studies is not only crucial, but can be feasible, and well accepted by patients, their advocates and communities at large.
Introduction Patients with severe burn injury experience a rapid elevation in multiple circulating proinfl ammatory cytokines, with the levels correlating with both injury severity and outcome. In animal Critical Care 2013, Volume 17 Suppl 2 http://ccforum.com/supplements/17/S2 S109 models, accumulations of these cytokines have been observed in remote organs, including the heart, brain and lungs. However, data are lacking regarding the long-term levels of cytokines in the heart following severe burn injury and also how infusion of parenteral estrogen, a powerful anti-infl ammatory agent, would aff ect these levels. Using a rat model, we studied the eff ects of a full-thickness thirddegree burn on cardiac levels of IL-6 and TNFα over 45 days with and without 17β-estradiol infusion. Methods A total of 168 male rats were assigned randomly to one of three groups: (1) Conclusion Following severe burn injury in an animal model, an early single dose of estrogen can decrease the prolonged let alone the early onset of cardiac infl ammation. Based on these data, clinical studies of estrogen infusions should be seriously entertained as estrogen may not only be an inexpensive, simple adjunctive therapy in burn management, it may also obviate the need for many subsequent interventions altogether and even diminish mortality. Conclusion The results of this study highlight the risk factors for the development of complications following blunt chest trauma. A risk stratifi cation tool has also been developed that could assist in the prediction of poor outcomes in this patient group. The next stage is to complete a prospective validation study. Reference
Introduction We have reported the risk of chest drain insertion inferior to the diaphragm when using current international guidelines [1] . Another complication is damage to signifi cant peripheral nerves, such as the long thoracic nerve causing winging of the scapula [2] . We assessed these risks using: the European Trauma Course method, a patient's handbreadth below their axilla just anterior to the midaxillary line; the British Thoracic Society safe triangle [3] ; and the Advanced Trauma Life Support (ATLS) course guidance [4] . Methods We used the above guidelines to place markers (representing chest drains) in the thoracic wall of 16 cadavers bilaterally (32 sides), 1 cm anterior to the midaxillary line. Subsequent dissection identifi ed the course and termination of the long thoracic nerve, the site of lateral cutaneous branches of intercostal nerves, and their relation to the markers.
The long thoracic nerve was found in the fi fth intercostal space in 16 of 32 cases, always in or posterior to the midaxillary line. Contrary to the description in Grays' Anatomy (40th edition) it terminated before the inferior border of serratus anterior. Most commonly it was found to end by branching in the fourth (right) or fi fth (left) intercostal space (range third to sixth). Lateral cutaneous branches of intercostal nerves were found in the fi fth intercostal space in 25 of 32 cases. Contrary to the description in Last's Anatomy (12th edition) they always passed anterior to the midaxillary line (and marker). Conclusion Placement 1 cm anterior to the midaxillary line minimises risk to the long thoracic nerve and lateral cutaneous branches of intercostal nerves. We therefore conclude that not all areas of the British Thoracic Society safe triangle are indeed safe, and anteroposterior placement should follow the European Trauma Course and ATLS guidelines: just anterior to the midaxillary line (for example, 1 cm).
Introduction Whole body computed tomography (WBCT) appears to be useful for the early detection of clinically occult injury, although its indications have been controversial. The purpose of this study was to develop a clinical prediction score to clarify the indications for blunt trauma patients with multiple injuries (MI) who require WBCT.
Methods We conducted a retrospective study of 173 patients with blunt trauma who underwent WBCT at our emergency center between June 2011 and July 2012. We chose the presence or absence of MI (Injury Severity Score ≥15) in need of surgical intervention as the outcome variable. We used bivariate analyses to identify variables potentially predicting the presentation of MI. The predictor variables were confi rmed by multivariate logistic regression analyses. We assigned a score based on the corresponding coeffi cients. Results Among the 173 patients enrolled, 53 were in the MI group. Four predictors were found to be independently signifi cant by the logistic analysis: (1) body surface wound ≥3 regions, (2) positive focused assessment with sonography for trauma, (3) white blood cell count ≥11,000/μl, and (4) D-dimer ≥8 μg/ml. Score 1 was assigned to predictor (1), score 2 was assigned to predictors (2), (3) and (4). A prediction score was calculated for each patient by adding these scores. The area under the receiver operating characteristic curve was 0.89. No patients with a score of 1 or less had MI (Figures 1 and 2) . Conclusion In patients with a score of 1 or 0, the presence of MI is less likely. These patients may not require WBCT, and selective CT scans of body parts based on clinical presentation should be considered. (Figure 1) . The most common intervention as a result of the ultrasound was initiation of a pressor infusion (33.3%), of which 71.4% were ionotropes. Additional therapies included blood transfusion (4.8%), heparin (9.5%), tPA (4.8%), cardiac catheterization (4.8%), and surgery (9.5%). ROSC was achieved in 37.5% of patients; average time to ROSC was 13 minutes. A total 33.3% of patients who underwent ALS were alive at hospital discharge and 28.6% at 1 year. Conclusion Focused cardiac ultrasound is a feasible adjunct to ALS resuscitation and may assist in the early identifi cation of reversible causes of cardiac arrest. Care must be taken to ensure no interruptions to cardiac compressions are made by performance during pulse checks. Further studies are needed to examine the outcomes associated with its integration into resuscitations.
Introduction In this case report, we describe a patient who presented with a cardiac arrest as a result of an obstructive shock, which progressed into cardiac arrest, caused by an acute para-esophageal gastric herniation. Methods Our patient, with a medical history of a laparoscopic repair of a symptomatic diaphragmatic hernia 6 months prior, presented herself at the emergency department with pain in the upper abdomen and nausea. The physical examination, laboratory tests and X-ray of the thorax were normal and she was sent home. Twenty-four hours later paramedics were summoned to our patient because of increased complaints. On arrival of the paramedics she had a normal electrocardiogram (ECG) and during the transfer from her bed to the stretcher she collapsed due to pulseless electric activity (PEA), for which cardiopulmonary resuscitation was started. Sinus rhythm and output was regained after several minutes and the patient was transported to the hospital. At arrival in the hospital, the X-ray of the thorax showed an intrathoracic stomach and a signifi cant mediastinal shift to the right. Results After emergency laparotomy, which concerned correcting the gastric herniation and resection of an ischemic part of stomach, the patient remained hemodynamically stable. Cardiac ischemia was ruled out based on ECG, laboratory fi ndings, cardiac ultrasound and cardiac computed tomography. The ultrasound in the emergency department did show a distended right ventricle and normal left function, which disappeared later (after repositioning the stomach), which is evidence for the mediastinal shift as a cause for the PEA. Conclusion We are the fi rst to describe a patient requiring cardiopulmonary resuscitation for progressive obstructive shock, due to an intrathoracic stomach. Especially after a laparoscopic repair of a diaphragmatic hernia, this is a rare cause for shock and cardiac arrest, which requires a diff erent medical approach. 
is a key factor in improving survival from out-of-hospital cardiac arrest (OOH-CA). The ALERT algorithm, a simple and eff ective compression-only telephone CPR protocol, has the potential to help bystanders initiate CPR. This study evaluates the eff ectiveness of the implementation of this protocol in the Liege dispatching centre. Methods We designed a before-and-after study based on a 3-month retrospective assessment of the adult victims of OOH-CA in 2009, before the implementation of the ALERT protocol in the Liege dispatching centre, and the prospective evaluation of the same 3-month period in 2011, immediately after the implementation of this protocol. Data were extracted from ambulance, paramedical and medical intervention teams fi les, as well as the audio recordings of the dispatching centre. Conclusion In OHCA patients with unshockable initial rhythm, prehospital epinephrine administration signifi cantly increased the rate of survival at 1 month after cardiac arrest. The best single predictor for favorable neurological outcomes at 1 month following prehospital epinephrine administration after cardiac arrest was age (<66 years) followed by total dose of epinephrine (1 mg) and then by call-response time (<5 minutes). [3] . Methods This was a single-center retrospective cohort study of patients who suff ered OHCA and were transported to our hospital between April 2009 and March 2011. We investigated the patients' characteristics, whether they met the TOR criteria, and their outcome at the time of hospital discharge. Results A total of 195 patients (mean age, 69 years), 67% of whom were male, were transported to our hospital after suff ering OHCA. Cardiopulmonary arrest was witnessed in 52 cases (27%). The 2010 AHA Guidelines for CPR and ECC regarding the criteria for TOR were applied in 126 cases (65%), of whom 113 (90%) were dead on arrival, and 13 were successfully resuscitated and admitted. The outcomes for these 13 patients were as follows: 10 died in the hospital, two patients were discharged with a Glasgow Pittsburgh Cerebral Performance Category (CPC) score of 1, and one patient was transferred to another hospital with a CPC score of 3. Conclusion In our study, 65% of the patients who were transported to the hospital after OHCA met the criteria for TOR. Outcomes for patients who met the TOR criteria were signifi cantly worse than those who did not meet the criteria (2.4% vs. 14.5%, P <0.005). In Japan, eff orts are made to resuscitate almost all individuals who suff er OHCA, but 75% of those patients die within a day. In light of the fact that even the medical cost for each of these patients who die within a day amounts to US$1,500 [4] , the introduction of TOR will have a particularly strong impact in Japan.
Introduction Detection and treatment of cardiopulmonary arrest and their antecedents may be less eff ective at night and weekend than weekdays because of hospital staffi ng and response factors [1] . Early detection and resuscitation of cardiopulmonary arrest are crucial for better clinical outcome. We conducted our study to evaluate event survival of in-hospital cardiopulmonary arrest after regular working hours in nonmonitored areas of a tertiary-care center. = 17) , hypoxia (n = 10), cardiac other (n = 5), sepsis (n = 4), arrhythmia (n = 3) and PE (n = 3). In two IHCA patients more than one likely cause of arrest was reported and in 19 cases no cause was identifi ed. The presenting rhythm was ventricular fi brillation (VF) in 45.3% (n = 29), pulseless electrical activity in 32.8% (n = 21) and asystole in 20.3% (n = 13). A total of 9.4% (n = 6) were thrombolysed and one (1.6%) patient was referred for emergency PCI.
Conclusion As previously reported [2] , IHCA was associated with a worse prognosis than OHCA. The OHCA survival rate was better than reported elsewhere [3] . The percentage of IHCA attributed to MI was low. Only one OHCA patient was referred for emergency PCI. Routine coronary angiography with ad hoc PCI in VF OHCA has been associated with increased survival [4] . Greater availability of PCI post OHCA could further improve mortality in patients with a primary cardiac pathology. Further investigation should include management of noncardiogenic cardiac arrest.
Introduction Mild therapeutic hypothermia (MTH) is the most powerful therapy to improve survival and neurologic outcome after out-ofhospital cardiac arrest. Such benefi t may also occur for unconscious patients after in-hospital cardiac arrest. The aim is to compare 1-year evolution of neurological outcomes of patients treated with MTH after in-hospital versus out-of-hospital cardiac arrest. Methods A prospective study of patients treated with MTH after cardiac arrest in a community hospital in São Paulo, Brazil. After return of spontaneous circulation, unconscious survivors received MTH using topical ice and cold saline infusions in order to achieve a 32 to 34°C goal temperature, within 6 hours of cardiac arrest, and maintained 
in the management of out-of-hospital cardiac arrest (OHCA) is not clear cut [1] . It has historically been used in patients with ST elevation on post-resuscitation electrocardiogram (ECG) although this is a poor predictor of acute coronary occlusion after cardiac arrest [2] . This study investigates the benefi t of PCI regardless of post-resuscitation ECG. Benefi t is widely claimed for therapeutic hypothermia, so cooling parameters were included. Methods We analysed all 41 consecutive adults admitted post OHCA to a university hospital ICU between January 2010 and December 2011. Patients received PCI regardless of ECG changes. A Cox proportional hazards model was used to determine the relationship between PCI, cooling and survival to discharge. Routinely collected data such as demographics and details of resuscitation (OHCA Utstein data) were also included. Results Survival to hospital discharge was 41% with 29% of survivors discharged to a neurological rehabilitation centre. Multivariate analysis using a Cox proportional hazards model showed PCI to be an independent predictive factor of survival, unrelated to ECG (hazards ratio, 0.0583; 95% CI, 0.0076 to 0.4485). Cooling had no signifi cant impact on patient survival. See Figure 1 .
Conclusion In this small retrospective study primary PCI appears to be an independent predictor of survival after OHCA. This is consistent with other studies suggesting benefi t for primary PCI regardless of the post-resuscitation ECG [3] . Cooling was not found to improve survival to discharge but further analysis is required to determine impact on neurological function.
Introduction Sedation and therapeutic hypothermia (TH) modify neurological examination and alter prognostic prediction of coma after cardiac arrest (CA). Additional tools, such as EEG and evoked potentials, improve prediction of outcome in this setting, but are not widely available and require signifi cant implementation. Methods Using a new device for infrared pupillometry, we examined the value of quantitative pupillary light reactivity (PLR) to predict outcome in comatose post-CA patients treated with TH. Twenty-four comatose CA patients treated with TH (33°C, 24 hours) were prospectively studied. The percentage variation in PLR was measured during TH (12 hours from CA), using the NeuroLight Algiscan® (IDMED, Marseille, France). For each patient, three consecutive measures were performed and the best value was retained for analysis. The relationship of PLR with survival and neurological outcome (CPC scores) at 3 months was analyzed, and the predictive value of PLR was compared with that of standard clinical examination (motor response and brainstem refl exes) performed at 48 hours from CA. Results Quantitative PLR was strongly associated with survival (median left-eye PLR 14% (11 to 16%) variation in survivors vs. 5.5% (4 to 8.5%) in nonsurvivors, P < 0.0001) and 3-month neurological outcome (14% (11 to 21%) in patients with CPC 1 to 2 vs. 5.5% (4 to 8.5%) in those with CPC 3 to 5, P <0.0001). Comparable fi ndings were obtained using right-eye PLR. A PLR >10% was 100% predictive of patient prognosis, with false-positive and false-negative rates of 0% for outcome. Clinical examination was signifi cantly associated with outcome; however, motor response (MR) and brainstem refl exes (BRS) yielded higher falsepositive and false-negative rates than PLR (Table 1) .
Conclusion Quantitative PLR appears highly accurate and superior to standard neurological examination to predict outcome in patients with post-CA coma. Further study is warranted to confi rm these promising fi ndings. Acknowledgements Supported by Grants from the Swiss National Science Foundation (FN 320030_138191) and the European Critical Care Research Network (ECCRN). Figure 1 . MV was associated with a signifi cant reduction of SctO 2 from baseline (75% (73 to 76) to 69% (67.5 to 71.5), P <0.001).
No signifi cant changes in SctO 2 were found after IH (74 (72 to 76) vs. 75 (73 to 75.5), P = 0.24). Conclusion Moderate HV was associated with signifi cant reduction in cerebral saturation, whilst IH may be detrimental after CA and TH, whilst increasing MAP to supranormal levels with vasopressors does not improve cerebral oxygenation. These data stress the importance of strict control of PaCO 2 following CA and TH to avoid secondary cerebral ischemic insults.
Introduction After cardiac arrest, microcirculatory reperfusion dis orders develop despite adequate cerebral perfusion pressure. Increased blood viscosity strongly hampers the microcirculation, resulting in plugging of the capillary bed, arteriovenous shunting and diminished tissue perfusion. The aim of the present study was to assess blood viscosity in relation to cerebral blood fl ow in patients after cardiac arrest.
Methods We performed an observational study in 10 comatose patients after cardiac arrest. Patients were treated with hypothermia for 24 hours. Blood viscosity was measured ex vivo using a Contraves LS300 viscometer. Mean fl ow velocity in the middle cerebral artery (MFVMCA) was measured by transcranial Doppler (TCD) at the same time points. <0.001) . There was a signifi cant association between viscosity and the MFVMCA (P = 0.0019). See Figure 1 . Conclusion Viscosity decreases in the fi rst 3 days after cardiac arrest and is strongly associated with an increase in cerebral blood fl ow. Since viscosity is a major determinant of cerebral blood fl ow, repeated measurements may guide therapy to help restore cerebral oxygenation after cardiac arrest. In preliminary data, we report that SR >75 might correlate with bad outcome and that combining NSE and SR might improve the predictive value. Also, low NSE and good initial BIS values correlate with preserved cerebral potential and should encourage the clinician.
Introduction Accurate prediction of neurological outcome after cardiac arrest is desirable to prevent inappropriate withdrawal of lifesustaining therapy in patients who could have a good neurological outcome, and to limit active treatment in patients whose ultimate neurological outcomes are poor. Established guidelines to predict neurological outcome after cardiac arrest were developed before the widespread use of therapeutic hypothermia. The American Association of Neurology guidelines [1] currently recommend that absent or extensor motor scores on day 3 post arrest are reliable indicators or poor neurological outcome with a false positive rate of 0 to 3%. Methods A review of existing literature was undertaken to examine whether the utility of motor scores to predict poor neurological outcome is infl uenced by the use of therapeutic hypothermia. Results Six studies were identifi ed [2] [3] [4] [5] [6] [7] that investigated the use of motor scores on day 3 post cardiac arrest in patients who had received therapeutic hypothermia. False positive rates (defi ned as 1 -specifi city) for predicting poor neurological outcome were calculable in fi ve of the six studies [2] [3] [4] [5] [6] and were 14%, 24%, 11%, 25% and 12% respectively. In all studies the FPR for motor scores of extension or worse were signifi cantly higher than the 0% (0 to 3% 95% CIs) in the AAN guidelines.
Conclusion Motor scores at day 3 post cardiac arrest of extension or worse do not reliably predict poor neurological outcome when therapeutic hypothermia has been used. Clinical neurological fi ndings may not be valid predictors of poor neurological outcome after therapeutic hypothermia.
Introduction It has been reported that the young are much more resistant to transient cerebral ischemia than the adult. Methods In the present study, we compared the chronological changes of calcium binding proteins (CBPs) (calbindin 28k (CB-D 28k), calretinin (CR) and parvalbumin (PV)) immunoreactivities and levels in the hippocampal CA1 region of the young gerbil with those in the adult following 5 minutes of transient cerebral ischemia induced by the occlusion of both the common carotid arteries.
In the present study, we examined that about 90% of CA1 pyramidal cells in the adult gerbil hippocampus died at 4 days post ischemia; however, in the young hippocampus, about 56% of them died at 7 days post ischemia. We compared immunoreactivities and levels of CBPs, such as CB-D 28k, CR and PV. The immunoreactivities and protein levels of all the CBPs in the young sham were higher than those in the adult sham. In the adult, the immunoreactivities and protein levels of all the CBPs were markedly decreased at 4 days post ischemia; however, in the young, they were apparently maintained. At 7 days post ischemia, they were decreased in the young; however, they were much higher than those in the adult. Conclusion In brief, the immunoreactivities and levels of CBPs were not decreased in the ischemic CA1 region of the young 4 days after transient cerebral ischemia. This fi nding indicates that the longer maintenance of CBPs may contribute to a less and more delayed neuronal death/ damage in the young.
delay in reaching target temperature [1] . We hypothesize that early and rapid induction of hypothermia will mitigate neuronal injury and improve survival in a swine model of TBI. Methods Twenty domestic cross-bred pigs (34 to 35 kg) were subjected to a 5 ATM (100 ms) lateral fl uid percussion TBI. The brain temperature and ICP were measured using Camino®. Serum biomarkers for neuronal injury -S-100β, neuron-specifi c enolase, glial fi brillary acid protein (GFAP), and neurofi laments heavy chain -were measured daily using enzyme-linked immunosorbent assay. Twelve of the injured animals were rapidly cooled to 32°C within 90 minutes of the injury using a transpulmonary hypothermia technique [2] . Hypothermia was maintained for 48 hours. Eight injured control animals were maintained at 37°C. In both groups, anesthesia (isofl urane 1%) was discontinued and the animals were weaned off the ventilator after 48 hours. Five days post injury, the surviving animals were euthanized and necropsied. The data were analyzed using a log-rank (Mantel-Cox) test, and ANOVA. Results Ten of the 12 hypothermia and four of the eight normothermia animals survived to the end of the 5-day study (χ 2 = 2.597, df = 1, P = 0.1071). Although the probability of type I error between survival curves was 11%, the study was clinically signifi cant and showed a clear trend toward improved survival with hypothermia. The intracranial pressures were signifi cantly (P <0.05) lower in the hypothermia group. Both interventions -that is, general anesthesia and hypothermiamitigated the rise of serum biomarkers following TBI. However, the suppression of biomarkers was sustained during the recovery period only in the hypothermia group. With the exception of the GFAP levels, the curves of all biomarkers were signifi cantly diff erent between the groups. Conclusion Our preliminary fi ndings show early initiation, rapid induction, and prolonged maintenance (48 hours) of cerebral hypothermia to lower intracranial pressure, blunt the rise in serum biomarkers, and improve survival following TBI. References Introduction Traumatic brain injury (TBI) is a contributing factor to approximately one-third of all injury-related deaths in the USA annually. Updated statistical records for TBI in Egypt are lacking. The current research is aiming to estimate the prevalence of TBI in Egypt in order to develop a comprehensive TBI prevention program. Methods A 1-year period (one calendar month every quarter of 2010) descriptive epidemiological study of moderate and severe TBI cases admitted to the emergency department, Cairo main university hospital. The data collection sheet included personal data (age, sex and residency), incident-related data (cause, nature and time of injury) and both clinical and radiological fi ndings. Introduction One of the most used prognostic models for traumatic brain injury is the IMPACT-TBI model, which predicts 6-month mortality and unfavorable outcome. Our aim was to study whether adding markers of coagulation improves the model's predictive power when accounting for extracranial injury.
Methods Patients with a TBI admitted to a designated trauma center in 2009/10 were screened retrospectively and included according to the IMPACT study criteria. The predictive outcome was calculated for included patients using the full IMPACT-TBI model. To assess coagulopathy and extracranial injury we used the prothrombin time percentage (PT), platelet count (10 9 ), and injury severity score (ISS Introduction Evidence suggest that endogenous lactate, produced by aerobic glycolysis, is an important substrate for neurons, particularly in conditions of increased energy demand. This study aimed to examine brain lactate metabolism in patients with severe traumatic brain injury (STBI). Methods A prospective cohort of STBI patients monitored with cerebral microdialysis (CMD) and brain tissue oxygen (PbtO 2 ) was studied. Brain lactate metabolism was assessed by quantifi cation of elevated CMD lactate samples (>4 mmol/l). These were matched to pyruvate and PbtO 2 , and dichotomized as hyperglycolytic (CMD pyruvate >119 μmol/l) versus nonhyperglycolytic or as hypoxic (PbtO 2 <20 mmHg) versus nonhypoxic. Data were expressed as percentages per patient. Global brain perfusion (categorized as oligemic, normal or hyperemic) was assessed with CT perfusion (CTP). Results Twenty-four patients (total 1,782 CMD samples) were studied. Samples with elevated CMD lactate were frequently observed (41 ± 8% SEM of individual samples). Brain lactate elevations were predominantly hyperglycolytic (73 ± 8.2%), whilst only 14 ± 6.3% of them were hypoxic. Trends over time of both lactate patterns are shown in Figure 1 . On CTP (n = 17; average 48 hours from TBI) hyperglycolytic lactate was always associated with normal or hyperemic CTP, whilst hypoxic lactate was associated with oligemic CTP (Table 1) . 
Our fi ndings suggest predominant nonischemic lactate release after TBI and identify, for the fi rst time, an association between cerebral hyperglycolytic lactate production and normal to supranormal brain perfusion. Our data support the concept that lactate may be used as energy substrate by the injured human brain.
In the prehospital setting, it is diffi cult to use the Glasgow Coma Scale (GCS) to evaluate the consciousness state using in pediatric patients with severe trauma. The Japan Coma Scale (JCS) is a consciousness scale used widely in Japan and, with its four grades, is simpler and quicker to use than the GCS. This study examined whether 
Our study identifi ed a moderate relation between PEEP and OSND and a weaker one between Ppeak, Pm and OSND. Thus, in selected cases OSND could serve as a bedside marker of eff ect of airway pressure to ICP. Yet, larger studies are needed to come to a safer conclusion. Reference Introduction Following primary neurological insult, initial manage ment of traumatic brain-injured (TBI) patients has a clearly defi ned pathway [1] . However, after arrival at tertiary centers, further manage ment is not standardized. Intracranial hypertension (ICH), systemic hypotension, hypoxia, hyperpyrexia and hypocapnia have all been shown to independently increase mortality [2] . Despite numerous studies, there is currently no level 1 evidence to support any specifi c management [3] . Our objective was to provide an overview of the current clinical management protocols in the UK. Methods Thirty-one ICUs managing patients with severe TBI were identifi ed from the RAIN (Risk Adjustment In Neurocritical care) study, and a telephone survey was conducted.
Results A total 97% of units used a cerebral perfusion pressure protocol for the initial management, with 83% targeting pressures of 60 to 70 mmHg and 17% aimed for >70 mmHg. Ninety-one percent of units monitored CO 2 routinely with 61% targeting CO 2 of 4.5 to 5 kPa (Figure 1 ). Regarding osmotherapy, mannitol was still the preferred agent, with 48% of units using it as fi rst line; 32% used hypertonic saline, while 20% of units used either depending on clinicians' preference. Sixteen percent questioned were currently enrolled on the Eurotherm hypothermia trial, while 16% never used hypothermia and one unit used prophylactic hypothermia routinely. The remaining 65% of units used hypothermia only to manage refractory ICH. Conclusion There is no clear consensus on the initial targets used. The surviving sepsis campaign showed that protocol-led care can reduce mortality [4] . Perhaps it is time for a similar approach to be adopted, with specialists coming to together to review the evidence and formulate guidelines that can then be tested.
Introduction Traumatic brain injury (TBI) is a major cause of permanent disability and death in young patients. Controversy exists regarding the optimal cerebral perfusion pressure (CPP) required in TBI management. A tool for monitoring autoregulation and determining an optimal CPP is the pressure reactivity index (PRx), defi ned as a moving correlation coeffi cient between the mean arterial blood pressure (MAP) and intracranial pressure (ICP) at a frequency of at least 60 Hz. This requirement of high frequency has constrained its use to a few academic centers. An association was shown between outcome and continuous optimal CPP based on 4 hours of PRx [1] . We present a novel low-frequency autoregulation index (LAx), based on correlations between ICP and MAP at a standard minute-by-minute time resolution. Methods A total of 182 patients from the Brain-IT [2] multicentre European database had registered outcome and ICP and MAP for the fi rst 48 ICU hours. Twenty-one TBI patients admitted to the university hospitals of Leuven, Belgium and Tubingen, Germany were continuously monitored using ICM+ software (Cambridge Enterprise) allowing for continuous PRx calculation. Autoregulation indices versus CPP plots for PRx and LAx were computed to determine optimal CPP every minute during the fi rst 48 ICU hours [1] .
Results On the Brain-IT database, LAx resulted in an optimal CPP for 90% of the fi rst 48 hours. Table 1 shows recommendations with respect to outcome. In the Leuven-Tübingen database, PRx and LAx resulted in 44% and 92% recommendations respectively. The average diff erence between methods was 5.26 mmHg. Conclusion The diff erences in optimal CPPs derived from PRx and LAx were not clinically signifi cant. LAx allowed for recommendations to be computed for longer periods. Signifi cantly better outcome (Table 1) was observed in patients for whom optimal CPP derived from LAx was maintained.
Introduction Pediatric patients with altered mental status (AMS) present with poor histories resulting in delayed testing and potential poor outcomes. Non-invasive detection for altered cerebral physiology related to TBI would improve resuscitation and outcome. Cerebral rSO 2 (r c SO 2 ) studies demonstrate its utility in certain neurological emergencies.
Methods A retrospective analysis of r c SO 2 utility in AMS. rcSO 2 data were collected every 30 seconds for AMS patients who had a head CT. Patients with a negative head CT were compared with those with an abnormal head CT. ROC analysis was performed to fi nd the AUC for each summary statistic and performance characteristics. Subgroup analysis was done to determine whether r c SO 2 predicted injury and location. Results r c SO 2 readings across 5, 15, 20, and 30 minutes were stable (Figure 1 ). r c SO 2 readings with one or both sides <50% or a wide diff erence between L and R cerebrum was predictive of an abnormal CT scan. A mean diff erence of 4.2 was 82% sensitive for detecting a CT lesion with 62% specifi city, 88% PPV, and 52% NPV; a mean diff erence of 12.2 was 100% specifi c for an abnormal head CT. Lower mean r c SO 2 readings localized to the CT pathology side, and higher r c SO 2 readings trend toward the EDH group. Conclusion Cerebral rcSO 2 monitoring can non-invasively detect altered cerebral physiology and pathology related to TBI as the cause for pediatric altered mental status. The utility of r c SO 2 monitoring has shown its potential for localizing and characterizing intracranial lesions among these altered children. Further studies utilizing r c SO 2 monitoring as an adjunct tool in pediatric altered mental status evaluation and management are ongoing. Introduction Fever is a dangerous secondary insult for the injured brain [1] . We investigated the cerebral and hemodynamic eff ects of intravenous (i.v.) paracetamol administration for the control of fever in neurointensive care unit (NICU) patients. Methods The i.v. paracetamol (1 g in 15 minutes) was administered to NICU patients with a body temperature (Temp.) >37.5°C. Its eff ects on mean arterial pressure (MAP), heart rate (HR), intracranial pressure (ICP), cerebral perfusion pressure (CPP), jugular venous oxygen saturation (SjVO 2 ) and Temp. were recorded at the start of paracetamol infusion (T0) and after 30 (T30), 60 (T60) and 120 (T120) minutes. Interventions for the maintenance of CPP >60 mmHg or ICP <20 mmHg were recorded. (Figure 1 ). In fi ve cases norepinephrine infusion was started for CPP <60 mmHg. In another two cases, for the same reason, the norepinephrine dosage was augmented. The proportion of patients who had infusion of norepinephrine increased from 42.8% at T0 to 78.6% at T120 (P = 0.02, chi-square for trends).
Conclusion Use of i.v. paracetamol is eff ective in the maintenance of normothermia in acute brain-injured patients. However, adverse hemodynamic eff ects, which could represent a secondary insult for the injured brain, must be rapidly recognized and treated. Reference Introduction Evaluating resource utilization is paramount in critically ill patients with traumatic brain injury (TBI), but little is known on readmissions after hospital discharge. We evaluated rates and determinants of unplanned readmission following TBI. Methods We conducted a multicenter retrospective cohort study from April 1998 to March 2009. Data were obtained from a Canadian provincial trauma system, based on mandatory contribution from 59 trauma centres, and a hospital discharge database. Patients aged ≥16 years with TBI (ICD-9 or ICD-10 codes of 850-854 and S06, respectively) were included. Patients who died during the index hospitalization, who lived outside the province, who could not be linked with the hospital discharge database were excluded. We collected baseline and trauma characteristics, hospital admissions in the 12 months preceding index admission, and readmissions in the 12 following months. Primary outcome was unplanned readmission 30 days, 3 months and 6 months post discharge. We evaluated sociodemographic and clinical factors associated with readmissions using a logistic regression model. Results Among 18,342 adult patients with TBI identifi ed in the registry, 14,777 patients were included among which 2,363 had severe, 1,106 moderate and 11,308 mild traumatic brain injury. Most patients were young (mean age: 52 ± 23 years) and had no comorbidity (73.6%). Overall, 1,032 patients (7.0%) were readmitted within 30 days, 12.7% within 3 months and 17.6% within 6 months. At 30 days post discharge, 311 (30.1%) were readmitted for a complication. The median length of stay was 8 days (Q1 to Q3: 3 to 20). More than 10% of patients aged ≥75 years with ≥1 comorbidity or with ≥1 admission prior to index hospitalisation were readmitted. The severity of the TBI was not an independent predictor of readmission. Age, highest AIS, number of comorbidities, number of admissions prior to index hospitalization, level of index trauma center and discharge destination were associated with readmissions on multivariate analysis. Conclusion Readmissions in the months following TBI are frequent, but were not found to be associated with the TBI severity. Further studies evaluating reasons for readmission are warranted in order to develop strategies to prevent such events. Introduction Pituitary disorders following traumatic brain injury (TBI) are frequent, but their determinants are poorly understood. We performed a systematic review to assess the risk factors of TBIassociated pituitary disorders. Methods We searched MEDLINE, Embase, Scopus, The Cochrane Library, BIOSIS, and Trip Database, and references of narrative reviews for cohort, cross-sectional and case-control studies enrolling at least fi ve adults with TBI in whom ≥1 pituitary axis was tested and one potential predictor reported. Two independent investigators selected citations, extracted data and assessed the risk of bias. We pooled the data from all studies assessing a specifi c predictor, regardless of the pituitary axis being evaluated. When more than one pituitary axis was assessed, we used the data related to hypopituitarism or the data from the most defective axis. When a pituitary axis was evaluated several times, we used assessment farthest from the injury. A meta-analysis was performed using random eff ect models and I 2 was used to evaluate heterogeneity. Introduction Prevention of secondary neurologic injuries is paramount for improved neurologic outcomes after traumatic brain injury (TBI). Evidence suggests that although therapeutic hypothermia (TH) lowers intracranial pressure and attenuates secondary cerebral insults after TBI [1] , it also induces hypotension. Brief episodes of mild hypotension in brain-injured patients can trigger secondary injuries, which have been associated with increased mortality in patients with TBI [2] . Vasopressin mitigates hypotension in septic shock and improves coronary perfusion in hypothermic cardiac arrest models [3] . We hypothesized that a lowdose vasopressin infusion may reduce the cumulative epinephrine dose in hypothermic, brain-injured swine. Methods Six domestic cross-bred pigs were subjected to epinephrine infusion after general anesthesia, standardized TBI and transpulmonary hypothermia (32°C for 48 hours). All animals received the same care, aiming for a mean arterial pressure >60 mmHg. At hour 24, animals received additional vasopressin infusion at 0.04 units/minute. We measured the cumulative epinephrine dose for each animal pre and post vasopressin infusion ( Figure 1 ) and performed a twosample Wilcoxon rank-sum test, comparing the median cumulative epinephrine doses in the two groups.
The median cumulative epinephrine dose in the animals that received the vasopressin infusion was 715 mg with a 25th to 75th interquartile range (IQR) of 320 to 930 mg. The median cumulative epinephrine dose in the control group was 2,044 mg (IQR 1,640 to 2,344 mg). This was statistically signifi cant (P = 0.003), based on the Wilcoxon rank-sum test. Conclusion A low-dose infusion of vasopressin can signifi cantly reduce vasopressor requirements and improves hemodynamics in hypothermic, brain-injured swine. This hemodynamic stability may improve neurological outcomes.
Introduction Severe traumatic brain injury (TBI) is a major cause of death and of severe neurologic sequelae. Long-term functional outcome of TBI and its best timing of assessment are not well understood, and may be evaluated too prematurely in clinical studies because of resources required to do so without too much missing data. Hence, we conducted a systematic review of studies in severe TBI patients to evaluate the long-term functional outcome. We hypothesized that functional impact measured by the Glasgow Outcome Scale (GOS), or the extended version (GOSe), may plateau after several months in patients with severe TBI. Methods We performed a systematic review of randomized controlled trials and cohort studies (prospective and retrospective) in patients with severe TBI. We searched MEDLINE, Embase, Cochrane Central, BIOSIS, CINAHL and Trip Database from their inception to December 2011. References of included studies were searched for additional studies. Two reviewers independently determined study eligibility and collected data. The primary outcome measure was the proportion of unfavourable functional outcome (GOS 1 to 3 or GOSe 1 to 4) at 6 to 12 months, 12 to 18 months, 18 to 24 months and more than 24 months after severe TBI. We calculated Freeman Tukey-type arcsine squareroot transformations and pooled data using random-eff ect models. Heterogeneity was assessed with the I 2 test and sensitivity analyses were based on a priori hypotheses.
In total, 4,432 studies were assessed for eligibility; 209 studies (n = 31,540) were included. In the 188 studies using the GOS, a poor functional outcome was observed in 56.6% (95% CI = 54.0 to 59.1%, I 2 = 91%), 51.9% (95% CI = 38.0 to 59.0%, I 2 = 84%), 57.0% (95% CI = 48.2 to 55.5%, I 2 = 93%) and 56.9% (95% CI = 48.2 to 65.1%, I 2 = 93%) of patients at 6 to 12 months, 12 to 18 months, 18 to 24 months and beyond 24 months, respectively. In the 18 studies using GOSe, a poor functional outcome was observed in 62.9% of patients at 6 to 12 months (95% CI = 55.9 to 69.2%, I 2 = 90%) and 54.6% at 12 to 18 months (95% CI = 43.2 to 65.8%, I 2 = 90%). Heterogeneity was present in most analyses and was not entirely explained by the planned sensitivity analyses. Conclusion Considering that the incidence of patients with an unfavourable outcome remained constant at diff erent assessments, a follow-up of severe TBI patients longer than 12 months does not provide incremental information. Functional outcomes measured longer than 12 months after the injury may not be warranted in clinical studies. Introduction Prevention of secondary brain injury is the cornerstone in the management of patients with severe traumatic brain injury (TBI) and raised intracranial pressure (ICP). Although a variety of monitoring methods are available, due to lack of strong evidence their use varies considerably [1] . The objective of this survey was to provide an overview of the current practice in monitoring of patients with severe TBI in all neuro-ICUs across the UK. Introduction Pulmonary complications are frequently occurring medical complications after aneurysmal subarachnoid hemorrhage (aSAH) [1] . Early respiratory deterioration (ERD) may be associated with delayed cerebral ischemia (DCI) or outcome and would then be a potential target for therapeutic interventions. We investigated whether respiratory deterioration within the fi rst 72 hours after admission predicted DCI or poor outcome.
Methods We conducted a retrospective study in 137 consecutively admitted patients with aSAH, admitted between October 2007 and October 2011 to the ICU of a university hospital. ERD was defi ned as increased need for ventilatory support the second or third day after admission (Table 1) . Elective intubation for a surgical procedure was not included as ERD. Inclusion criteria were availability of detailed information on respiratory status and level of support, admission within 48 hours after hemorrhage and age ≥18 years. Multivariable survival analysis was used to investigate associations of DCI, death and Glasgow Outcome Scale (GOS) with ERD adjusted for condition on admission, Hijdra score, treatment of ruptured aneurysm and pulmonary comorbidity. GOS was assessed at 3 to 6 months after the bleed. DCI was defi ned as described recently [2] . Results Mean age of the patients was 55.9 (± 12.7) and 63.5% was female. A total 46.7% of the patients developed DCI. Mortality was 25.6%. Forty percent of the patients were classifi ed as having ERD. ERD was not associated with DCI (adjusted HR = 1.48; 95% CI = 0.88 to 2.49; P = 0.14). ERD showed a trend towards an association with mortality (adjusted HR = 2.22; 95% CI = 0.96 to 5.14; P = 0.06; additionally adjusted for age, and rebleed). A clear association was found between absence of ERD and functional outcome with ordinal logistic regression analysis (0.98 point increase in GOS score at 3 to 6 months; 95% CI = 0.14 to 1.82; P = 0.02; additionally adjusted for age and rebleed). Conclusion ERD within 72 hours after admission is associated with increased risk of poor functional outcome after aSAH, but not DCI. Further investigations are required to assess whether prevention of ERD may improve outcome. Introduction Elevated intracranial pressure (ICP) may have deleterious eff ects on cerebral metabolism and mortality after aneurysmal subarachnoid hemorrhage (SAH) [1, 2] , but its relevance has not yet been well explored. Aims of this study are to track ICP changes after SAH, to identify clinical factors associated with it and to explore the relationship between ICP and outcome. Methods A total of 116 consecutive SAH patients with ICP monitoring were enrolled. Episodes of ICP >20 mmHg for at least 5 minutes and the mean ICP value for every 12-hour interval were analyzed. The highest mean ICP collected in every patient was identifi ed. ICP values were analyzed in relation to clinical and CT fi ndings; 6-month outcome and ICU mortality were also introduced in multivariable logistic models. Results Eighty-one percent of patients had at least one episode of elevated ICP and 36% had a highest mean ICP >20 mmHg. The number of patients with highest mean ICP >20 mmHg or with episodes of HICP was maximum at day 3 after SAH and decreased only after day 7. Neurological status, aneurysmal rebleeding, amount of blood on CT and CT ischemic lesion occurred within 72 hours from SAH were signifi cantly related to highest mean ICP >20 mmHg in a multivariable model. Patients with highest mean ICP >20 mmHg showed signifi cantly higher mortality in ICU. However, ICP is not an independent predictor of 6 months unfavorable outcome. Conclusion Elevated intracranial pressure is a common complication in the fi rst week after SAH. It is associated with early brain injury severity and ICU mortality. . We systematically reviewed their prevalence, aiming particularly at studies with low risk of bias. Methods We searched Embase, MEDLINE, The Cochrane Library, Trip Database, references of included studies and narrative reviews. We included cohort studies, cross-sectional studies and RCTs published in any language that tested the integrity of ≥1 pituitary axis in adults with aSAH. Studies including more than 50% of non-aneurismal SAH were excluded. Studies were considered at low risk of bias if the authors defi ned inclusion/exclusion criteria, avoided voluntary sampling, and tested >90% of included patients with proper detailed diagnostic criteria. Studies testing all pituitary axes were considered as evaluating hypopituitarism, which was defi ned as the dysfunction of ≥1 axis. We used a Freeman Tukey-type arcsine square-root transformation and pooled prevalences using the DerSimonian-Laird random-eff ect method. We determined the degree of heterogeneity with I 2 values. Results Among 12,363 citations, we included 28 studies (1,628 patients). Patients were mostly female (64%) aged 50.5 ± 5.4. Sixteen studies reported the severity of aSAH, 14 reported the procedure for securing the aneurysm and 13 reported the location of aneurysm.
Overall, hypopituitarism was observed in 53.4% of patients at shortterm (<3 months), 36.5% at mid-term (3 to 12 months) and 34.2% at long-term (>12 months) ( Table 1 ). There was an insuffi cient number of studies with low risk of bias to perform sensitivity analyses according to study quality. Conclusion The exact prevalence of pituitary disorders following aSAH remains uncertain, mainly due to high heterogeneity and the small number of studies with low risk of bias. However, the prevalence seems to decrease during the recovery phase. The prevalence, risk factors and clinical signifi cance of pituitary disorders in aSAH will require further rigorous evaluation. is associated with a high morbidity and mortality. Although UK Anaesthesia Guidelines advocate early coiling or clipping of the aneurysm within the fi rst 72 hours of admission for all grades of aSAH, the optimal timing of treatment and whether this is linked with better neurological long-term outcome are a subject of debate [1] . We aimed to investigate whether the timing of the occlusion of the aneurysm translates into better outcome.
Methods A retrospective analysis of prospective collected data in a tertiary neuroscience centre from January to September 2012. All patients were managed according to the local Guidelines for the management of aSAH. Outcome was assessed at 3 months using the extended Glasgow Outcome Scale (GOSE) defi ning good recovery as a GOSE ≥7 and poor outcome as GOSE ≤6. Results A total of 28 patients were included within the study period. Three patients were not expected to survive the fi rst 24 hours and were not included in the study. Seventeen patients were classifi ed as good grade aSAH (WFNS I to III) and eight as poor grade (IV to V). Twenty-two patients underwent successful coiling while the other three required clipping due to unsuccessful coiling. We did not fi nd any correlation between the timing of coiling/clipping and the 3-month GOSE (Figure 1) . A total 44% of the patients had a poor 3-month GOSE while 56% had a good long-term functional outcome. The overall mortality rate was 21%. Conclusion Overall mortality in patients with aSAH is low when aneurysm is treated early post rupture of aneurysm. We did not fi nd any correlation between the timing of occlusion of aneurysm and the 3-month functional outcome. Patients underwent neuropsychological evaluation at early (<3 days, 10 days) and delayed time points (1 month, 3 months). Patients were tested for language, verbal fl uency, short-term and long-term memory, attention, executive functions, praxis, and neglect. Impairments in activities of everyday life were assessed using the Activities of Daily Living scale and the Instrumental Activities of Daily Living scale. The SF-36 was used to assess the quality of life at 3 months. Since complications of aneurysm treatment in addition to aSAH severity may signifi cantly aff ect cognitive status, patients were evaluated according to the World Federation of Neurological Surgeons score after treatment (WFNSpt).
All WFNSpt 1 to 2 patients completed neuropsychological tests at each time point. WFNSpt 3 and WFNS 4 patients were testable in 80% and 50% of the cases respectively at early time points. WFNS 5 patients were not testable at any time point. In all testable patients, cognitive functions were severely impaired at early time points. At 3 months in WFNS 1 to 3 a good recovery of language defi cits while only a partial recovery of attention, memory and executive functions were observed; at the same time point 70% of WFNS 4 patients became testable, but they had a worse recovery of all cognitive functions. At 1 month after SAH less than 30% of patients return to work, at 3 months approximately 50%. Despite a good recovery of everyday life activities at 3 months, for all patients quality of life was lower than a normal population.
Conclusion Cognitive dysfunction has diff erent time courses after aSAH: signifi cant defi cits in diff erent cognitive domains, worse quality of life and diffi culties in return to work persist in more than 50% of patients at 3 months following SAH. Results Pretreatment with 20 mg/kg, but not 10 mg/kg, of ASA-DA protected against ischemic neuronal death and damage, and its neuroprotective eff ect was much more pronounced than that of ASA or DA alone. In addition, treatment with 20 mg/kg ASA-DA reduced the ischemia-induced activation of astrocytes and microglia. Conclusion Our fi ndings indicate that ASA-DA, a new synthetic drug, prevents against transient focal cerebral ischemia, which provides a resource for the development of its clinical application for stroke.
Introduction Acute neurological injury is a leading cause of morbidity and mortality in children. Global prevalence and regional disparities of etiology, interventions, and outcomes are unknown. The aim of this point-prevalence study was to measure the burden of pediatric neurological injury and to describe variations in interventions and outcomes in ICUs. Methods One hundred and three ICUs on six continents enrolled subjects on 4 specifi c days in a 1-year period. Included subjects were between ages 7 days and 17 years who were diagnosed with acute traumatic brain injury, stroke, cardiac arrest, central nervous system infection or infl ammation, status epilepticus, spinal cord lesion, hydrocephalus, or brain mass. Sites completed a secure web-based case report form that included subject and hospital demographics, details about the neurological disease, interventions, length of stay, and Pediatric Cerebral Performance Category (PCPC) score (good outcome = PCPC 1 to 3) and mortality at hospital discharge. Results Of 3,113 subjects screened, 1,009 (32%) met enrollment criteria. The mean number of subjects enrolled per site for each study day was 2.4. Most sites were dedicated pediatric ICUs with a mean number of 22 ICU beds (range 3 to 72). ICUs had resources to invasively monitor intracranial pressure (93%), continuous electroencephalography (14%), invasive and non-invasive brain tissue oxygenation (14% and 57%), and somatosensory evoked potentials (39%). There were on average 11 ICU faculty and six fellows per site, and nearly one-half reported a neurocritical care ICU team. Subjects were 58% male and 52% white, and 60% had normal pre-admission PCPC scores (85%). Status epilepticus and cardiac arrest (both 21%) had the highest prevalence. Sixty-one per cent of subjects were mechanically ventilated during ICU admission. ICU length of stay was a mean 29 days (median 43 days) and hospital LOS was a mean 43 days (median 61 days). Survival at hospital discharge was 87% with 58% of subjects discharged home and 17% to inpatient rehabilitation. Conclusion Acute neurological disease is a signifi cant pediatric health issue. These data suggest a vital need for increased research and healthcare resources to assist in the challenge of improving outcomes for these children. 
The newly approved oral anticoagulant dabigatran has no eff ective antidote. We therefore suspected an overall increase in mortality in patients presenting to the emergency department (ED) with a bleeding complication on dabigatran compared with warfarin or aspirin. Methods We conducted a post hoc analysis on a database of all patients admitted to a tertiary-care ED with any kind of bleeding or suspicion of one from March 2011 to August 2012 who were taking dabigatran, warfarin, or aspirin. The primary endpoint was long-term survival.
Patients were censored at death or at the end of the study period (7 December 2012). We performed a Cox proportional hazard model, controlled for age, to calculate the hazard ratio (HR) for dabigatran versus warfarin and one for warfarin versus aspirin. Statistical signifi cance was set at α = 0.05 and results are presented with 95% CI. Results In total, 943 patients met the inclusion criteria with a mean follow-up period of 1 year. The mean age was 74.3 years and 50.4% were men. A total of 108 deaths (11.5%) were recorded within the follow-up period; eight (25%) for dabigatran compared with 44 (12.6%) for warfarin and 56 (9.9%) for aspirin. The mortality risk for patients on dabigatran was signifi cantly higher than for patients on warfarin: HR = 2.1 (95% CI: 1.0 to 4.5), P = 0.05 after controlling for age. Aspirin had a lower (but not statistically signifi cant) mortality risk compared with warfarin; HR = 0.75 (95% CI: 0.50 to 1.14), P = 0.18 after controlling for age.
The results showed higher overall mortality in patients who presented to the ED with a bleeding complication and were taking dabigatran compared with warfarin or aspirin. Physicians should be aware of the potential higher mortality with dabigatran over warfarin when treating a bleeding patient. However, this was a singlecentre retrospective analysis with a small number of patients taking dabigatran (n = 32), and further studies are needed to corroborate the results. Introduction Dose adjustments of low molecular weight heparin (LMWH) based on daily anti-Xa measurement by chromogenic assay remain controversial in daily clinical practice. One of the major obstacles is the cost of such a test. An aff ordable and reliable bedside test could change practice to an individual tailored dosing of LMWH. The aim of our study was to evaluate whether a prophylactic dose regimen of 40 mg enoxaparine in cardiac surgical patients increases the anti-Xa activity to the level necessary for effi cient prevention of a thromboembolic event [1] . Secondarily we tested whether there was a reliable correlation between a bedside anti-Xa measurement compared with a two-stage chromogenic assay at the laboratory [2] .
This was an open, single-centre, prospective, nonrandomized clinical trial at a university hospital. All patients that needed prophylactic dosing of enoxaparine after cardiac surgery were duly informed and after giving written consent we included 44 patients with a mean Euroscore of 1.66. The demographic specifi cations, medical and surgical history of all patients were collected. Anti-Xa activity was measured at three diff erent points in time. We determined baseline, peak and trough anti-Xa activity: preoperatively, and respectively 4 hours after the third dose of enoxaparine and 30 minutes before the fourth dose. Each measurement was done with both techniques, the two-stage chromogenic assay at the laboratory (Biophen®) and the bedside assay (Hemochron® Jr). Results Our dose regimen of enoxaparine achieved in one-half of the included patients a suffi cient anti-Xa activity for prevention of thromboembolic events. One-half of the patients with insuffi cient anti-Xa activity had a body mass index over 30 kg/m 2 . Comparison of the bedside assay with the two-stage chromogenic assay by means of the Pearson's correlation coeffi cient showed correlation of the two tests if no variables were taken into account. In the Bland-Altman analysis we could not confi rm this correlation. Conclusion The bedside anti-Xa activity assay with a Hemochron device tends to show some correlation with the two-stage chromogenic assay, but insuffi cient to be used as an alternative, in this small but uniform patient population. Use of a standard dosing protocol for enoxaparine administration is prone for underdosage in post-cardiac surgery patients and may increase postoperative morbidity. References Introduction We hypothesized that higher doses of enoxaparin would improve thromboprophylaxis without increasing the risk of bleeding. Critically ill patients are predisposed to venous thromboembolism, leading to increased risk of adverse outcome [1] . Peak anti-factor Xa (anti-Xa) levels of 0.1 to 0.4 IU/ml, 4 hours post administration of enoxaparin, refl ect adequate thromboprophylaxis for medico-surgical patients.
Methods The sample population consisted of 78 patients, randomized to receive subcutaneous (s.c.) enoxaparin: 40 mg ×1 (control group), versus 30 mg ×2, 40 mg ×2 or 1 mg/kg ×1 (test groups) for a period of 3 days. Anti-Xa activity was measured at baseline, and at 4, 12, 16 and 24 hours post administration on each day. Patients did not diff er signifi cantly between groups.
Results On day 1 of administration, doses of 40 mg ×1 and 40 mg ×2 yielded similar mean peak anti-Xa of 0.20 IU/ml and 0.17 IU/ml respectively, while a dose of 30 mg ×2 resulted in subtherapeutic levels of anti-Xa (0.08 IU/ml). Patients receiving 1 mg/kg enoxaparin achieved near-steady-state levels from day 1 with mean peak anti-Xa levels of 0.34 IU/ml. Steady-state anti-Xa was achieved for all doses of enoxaparin at day 3. At steady state, mean peak anti-Xa levels of 0.13 IU/ ml and 0.15 IU/ml were achieved with doses of 40 mg ×1 and 30 mg ×2 respectively. This increased signifi cantly to 0.33 IU/ml and 0.40 IU/ml for doses of 40 mg ×2 and 1 mg/kg enoxaparin respectively (P = 0.0000) (Figure 1) . A dose of 1 mg/kg enoxaparin yielded therapeutic anti-Xa levels for over 80% of the study period. There were no adverse eff ects.
Introduction Unfractionated heparin is preferred over LMWH in ICU patients but LMWH is used more frequently in many European ICUs. Thromboprophylaxis with standard doses of nadroparin and enoxaparin has been shown to result in signifi cantly lower anti-Xa in ICU patients when compared with medical patients [1, 2] . Methods ICU patients (SAPS 44 ± 16, MV, n = 44; pressors n = 32) received 7,500 IU (Group 1, n = 25) or 5,000 IU dalteparin s.c. (Group 2, n = 29). Twenty-nine medical patients receiving 5,000 IU dalteparin served as controls (Group 3). Results Group 2 had signifi cantly lower areas under the Xa curve (AUC) compared with Groups 1 and 3 (Table 1) . Diff erences were not signifi cant between Groups 1 and 3. Peak anti-Xa activities (C max -anti-Xa)
were delayed (t max -anti-Xa) in Group 2 compared with Groups 1 and 3 (Table 1) .
Conclusion In ICU patients a s.c. dose of 5,000 IU dalteparin results in signifi cantly lower Xa activities when compared with normal ward patients. A s.c. dose of 7,500 IU dalteparin in ICU patients resulted in kinetics and peak anti-Xa activities comparable with medical patients receiving 5,000 IU dalteparin.
Introduction Anemia is very frequently encountered on the ICU. Increased hepcidin production is one of the cornerstones of the pathophysiology of anemia of infl ammation. The fi rst-in-class hepcidin antagonist NOX-H94, a PEGylated anti-hepcidin L-RNA oligonucleotide, is in development for targeted treatment of anemia of infl ammation. We investigated whether NOX-H94 prevents the infl ammation-induced serum iron decrease during experimental human endotoxemia. Methods A randomized, double-blind, placebo-controlled trial in 24 healthy young men. At T = 0 hours, 2 ng/kg E. coli endotoxin was administered intravenously (i.v.), followed by 1.2 mg/kg NOX-H94 or placebo i.v. at T = 0.5 hours. Blood was drawn serially after endotoxin administration for measurements of infl ammatory parameters, cytokines, NOX-H94 pharmacokinetics, total hepcidin-25, and iron parameters. The diff erence of serum iron change from baseline at T = 9 hours was defi ned as the primary endpoint. Results Endotoxin administration led to fl u-like symptoms. Infl ammatory parameters (CRP, body temperature, leucocytes, and plasma levels of TNFα, IL-6, IL-10, and IL-1RA) peaked markedly and similarly in both treatment groups. NOX-H94 was well tolerated. Plasma concentrations peaked at 0.7 ± 0.4 hours after the start of administration, after which they declined according to a two-compartment model, with a T 1/2 of 22.5 ± 4.28 hours. In the placebo group, serum iron increased from 19.0 ± 7.6 μg/l at baseline to a peak at T = 3 hours, returned close to baseline at T = 6 hours and decreased under the baseline concentration at T = 9 hours, reaching its lowest point at T = 12 hours. In the NOX-H94 group, serum iron concentrations rose until T = 6 hours and then slowly declined until T = 24 hours. From 6 to 12 hours post LPS, the serum iron concentrations in NOX-H94-treated subjects were signifi cantly higher than in placebo-treated subjects (P <0.0001, ANCOVA). Conclusion Experimental human endotoxemia induces a robust infl am matory response and a subsequent decrease in serum iron. Treatment with NOX-H94 had no eff ect on innate immunity, but eff ectively prevented the infl ammation-induced drop in serum iron concentrations. These fi ndings demonstrate the clinical potential of the anti-hepcidin drug NOX-H94 for further development to treat patients with anemia of infl ammation. 
The association between haemoglobin concentrations and mortality has been studied in patients with various comorbidities [1, 2] . This study aims to determine the association between haemoglobin levels on admission to intensive care and patient length of stay and mortality. Methods A retrospective collection of data from patient admissions to a single fi ve-bed ICU over a 15-year period identifi ed 5,826 patients between April 1994 and November 2012. Patients were split into groups according to haemoglobin concentration on admission. The data were analysed to determine whether there was any relationship between haemoglobin concentration at ICU admission and any of our outcome measures (unit and hospital mortality, unit and hospital length of stay). Results Patients with haemoglobin concentrations ≤10 g/dl and >10.1 g/dl were used in mortality comparisons. Patients with a haemo globin concentration ≤10 g/dl had an increase in ICU mortality compared with those with haemoglobin levels >10 g/dl (OR = 1.36, 95% CI = 1.30 to 1.71, P <0.0001). A similar diff erence was seen with hospital mortality (≤10 g/dl 37.7% vs. >10 g/dl 27.3%, P <0.0001). Unit length of stay was signifi cantly longer in patients with admission Hb ≤10 g/ dl (5.34 days) compared with an admission Hb >10 g/dl (4.08 days), P <0.0001. Hospital length of stay was also signifi cantly longer in patients with Hb ≤10 g/dl versus Hb >10 g/dl (21.6 days vs. 15.5 days, P <0.0001). There was seen to be an inverse correlation between haemoglobin concentration and patient age (r = -0.174; P <0.0001). Conclusion Haemoglobin concentrations ≤10 g/dl on admission to the ICU are associated with an increase in ICU mortality, hospital mortality, unit length of stay and hospital length of stay when compared with patients admitted with haemoglobin concentrations >10 g/dl. Introduction According to many authors, acute necrotizing pancreatitis (ANP) still remains one of the diffi cult problems of abdominal surgery. The complexity of the pathogenesis of the disease, features of the pancreas pathomorphology, abdominal hypertension, and high mortality (30 to 70%) necessitate a search for new ways to treat this disease.
The study was conducted in 44 patients with ANP, who were divided into two groups according to type of analgesia: epidural or opioids. Patients from the fi rst group (n = 23) had epidural analgesia by ropivacaine 6 to 14 mg/hour during 7 to 10 days, and from the second group (n = 21) opioid analgesia by trimeperidine 20 mg three times a day during the same period. We monitored the level of septic and thrombohemorrhagic complications by clinical and instrumental data, during the month after treatment starting. The hemostatic system was evaluated using indicators of hemoviscoelastography (Mednord-01M analyzer).
Results It was found that all patients with ANP initially have hypercoagulation and fi brinolysis inhibition. Levels of hemostatic disorders correlate with the level of septic complications, treatment in the ICU, and mortality. In the fi rst group we noted a deep vein thrombosis, two pneumonia, seven pseudopancreatic cysts and abscesses, two deaths and time of stay in the ICU as 15.4 days. In the second group: three cases of deep vein thrombosis, four pneumonia, 10 pseudopancreatic cysts and abscesses, two episodes of gastroduodenal bleeding, fi ve deaths and time of stay in the ICU as 27.8 days. Conclusion Using epidural anesthesia in patients with ANP reduced the number of septic complications on 36.6%, and reduced the mortality rate from 23.8% (second group) to 8.7% (fi rst group). We think that violations of blood coagulation and microcirculation are the basis for ischemia, necrosis in tissues and septic complications. Epidural analgesia is an eff ective method to decrease the level of septic and thrombohemorrhagic complications and mortality in ANP patients. Methods After ethics approval and informed consent, we studied the functional state of hemostasis in a group of 57 healthy volunteers, who were not receiving drugs aff ecting coagulation, and 43 patients with postphlebothrombotic syndrome (PPTS). In the PPTS patients we conducted baseline studies of coagulation state and daily monitoring of dynamic changes in the functional state of hemostasis, a comparative evaluation of performance low-frequency piezoelectric vibration hemoviscoelastography (LPVH) and platelet aggregation test (PAT), standard coagulation tests (SCT), and thromboelastogram (TEG).
We found that LPVH correlated with SCT, PAT and TEG. However, our proposed method is more voluminous: indexes ICC (the intensity of the contact phase of coagulation), t1 (the time for the contact phase of coagulation), and AO (initial rate of aggregation of blood) are consistent with PAT; indexes ICD (the intensity of coagulation drive), CTA (a constant thrombin activity) and CP (the clot intensity of the polymerization) are consistent with SCT and TEG. In addition, the advantage of this method is to determine the intensity of fi brinolysiswith the indicator IRIS (the intensity of the retraction and clot lysis). Conclusion LPVH allows one to make a total assessment of all parts of hemostasis: from initial viscosity and platelet aggregation to coagulation and lysis of clots, as well as their interaction. These fi gures are objective and informative, as evidenced by close correlation with the performance of traditional coagulation methods.
prophylaxis in orthopedics or in cases of acute coronary syndromes. The main drawback of FOND is that routine monitoring is not currently available. This could be a problem during the management of critical and surgical patients, especially in cases of old patients and renal failure. The aim of this study is to evaluate the ability of thromboelastography (TEG) to determine the level of anticoagulation due to FOND in a surgical population.
We prospectively analyzed all patients to whom elective major orthopedic surgery was consecutively performed in a 2-month period. All the patients received FOND 2.5 mg in the postoperative period according to ACCP 2012 Guidelines. Native and heparinase (hep) TEG (Haemoscope Corporation, Niles, IL, USA) tests activated with kaolin were performed using whole blood citrated samples at four times: T0, before FOND administration; T1, 2 hours after administration; T2, 17 hours after administration (half-life); T3, 24 hours after administration. The following native and hep TEG parameters were analyzed: reaction time (R), α angle, maximum amplitude (MA) and coagulation index (CI). These parameters were compared with levels of anti-Xa. Unvariate analysis and Spearman's test were applied to our data. Results Eighteen patients were analyzed. Ten patients met the inclusion criteria. The mean R value increased from T1 to T3. The mean R parameter was in the normal range at any phase of the study and there was no signifi cant diff erences between the R mean value at the diff erent phases. The lowest value of R was at T1, which coincides with plasmatic peak concentration of FOND. This value did not correlate with anti-Xa mean value at T1, which showed the highest value at that time.
There was signifi cant diff erence between the mean native and hep R value only at T2 (P <0.05), native and hep α angle at T3, MA and MA hep at T2 (P <0.01) and CI and CI hep at T3 (P <0.02). Only the parameter MA had signifi cant variation over time (P <0.02). Conclusion R represents the time necessary to thrombin formation and in the presence of FOND we hypothesized a prolonged R time. In our population, TEG performed with citrated kaolin-activated whole blood was not able to detect prophylatic doses of FOND in every phase. On the contrary, levels of anti-Xa were able to reveal the exact pharmacokinetics of the drug. Further studies including a large number of patients are necessary. Introduction Coagulopathy, particularly a trend toward hypercoagula bility and hypofi brinolysis, is common in critically ill patients and correlates with worse outcome. Available laboratory coagulation tests to assess fi brinolysis are expensive and time demanding. We investigated whether a modifi ed thromboelastography with the plasminogen activator urokinase (UKIF-TEG) [1] may be able to evaluate fi brinolysis in a population of critically ill patients. Methods UKIF-TEG was performed as follows: fi rst urokinase was added to citrate blood to give fi nal concentrations of 160 UI/ml, then thromboelastography (TEG) analysis was started after kaolin activation and recalcifi cation with calcium chloride. Basal TEG (no addition of urokinase) was also performed. Fibrinolysis was determined by the loss of clot strength after the maximal amplitude (MA), and recorded as Ly30 (percentage lysis at 30 minutes after MA) and as Ly60 (percentage lysis at 60 minutes after MA).
Results UKIF-TEG was performed on 17 healthy volunteers and 18 critically ill patients. Ly60 was predicted by Ly30 according to an exponential function, so we used Ly30 as an indicator of clot lysis. Basal TEG showed increased coagulability and a trend toward less fi brinolysis in critically ill patients compared with healthy volunteers (reaction time 8.7 ± 3.4 minutes vs. 12.2 ± 1.9 minutes, P <0.001; α-angle 59.7 ± 9.4 vs. 47.2 ± 11.8, P <0.01). This reduction of fi brinolysis was more evident at a urokinase concentration of 160 UI/ml (Figure 1 ). Conclusion UKIF-TEG could be a feasible point-of-care method to evaluate fi brinolysis in critically ill patients. Methods We performed a randomized, double-blind study in 37 patients who underwent cesarean section. Patients were divided into two groups: the fi rst group (n = 19) received preoperative (30 minutes before operation) tranexamic acid 10 mg/kg; the second group (n = 18) received preoperative placebo. The condition of hemostasis was monitored by haemoviscoelastography. Results All patients included in the study before surgery had moderate hypercoagulation and normal fi brinolysis: increasing the intensity of clot formation (ICF) to 11.4% compared with normal rates; the intensity of the retraction and clot lysis (IRCL) was 16.45 ± 1.40 in both groups. At the start of the operation in patients (Group 1), ICF decreased by 9.7% (P <0.05), and IRCL decreased by 27.6% (P <0.05) compared with preoperatively. In Group 2, there was ICF decrease by 8.8% (P <0.05), and IRCL increase by 11.4% (P <0.05) compared with preoperatively. At the end of the operation, the condition of hemostasis in both groups came almost to the same value -moderate hypocoagulation, depressed fi brinolysis. In both groups there were no thrombotic complications. Intraoperative blood loss in the fi rst group was 300 ± 40.5 and in the second was 500 ± 60.6. Conclusion Using of tranexamic acid before surgery signifi cantly reduces intraoperative blood loss by 60%, without thrombotic complications. Introduction Rotational thromboelastography (ROTEM) can detect dilutive and hypothermic eff ects on coagulation and evaluate corrective treatments. The aim of this in vitro study was to study whether fi brinogen concentrate alone or combined with factor XIII could reverse colloid-induced and crystalloid-induced coagulopathies in the presence and absence of hypothermia. Methods Citrated venous blood from 10 healthy volunteers was diluted by 33% using 130/0.42 hydroxyethyl starch or Ringer's acetate. ROTEM was used to evaluate the eff ect of addition of either fi brinogen concentrate corresponding to 4 g/70 kg, or this fi brinogen dose combined with factor XIII equivalent to 20 IU/kg. Blood was analyzed at 33 or 37°C with ROTEM ExTEM and FibTEM reagents. Results A signifi cant dilutive response was shown in both groups: hypocoagulation was greater in the starch group. Hypothermia lengthened the following: ExTEM clotting time (CT), clot formation time and α angle; FibTEM maximal clot formation (MCF). Irrespective of temperature, fi brinogen overcorrected Ringer's acetate's eff ects on all ROTEM parameters and partially reversed starch's eff ects on ExTEM CT and FibTEM MCF. FibTEM demonstrated that factor XIII provided an additional procoagulative eff ect in the Ringer's acetate group at both temperatures but not the starch group. The only ExTEM parameter to be improved by addition of factor XIII was MCF at 33°C. Conclusion ROTEM shows that fi brinogen concentrate can reverse dilutive coagulation defects induced by colloid and crystalloid at both 33 and 37°C. Some additional reversal was provided by factor XIII: higher doses of both fi brinogen and factor XIII may counteract starch's eff ects on clot structure. Introduction Natural colloid albumin induces a lesser degree of dilutional coagulopathy than synthetic colloids. Fibrinogen concentrate has emerged as a promising strategy to treat coagulopathy, and factor XIII (FXIII) works synergistically with fi brinogen to correct coagulopathy following haemodilution with crystalloids. Objectives were to examine the ability of fi brinogen and FXIII concentrates to reverse albumininduced dilutional coagulopathy.
High and low concentrations of both fi brinogen and FXIII were used to reverse coagulopathy induced by 1:1 dilution in vitro with 5% albumin of blood samples from healthy volunteers, monitored by rotational thromboelastometry (ROTEM).
Results Haemodilution with albumin signifi cantly attenuated EXTEM maximum clot fi rmness (MCF), α angle (AA), clotting time (CT) and clot formation time (CFT), and FIBTEM MCF (P <0.001). Following haemodilution, both doses of fi brinogen signifi cantly corrected all ROTEM parameters (P ≤0.02), except the lower dose did not correct AA. Compared with the lower dose, the higher dose of fi brinogen signifi cantly improved FIBTEM MCF and EXTEM MCF, AA and CFT (P <0.001). The lower dose of FXIII did not signifi cantly correct any of the ROTEM parameters, and the high dose only improved EXTEM CT (P = 0.004). All combinations of high/low concentrations of fi brinogen/ FXIII signifi cantly improved all ROTEM parameters examined (P ≤0.001). Fibrinogen concentration generally had a greater eff ect on each parameter than did FXIII concentration; the best correction of ROTEM parameters was achieved with high-dose fi brinogen concentrate and either low-dose or high-dose FXIII. Conclusion Fibrinogen concentrate successfully corrected initiation, propagation and clot fi rmness defi cits induced by haemodilution with albumin, and FXIII synergistically improved fi brin-based clot strength. Results IOCS was used in 70 severe PPHs and 254 severe PPH controls were managed without IOCS. Placenta accreta can be selected as the best indication for RBC restitution. In the 1,500 to 3,000 ml PPH, allogeneic transfusion was decreased in the IOCS group: 17.6 versus 56.3% (P = 0.006); PRBC: 0 (0 to 3) versus 3 (0 to 6) (P = 0.045). IOCS spared 87 blood bank PRBC (17,374 ml); that is, 24.2% of the total transfusion need. No amniotic fl uid embolism has been observed in the group with IOCS whereas one case appeared in the control group without IOCS. Conclusion Regarding the literature [1] [2] [3] [4] and our study, IOCS could be used safely in PPH during CS. A leukocyte fi lter for retransfusion has been recommended and Rhesus isoimmunization must be precluded and monitored by repeated fetal RBC testing.
bleeding with the use of a protamine infusion and an abolishment of heparin rebound [1] . The aim of this study was to see whether the use of postoperative protamine infusions in our cardiac ITU was associated with a reduction in heparin rebound and blood loss. Methods Data from 240 cardiac surgery patients were retrospectively analysed. Of these, 157 had routine management with a bolus of protamine to correct the activated clotting time and then expectant management of subsequent bleeding, and 47 had the same but also a protamine infusion of 10 to 80 mg/hour for between 3 and 8 hours postoperatively. Blood loss was measured at 1, 6, 12 and 24 hours. In all, excessive bleeding was investigated using thromboelastography (TEG). Rebound heparinisation was determined by a ratio of R-times (heparinase/plain) <0.8. The Mann-Whitney U test and the chi-squared test were used to assess statistical signifi cance. Results There was no signifi cant diff erence in blood loss between the two groups. Blood loss at 1 hour in the infusion and non-infusion group was 145 and 88 ml, respectively (P = 0.06); at 6 hours: 450 and 392 ml (P = 0.5); at 12 hours: 620 and 595 ml (P = 0.62); and at 24 hours: 971 and 872 ml (P = 0.12). There was also no signifi cant diff erence in those getting heparin rebound with 40% in the infusion group and 47% in the non-infusion group (P = 0.54).
Conclusion Unlike Teoh and colleagues [1] , we did not fi nd an advantage in using protamine infusions. That there were still cases of heparin rebound in the infusion group suggests that the infusion was not as eff ective as expected and/or the dose was inadequate. However, previous studies assessed heparin rebound using isolated clotting parameters [1, 2] . Here, we used TEG. As TEG measures the thrombodynamic properties of whole blood coagulation, perhaps it is a more reliable indicator of heparin activity? As a retrospective study, there are limitations; namely, the nonstandardised management of the patients and the potential bias in the anaesthetists' selection of patients for an infusion. This group may be inherently higher risk for bleeding. However, heparin rebound is common and protamine is a simple, relatively safe and low-cost intervention compared with transfusion and so further study is needed.
Introduction The purpose of this study was to evaluate whether a restrictive strategy of red blood cell (RBC) transfusion was superior to a liberal one for reducing mortality and severe clinical complications among patients undergoing major cancer surgery.
Methods The trial was designed as a phase III, randomized, controlled, parallel-group, superiority trial. The inclusion criteria were adult patients with cancer who were undergoing major abdominal surgery requiring postoperative care in an ICU. The patients were randomly allocated to treatment with either a liberal RBC transfusion strategy (transfusion when hemoglobin levels decreased below 9 g/dl) or a restrictive RBC transfusion strategy (transfusion when hemoglobin levels decreased below 7 g/dl). The primary outcome was a composite endpoint of death or severe complications. The patients were monitored for 30 days. Results A total of 1,521 patients were screened for eligibility and 248 met the inclusion criteria. After exclusions for medical reasons or a lack of consent, 198 patients were included in fi nal analysis, with 101 allocated to the restrictive group and 97 to the liberal group. The primary composite endpoint -all-cause mortality, cardiovascular complications, acute respiratory distress syndrome, acute kidney injury requiring renal replacement therapy, septic shock or reoperation at 30 days -occurred in 19.6% of the patients in the liberal strategy group and in 35.6% in the restrictive group (P = 0.012). The liberal strategy group had a signifi cantly lower 30-day mortality rate as compared with the restrictive group (8.2% (95% CI, 4.2 to 15.4%) vs. 22.8% (95% CI, 15.7 to 31.9%), respectively, P = 0.005). The occurrence of cardiovascular complications was lower in the liberal group than in the restrictive group (5.2% (95% CI, 2.2 to 11.5%) vs. 13.9% (95% CI, 8.4 to 21.9%), respectively, P = 0.038). The restrictive strategy group had a higher 60day mortality rate as compared with the liberal group (23.8% (95% CI, 16.5 to 32.9%) vs. 11.3% (95% CI, 6.5 to 9.2%), respectively, P = 0.022). Conclusion The liberal RBC transfusion strategy with a hemoglobin trigger of 9 g/dl was associated with fewer major postoperative complications in patients undergoing major cancer surgery compared with the restrictive strategy.
Introduction Red blood cell (RBC) transfusion is associated with morbidity and mortality in critically ill patients. Congenital cardiac surgeries are associated with high rates of bleeding and consequently with high rates of allogeneic transfusion. We aimed to evaluate the association of transfusion with worse outcomes in children undergoing cardiac surgery. Methods We performed a prospective cohort study of 205 patients undergoing cardiac surgery for congenital heart disease. We recorded baseline characteristics, RACHS-1 score, intraoperative data, transfusion requirement and severe postoperative complications as need for reoperation, acute kidney injury, arrhythmia, severe sepsis, septic shock, bleeding, stroke, and death during 30 days. We performed univariate analysis using baseline, intraoperative and postoperative variables. Selected variables (P <0.10) were included in a forward stepwise multiple logistic regression model to identify predictive factors of a combined endpoint including 30-day mortality and severe complications.
Results One hundred and thirty-six patients (66.3%) were exposed to RBC transfusion. In the intraoperative room, 63.4% of patients received at least one RBC unit, and in the ICU, 11.2% of children were transfused. From all patients, 66 (32.1%) presented the combined endpoint. Patients with complications had higher RACHS-1 score, were younger (69 months (0 to 137) vs. 73 months (37 to 138), P <0.001), had a lower weight (13 kg (3 to 23) vs. 20 kg (12 to 36), P <0.001), a longer time of surgery (475 minutes (410 to 540) vs. 353 (275 to 433), P <0.001), a longer duration of cardiopulmonary bypass (205 minutes (175 to 235) vs. 106 minutes (73 to 123), P = 0.003), a lower SVO 2 at the end of surgery (59% (IQR 39 to 80) vs. 78% (71 to 83), P <0.001), a higher arterial lactate at the end of surgery (6.9 mmol/l (4.3 to 9.2) vs. 2.7 mmol/l (73 to 123), P = 0.003), a lower intraoperative hematocrit (26.2 ± 5.6% vs. 29.5 ± 6% (P <0.001)) and a lower hematocrit at the end of surgery (33.4 ± 6.7% vs. 36.9 ± 6.9% (P <0.001)) as compared with patients without complications. Patients with complications were more exposed to RBC transfusion in the intraoperative room (75% vs. 57%, P = 0.011) and in the ICU (21% vs. 6.4%, P = 0.002). In an adjusted model of logistic regression, RBC transfusion is an independent risk factor of combined endpoint (OR 4.25 (95% CI, 1.359 to 13.328), P = 0.013). Conclusion Blood transfusion after pediatric cardiac surgery is a risk factor for worse outcome including 30-day mortality. Avoiding blood transfusion must be a goal of best postoperative care.
Introduction We do not have enough criteria to make a judgment of the need for a massive transfusion (MT) in severe blunt traumatic patients. As a scoring system to predict the need for a MT, we usually use the Assessment Blood Consumption score (ABCs) and/or the Trauma-Associated Severe Hemorrhage score (TASHs). However, for these scoring systems, the procedure is slightly complicated. The aim of this study was to establish a predictor of a MT using coagulation or fi brinolysis markers.
Methods A retrospective analysis of MT was conducted in patients with severe blunt traumatic injury, which was defi ned as Injury Severity Score (ISS) of 16 or more admitted to the ICU between 1 June 2009 and 31 December 2010. Blood samples were collected from patients immediately after arriving at our level I trauma center. We defi ned the patients who received more than 10 unit packed red blood cells (PRBCs) within the fi rst 24 hours as a MT group and who received less than 9 units PRBCs as a non-MT group. After the demographic data, number of units of PRBCs and the need for massive transfusions were recorded and analyzed in each groups, we compared data between two groups. Results There were 114 patients who met the inclusion criteria. Fifty patients received blood transfusions (43.9%; 50/114). There were 27 patients in the MT group (23.7%; 27/114) and 87 in the non-MT group. The MT group was signifi cantly higher in the ratio of females (P <0.001), ISS (P <0.01), PT-INR (P <0.001), APTT (P <0.05), ABCs (P <0.001) and TASHs (P <0.001) than in the on-MT group. On the other hand, the MT group was signifi cantly lower in Ps (P <0.05) and fi brinogen level (P <0.001) than the non-MT group. In the receiver operating characteristics (ROC) analysis, the area under the curve (AUC) to distinguish a MT was the highest for TASHs (0.831, P <0.001), followed by fi brinogen (0.758, P <0.001), and ABCs (0.732, P <0.001). Fibrinogen was only a predictor of a MT without a scoring system such as ABCs and TASHs, and the optimal cutoff value was 205 mg/dl. Conclusion We found that the level of fi brinogen was the most valuable predictor of a MT in the coagulation or fi brinolysis markers. It is certain that the level of fi brinogen at admission was not as useful as the TASHs about predicting a MT in this study. Whereas the scoring systems require the assessment of several factors, the measurement of fi brinogen is simple, easy and quick. We strongly suggest that the level of fi brinogen will be a useful predictor of a MT at in severe blunt traumatic patients.
Introduction Red blood cell (RBC) transfusions are frequent in critically ill children. Their benefi ts are clear in several situations. However, issues surrounding their safety have emerged in the past decades. It is important to identify the potential complications associated with RBC transfusions, in order to evaluate their risk-benefi t ratio better. Methods A single-center prospective observational study of all children admitted to the pediatric intensive care unit (PICU) over a 1-year period. The variables possibly related to RBC transfusions were identifi ed before the study was initiated, and their presence was assessed daily for each child. In transfused cases (TCs), a variable was considered as a possible outcome related to the transfusion only if it was observed after the fi rst transfusion. Results During the study period, 913 admissions were documented, 843 of which were included in the study. Among them, 144 (17%) were transfused. When comparing TCs with nontransfused cases (NTCs), the odds ratio (OR) of new or progressive multiple organ dysfunction syndrome (NPMODS) was 2.39 (95% CI = 1.58 to 3.62, P <0.001). This association remained statistically signifi cant in the multivariate analysis for children with admission PRISM score ≤5 (OR = 2.41, 95% CI = 1.08 to 5.37, P = 0.032). TCs were ventilated longer than NTCs (14.1 ± 32.6 days vs. 4.3 ± 9.6 days, P <0.001). This diff erence was still signifi cant after adjustment using a Cox model. Moreover, we observed an adjusted dose-eff ect relationship between RBC transfusions and length of mechanical ventilation. The PICU length of stay was signifi cantly increased for TCs (12.4 ± 26.2 days vs. 4.9 ± 10.2 days, P <0.001), even after multivariate adjustment (hazard ratio of PICU discharge for TCs: 0.61, 95% CI = 0.5 to 0.74, P <0.001). We also observed an adjusted dose-eff ect relationship between RBC transfusions and PICU length of stay. The paired analysis for comparison of pre-transfusion and posttransfusion values showed that the arterial partial pressure in oxygen was signifi cantly reduced after the fi rst transfusion (mean diff erence 42.8 mmHg, 95% CI = 27.2 to 58.3, P <0.001). The paired analysis also showed an increased proportion of renal replacement therapy, while the proportions of sepsis, severe sepsis and septic shock did not diff er. Conclusion RBC transfusions were associated with prolonged mechanical ventilation and prolonged PICU stay. The risk of NPMODS was increased in some transfused children. Moreover, our study questions the ability of stored RBCs to improve oxygenation in critically ill children. These results should help to improve transfusion practice in the PICU.
Introduction Microcirculatory alterations during sepsis impair tissue oxygenation, which may be further worsened by anemia. Blood transfusions proved not to restore O 2 delivery during sepsis [1] . The impact of storage lesions and/or leukocyte-derived mediators in red blood cell (RBC) units has not yet been clarifi ed [2] . We compared the eff ects of leukoreduced (LR) versus nonLR packed RBCs on microcirculation and tissue oxygenation during sepsis. Methods A prospective randomized study. Twenty patients with either sepsis, severe sepsis or septic shock requiring RBC transfusion randomly received nonLR (Group 1, n = 10) or LR (Group 2, n = 10) fresh RBCs (<10 days old). Before and 1 hour after transfusion, microvascular density and fl ow were assessed with sidestream dark-fi eld imaging sublingually. Thenar tissue O 2 saturation (StO 2 ) was measured using near-infrared spectroscopy and a vascular occlusion test was performed.
Results The De Backer score (P = 0.02), total vessel density (P = 0.08), perfused vessel density (P = 0.04), proportion of perfused vessels (P = 0.01), and microvascular fl ow index (P = 0.04, Figure 1 ) increased only in Group 2. The StO 2 upslope (Figure 2 ) during reperfusion increased in both groups (P <0.05). In Group 1 the baseline StO 2 and StO 2 downslope during ischemia increased, probably refl ecting a lower O 2 consumption. Conclusion Unlike nonLR RBCs, the transfusion of fresh LR RBCs seems to improve microvascular perfusion and might help to restore tissue oxygenation during sepsis.
Introduction Obstetric haemorrhage remains a leading cause of maternal mortality and severe morbidity. Cardiovascular and haemostatic physiology alters in pregnancy and massive transfusion protocols have been implemented for obstetric haemorrhage based on limited evidence. The objective of this study was to examine the pattern and rate of blood products used in massive transfusion for obstetric haemorrhage in a tertiary obstetric hospital. Methods Massive transfusion was defi ned as 5 or more units of red blood cells within 4 hours in accordance with the Australian Massive Transfusion Registry defi nition. Following ethics approval, all cases fi lling this criterion were identifi ed in the hospital's birthing and blood bank systems. Data were extracted from the medical histories and analysed using SPSS. P <0.05 was considered statistically signifi cant.
Results Twenty-eight women in three years (2009 to 2011) underwent a massive transfusion for obstetric haemorrhage, with nine receiving more than 10 units of RBCs in 24 hours. Eleven (39%) were admitted to the ICU and 11 underwent a hysterectomy, of which six were admitted to the ICU. The median estimated blood loss was 4,335 ml (IQR 3,000 to 5,000). Median blood product delivery was RBC 8 units (IQR 6 to 13); FFP 4 units (IQR 4 to 8); platelets 4 units (IQR 0 to 8) and cryoprecipitate 3 units (IQR 0 to 10). One-half of the women received the fi rst four units of RBCs in less than 34 minutes. Other blood products were started a median of 49 minutes, 44 minutes and 75 minutes after the RBC transfusion commenced, respectively. Eight women had a fi brinogen level <0.8 g/l on the initial coagulation test during the haemorrhage. The remaining 20 women had a median fi brinogen level of 3.7 g/l (IQR 3.15 to 4.9). There was no diff erence in the transfusion of RBCs (P = 0.20), FFPs (P = 0.96) and platelets (P = 0.48) in women who showed an initial low fi brinogen and those who did not, although there was a diff erence in the number of units of cryoprecipitate (P <0.05). The median lowest Hb during the haemorrhage was 66 g/l (IQR 51 to 80) and median discharge Hb was 103 g/l (IQR 96 to 113). No blood product reaction was noted and there was one death. Conclusion Massive transfusion for obstetric haemorrhage involved rapid blood product administration with no consistent pattern in the ratio of products administered. Introduction Blood transfusions are associated with longer ICU and hospital inpatient durations, and an increase in mortality [1] . This study was undertaken to investigate whether the practice of packed red cell Critical Care 2013, Volume 17 Suppl 2 http://ccforum.com/supplements/17/S2 S139 (PRC) transfusions in the ICU was in accordance with the best clinical evidence. A number of studies, most notably the TRICC study [2] , have shown that indications for ICU blood transfusions are a haemoglobin (Hb) level of <7 g/dl or evidence of acute haemorrhage [3] . These criteria were therefore employed. Methods This study prospectively examined episodes of PRC unit transfusions over a 2-month period in the ICU of a large level 1 trauma centre and a tertiary cardiac unit. The number of PRC units transfused in each episode was recorded by nurses, along with the proposed indication and concurrent Hb level. The data were analysed to assess the number of transfusions administered contrary to the guidelines, along with the average Hb level at which a PRC unit was transfused and the average number of units administered per episode. Results A total 175 units of PRC were transfused in the ICU, over 105 episodes during the 2-month period (excluding immediately postoperative transfusions). Ninety-four units (53.7%) administered in 64 transfusion episodes (61.0%) occurred contrary to the guidelines. In 89.3% of these cases the recorded reason for transfusion was an apparently low Hb level. The median (IQR (range)) Hb level at which patients were transfused: within guidelines was 6.9 g/dl (6.6 to 7.7 (4.8 to 13.9)); within guidelines, excluding cases of acute blood loss, was 6.6 g/dl (6.5 to 6.8 (5.5 to 6.9)); and outside the guidelines was 7.7 g/dl (7.4 to 8.2 (7.0 to 9.7)). One unit of PRC was transfused in 57 episodes (54.3%), 2 units of PRC were transfused in 36 episodes (34.3%), and 3 to 6 units were transfused in 12 episodes (11.4%), with two-thirds of the latter due to acute haemorrhage.
Our results indicate a liberal transfusion threshold currently exists in the ICU. Patients are frequently receiving excessive PRC transfusions for Hb levels above the recommended concentration.
In the 2-month study period, these were associated with a cost of approximately £12,220. We recommend increased staff awareness of the guidelines to reduce the number of unnecessary transfusions. This would decrease exposure of ICU patients to unnecessary risks of blood transfusion, reduce cost of treatment and help to preserve a valuable resource.
Introduction Transfusion-related acute lung injury (TRALI) has a high incidence in critically ill and surgical patients and contributes to adverse outcome, while specifi c therapy is absent. Recently it was demonstrated that complement activation plays a pivotal role in TRALI. We aimed to determine whether a C1 inhibitor is benefi cial in a two-hit mouse model of antibody-mediated TRALI.
Methods BALB/c mice were primed with lipopolysaccharide (LPS, from E. coli 0111:B4) that was administered intraperitoneally in a dose of 0.1 mg/kg, after which TRALI was induced by injecting MHC-I antibody against H2Kd (IgG2a,k) at a dose of 2 mg/kg. Mice infused with PBS or LPS served as controls. Concomitantly, mice infused with the MHC-I antibody were treated with C1 inhibitor (Cetor®; Sanquin, Amsterdam, the Netherlands) in a dose of 400 IU/kg intravenously. After infusion, mice were mechanically ventilated with a lung-protective pressurecontrolled mode for 2 hours and then sacrifi ced, after which a bronchoalveolar lavage (BAL) was done. Statistics were analyzed by one-way ANOVA, values expressed as mean and standard deviation. Results Injection of LPS and MHC-I antibodies resulted in TRALI, indicated by increased levels of protein in the BAL fl uid, wet/dry ratios and levels of KC, MIP-2 and IL-6. C1 inhibitor Cetor® signifi cantly reduced total protein in BAL fl uid from 792 (169) to 421 (230) μg/ml (P <0.001) and tended to reduce the wet/dry ratio from 5.3 ± 0.3 to 4.9 ± 0.5 (P = 0.09). Cetor® also reduced BALF levels of MIP-2 from 214 (54) to 127 (22) pg/ml (P <0.01). KC and IL-6 levels were not aff ected. Conclusion In a model of antibody-mediated TRALI, C1 inhibitor attenuated pulmonary infl ammation. C1 inhibition may be a potential benefi cial intervention in TRALI. Introduction Transfusion-related acute lung injury (TRALI) is a syndrome that presents as a sudden onset of respiratory distress 6 hours after transfusion of blood products. The diagnosis is based on clinical and radiographic fi ndings. Particularly at risk for TRALI are cardiac surgery patients. However, specifi c patient risk factors and data on outcome are largely unknown. The aim of this study was to investigate incidence, risk factors and outcome of TRALI in cardiac surgical patients on cardiopulmonary bypass. Methods All thoracic surgery patients from a university hospital in the Netherlands of 18 years and older admitted to the ICU from January 2009 until December 2011 were screened. Included patients were observed during surgery and the fi rst 24 hours on the ICU for the onset of possible TRALI. The Canadian Consensus Conference TRALI defi nition was used. Two independent physicians blinded to the predictor variables scored the chest radiographs for the onset of bilateral interstitial abnormalities on 2K monitors. When interpretation diff ered, chest radiographs were reviewed by a third physician to achieve consensus. The European System for Cardiac Operative Risk Evaluation (EURO score) and the American Association of Anesthesiology (ASA) were scored before surgery. By calculating the Acute Physiology and Chronic Health Evaluation (APACHE) II and IV scores the severity of illness was determined on arrival in the ICU.
In total, 1,787 cardiac surgical patients were included. A total of 19 (1.1%) patients developed TRALI within 24 hours following surgery. Patients developing TRALI were older compared with patients not developing TRALI, mean age respectively 71 and 65 years (P = 0.035). Furthermore, patients developing TRALI had higher APACHE II, APACHE IV, EURO and ASA score (P = 0.000, P = 0.000, P = 0.000 and P = 0.37 Introduction Volume resuscitation is essential to restore normovolemia during hemorrhagic shock, burns and sepsis. However, synthetic colloids cause dilutional coagulopathy. The aims were to determine whether the natural colloid albumin induces a lesser degree of coagulopathy compared with synthetic colloids, and the comparative eff ectiveness of fi brinogen concentrate to reverse coagulopathy following dilution with these solutions. Methods Rotational thromboelastometry-based tests were used to examine coagulation parameters in samples from 10 healthy volunteers, in undiluted blood and samples diluted 1:1 with saline, was seen for samples diluted with synthetic colloids (P <0.001) but not albumin (P = 0.10). Following addition of fi brinogen, FIBTEM MCF, EXTEM MCF and EXTEM AA were signifi cantly higher, and EXTEM CFT was signifi cantly shorter in samples diluted with albumin versus those treated with HES or dextran (P ≤0.001). Conclusion Hemodilution using albumin induced a lesser degree of coagulopathy compared with the synthetic colloids HES and dextran.
In addition, albumin-induced coagulopathy was more eff ectively reversed following addition of fi brinogen concentrate compared with coagulopathy induced by synthetic colloids. Comparative assessment of the diff erent fl uid modalities is hampered by a paucity of direct trials. We present a network meta-analysis for assessing the relative eff ectiveness of two fl uid treatments in sepsis when they have not been compared directly in a randomized trial but have each been compared with a common treatment.
Methods A systematic review of trials sepsis yielded 13 trials for assessment in network meta-analysis. The indirect comparison between albumin, HES and crystalloid was conducted using Bayesian methods for binomial likelihood, fi xed-eff ects network meta-analysis with a Monte Carlo Gibbs sampling method. Studies in septic patients with crystalloid as a reference treatment compared with any formulation of the colloid treatments albumin or HES were included, as were direct head-to-head trials between the two colloids. Results Odds ratios between the diff erent treatments were obtained ( Figure 1 ). Ranking the interventions [1] demonstrated that albumin ranked highest in lowering mortality at a 96.4% probability compared with 3.6% and 0.01% for crystalloid and HES, respectively. Conclusion Albumin as a fl uid therapy in sepsis is associated with the lowest mortality of the three modalities studied. (SAP), SV and CO were recorded directly before the administration of any colloid (T0) and every 5 minutes for the next 1 hour (T1 to T12). Kolmogorov-Smirnov was used to test normal distribution of data and ANOVA was used for the statistical analysis. P <0.05 was considered statistically signifi cant.
Results Demographic data and ASA classifi cation did not diff er statistically signifi cant among the six groups of the study. CO, SV, HR and SAP did not show any statistically signifi cant evolution compared with their baseline value during the study period. Moreover, there were no statistically signifi cant diff erences among the six study groups with regard to any of the recorded parameters. Conclusion According to our results, volume replacement with the six colloids tested in our study did not result in any hemodynamic response. Within comparison of these six colloids did not reveal any statistically signifi cant diff erence in any of the parameters recorded according to our protocol. 
The biochemical characteristics of infused fl uids may be important in regulating acid-base balance, by modifying plasmatic volume and strong ion diff erence. In vitro and animal studies [1, 2] have shown that volume and strong ion diff erence of infused fl uids (SIDin) as well as the arterial baseline bicarbonate concentration (HCO 3 -a) infl uence acid-base variations. Our aim was to verify these changes in critically ill patients after surgery. Methods An electronic-dedicated database was created to retrospectively collect volume, type of fl uids infused and plasmatic acidbase balance variations in postoperative ICU patients from admission to 9:00 am of the day after. SIDin was calculated as the average SID of all fl uids infused during the whole study period (crystalloids, colloids and blood products). Arterial base excess variation (ΔBEa) was computed as the diff erence between values at 9:00 am on the day after and those at entry. We report data from all patients admitted in 2006 and 2007 (650 patients).
Results Nine patients not receiving intravenous infusions were excluded. The remaining population was divided into three groups according to SIDin distribution (Group 1, 18 ± 12; Group 2, 47 ± 6; Group 3, 55 ± 0 mEq/l). We observed a progressive increment in ΔBEa between the groups (1.1 ± 2.0 vs. 2.8 ± 2.9 vs. 3.0 ± 2.8 mmol/l, P <0.001). We further subdivided each group by the median value of baseline HCO 3 -a (24.3 (22.3 to 26.1) mmol/l) and we analyzed the ΔBEa: we observed a greater increase in patients with lower baseline HCO 3 -a (Group 1, 1.8 ± 2.9 vs. 0.2 ± 2.6, mmol/l, P <0.001; Group 2, 4.0 ± 2.7 vs. 1.5 ± 2.6, mmol/l, P <0.001; Group 3, 4.4 ± 2.8 vs. 1.7 ± 2.0 mmol/l, P <0.001), as compared with those with higher baseline levels. When the study population was divided into quartiles of the diff erence between SIDin and HCO 3 -a, ΔBEa appeared to increase with the rise of such diff erence (P <0.001). Conclusion SIDin aff ects the acid-base status per se and in relationship with HCO 3 -a. We verifi ed this hypothesis in critically ill patients, highlighting the importance of the diff erence between SIDin and HCO 3 -a, which better describes and predicts the acid-base modifi cations to fl uid therapy.
Introduction Fluid resuscitation should improve tissue oxygenation in hypovolemia, besides restoring macrohemodynamic stability [1] . We evaluated the microvascular response to fl uid challenge with diff erent colloid solutions and its relation to macrohemodynamics. Methods An observational study of patients receiving a fl uid challenge (500 ml colloids in 30 minutes) according to the attending physician's decision. Before and after the infusion, sublingual microcirculation was evaluated with sidestream dark-fi eld imaging (Microscan; Microvision Medical, Amsterdam, the Netherlands). Microvascular fl ow and density were assessed for small vessels [2] . The cardiac index (CI), intrathoracic blood volume index (ITBVI) and extravascular lung water index (ELWI) were measured in seven patients with PiCCO2 (Pulsion Medical System, Munich, Germany). Results Ten patients (two sepsis, four trauma, three intracranial bleeding, one post surgery) received either saline-based hydroxyethyl starch (HES) 130/0.4 (Amidolite®; B.BraunSpA; n = 5) or balanced HES 130/0.42 (Tetraspan®; B.BraunSpA; n = 5). The CI (P = 0.02) and ITBVI (P = 0.07) tended to increase, the EVLWI did not change. Microvascular fl ow and density improved in the whole sample. No correlation was found between macro-circulatory and micro-circulatory parameters.
Balanced HES led to a greater increase in capillary density than NaCl HES (Figure 1 ). Conclusion Balanced HES may be more effi cacious than saline-based HES in recruiting the microcirculation, thereby improving tissue O 2 delivery.
Introduction Are safety guidelines being followed when administering procedural sedation in the emergency department? Between November 2004 and November 2008, the NPSA received 498 alerts of patients being given the wrong dose of midazolam for procedural sedation [1] . In the fi rst 5 years of midazolam use there were 86 deaths, most related to procedural sedation [2] . Methods We searched through the controlled drugs book in resuscitation over a 2-month period and found a list of patients who had received midazolam or fentanyl. From this, we could make a search for the relevant A and E notes for these patients. From these notes, we looked for (see shorthand in Table 1 ): verbal consent documentation (consent), past medical history recorded (pmhx), safe initial dose of midazolam (midaz), pre-procedure monitoring (pre), post-procedure monitoring (post), and monitoring for 1 hour before discharge (1hr). Following introduction of a reminder in the controlled drugs book/ sedation room and staff education, the case notes were analysed over another 2-month period (24 sets of notes) to assess practise against safety guidelines. Results See Table 1 (key for shorthand in Methods).
Conclusion The re-audit notices within the procedural sedation room and controlled drug book front cover served as a reminder of good practise. The visibility of this reminder (within the CD book) helped ensure better adherence to the audit standard. This reminder will now be kept within the CD book.
Introduction Daily sedation interruption and protocol implementation have been recommended to reduce excessive sedation; however, their use has been inconsistent. We hypothesized that the use of an age, kidney and liver function adjusted sedation protocol would be associated with reduced doses and improved outcomes compared with a standard protocol. Methods This was a prospective cohort study comparing 3 months of a standard protocol (control group) with 3 months of an adjusted protocol (intervention group). In the adjusted protocol, patients were divided into three categories: category 1 (age <60 years, and normal kidney and liver function), category 2 (age = 60 to 70 years, or moderate kidney or liver function impairment), and category 3 (age >70 years, or severe kidney or liver function impairment). The upper limits of analgesics and sedatives doses were determined by age, and kidney and liver function, being lowest in category 3, and lower in category 2 than category 1. All consecutive adults mechanically ventilated patients who required infusion of analgesics and/or sedatives for >24 hours were included in the study. We compared the main outcomes of both groups including average daily doses of analgesics and sedatives; average Sedation-Agitation Scale (SAS), pain and GCS scores; mechanical ventilation duration (MVD); sedation-related complications during ICU stay; ICU and hospital length of stay (LOS), and ICU and hospital mortality. Results Two hundred and four patients were included in the study (control group = 105; adjusted protocol group = 99). There was no diff erence in baseline characteristics between the two groups. The adjusted protocol group, compared with the control group, received signifi cantly lower average daily doses of fentanyl (2,162 ± 2,110 μg vs. 3,650 ± 3,253 μg, P = 0.0001), nonsignifi cant lower average daily doses of midazolam and dexmedetomidine, and a trend toward higher average daily doses of propofol. Pain score was higher in the adjusted protocol group (0.98 ± 0.72 vs. 0.16 ± 0.35, P <0.0001) with no diff erence in SAS or GCS scores. Sedation-related complications during ICU stay were not diff erent between the two groups; however, agitation (SAS = 5) was less frequent in the adjusted protocol group (3% vs. 30%, P <0.0001). ICU mortality was signifi cantly lower in the Introduction The aim of this research was to provide clinically relevant evidence for Y-site compatibility of drug infusion combinations used in the PICU. Pharmacists and clinicians regularly have to interpret limited published data, particularly when more than two drugs are Y-sited. The risk of potential incompatibility must be balanced against that of additional line insertion. Methods A full 28-factorial design (total 256 combinations) was used to investigate chemical and physical compatibility of fi ve drugs (clonidine, morphine, ketamine, midazolam and furosemide). The drugs were studied at their highest commonly infused concentrations and exposed to three variations in environmental conditions (diluent: sodium chloride 0.9% or glucose 10%; temperature 25 or 37°C; and normal room lighting or blue light phototherapy). Chemical stability was assessed using HPLC; >10% reduction in concentration indicated incompatibility. Physical incompatibility was confi rmed by precipitation, pH or colour change.
Results Environmental conditions had no eff ect on the drug mixtures. The precipitation observed in incompatible combinations was due to either a change in pH, or with ketamine the presence of benzethonium chloride. Of 31 possible drug combinations, 12 were incompatible. A further three combinations were incompatible at extreme pH, or were of concern and so should be avoided. The incompatible formulations all contained furosemide. All combinations of the sedative agents studied were chemically and physically compatible. Conclusion This work provides evidence for Y-site compatibility of morphine, midazolam, clonidine and ketamine in any combination, which will potentially reduce the need for extra intravenous lines. Furosemide is incompatible with any of these sedative drugs and must be infused via a separate line. These results will aid clinical decisionmaking and help satisfy the requirements of recent UK Department of Health legislation relating to the mixing of medicines. Reference
Introduction In light of the interest in the relationship between glycemia control in critically ill subjects and outcome, we set up a study to investigate whether benzodiazepine, commonly used in anesthesia and ICUs, interferes with glucose metabolism and to explore the mechanism. Methods A total of 40 sedated and paralyzed Sprague-Dawley rats (301 ± 55 g) were investigated in four consecutive studies. (1) To investigate the eff ects of diazepam on blood glucose, 15 rats were randomly assigned to intraperitoneal anesthesia with tiopenthal 80 mg/kg (DZP0), tiopenthal 40 mg/kg + diazepam 5 mg/kg (DPZ5) or tiopenthal 40 mg/kg + diazepam 15 mg/kg (DZP15). Blood levels of glucose (GEM premier 3000; IL) were measured at time intervals over 2 hours. (2) Ten animals randomized to DZP0 or DZP5 underwent an intravenous glucose tolerance test with glucose bolus (0.5 g/kg). Acute insulin response, the mean value of blood insulin (Insulin ELISA kit; Millipore) from 2 to 10 minutes after glucose bolus, was measured as index of insulin secretion. (3) A hyperinsulinemic euglycemic clamp obtained by a continuous intravenous infusion of insulin (130 mUI/ kg/minute) was run in 10 animals randomized to DZP0 or DZP5 and the glucose infusion rate (GIR, mg/kg/minute) was assessed [1] . (4) Introduction We report our experience in the use of isofl urane for prolonged sedation in severe ARDS patients. Prolonged sedation in the ICU may be diffi cult because of tolerance, drug dependence and withdrawal, drug interactions and side eff ects. Inhaled anesthetics have been proposed for sedation in ventilator-dependent ICU patients. AnaConDa is a device that allows a safety and easy administration of inhaled anesthetics in the ICU. Methods From January 2009 to June 2012, 15 patients were sedated with isofl urane by means of the AnaConDa device. We consider administration of isofl urane as a washout period from common sedative drugs in patients with (at least one of ): high sedative drug dosage (propofol ≥300 mg/hour or midazolam ≥8 mg/hour) to reach the target Richmond Agitation Sedation Score (RASS) or inadequate paralysis; two or more hypnotic drugs to reach the target RASS (propofol, midazolam, hydroxyzine, haloperidol, diazepam, quetiapine); and hypertriglyceridemia. During isofl urane administration previous hypnotic drugs were interrupted. We retrospectively collected data before, during and after administration of isofl urane: hemodynamic parameters, renal and hepatic function, level of sedation (RASS) and sedative drug dosage. All data are reported as mean ± standard deviation, otherwise as median (minimum to maximum). Results Mean age was 43 ± 12 years and SAPS II was 35.7 ± 11; 13 patients were treated with ECMO for severe ARDS and four had a history of drug abuse; median ICU length of stay was 41 (15 to 127) days and they were ventilated for 41 (12 to 114) days. Due to severe critical illness, target RASS was -4 for all patients, most of which were also paralysed. Isofl urane was administered in nine patients because of a high level of common sedative drugs, in fi ve patients due to the use of two or more hypnotic drugs and in one patient because of hypertrigliceridemia. Isofl urane administration lasted 5.6 ± 1.8 days.
During isofl urane administration no alteration in renal function or hemodynamic instability was recorded. After the isofl urane washout period we observed a reduction in sedative drug dosage in 10 patients while two patients were quickly weaned from mechanical ventilation and the target RASS raised to 0. In two patients isofl urane was precautionarily interrupted because of concomitant alteration of liver function and suspected seizures respectively. Conclusion Inhaled anesthetics could be successfully used in the ICU especially in case of an inadequate sedation plan; for example, in patients with a history of drug abuse or young severe ARDS patients that required deep sedation and paralysis for a long period. Introduction Pharmacological agents used to treat critically ill patients may alter mitochondrial function. The aim of the present study was to investigate whether fentanyl, a commonly used analgesic drug, interacts with hepatic mitochondrial function.
Methods The human hepatoma cell line HepG2 was exposed to fentanyl at 0.5, 2 or 10 ng/ml for 1 hour, or pretreated with naloxone (an opioid receptor antagonist) at 200 ng/ml or 5-hydroxydecanoate (5-HD; a specifi c inhibitor of mitochondrial ATP-sensitive K + (KATP) channels) at 50 μM for 30 minutes, followed by incubation with fentanyl at 2 ng/ml for an additional hour. The mitochondrial complex I-dependent, II-dependent and IV-dependent oxygen consumption rates of the permeabilized cells were measured using a high-resolution oxygraph (Oxygraph-2k; Oroboros Instruments, Innsbruck, Austria). The respiratory electron transfer capacity of intact cells was evaluated using FCCP (carbonyl cyanide p-trifl uoromethoxyphenylhydrazone) to obtain the maximum fl ux.
Results Incubation of HepG2 cells with fentanyl (1 hour, 2 ng/ml) induced a reduction in complex II-dependent and IV-dependent respiration ( Figure 1 ). Cells pretreated with 5-HD before the addition of fentanyl exhibited no signifi cant changes in complex activities in comparison with controls. Pretreatment with naloxone tended to abolish the fentanyl-induced mitochondrial dysfunction. Treatment with fentanyl led to a reduction in cellular ATP content (0.24 ± 0.06 in controls vs. 0.17 ± 0.14 μmol/mg cellular protein in stimulated cells; P = 0.02). We did not observe any diff erence in basal or FCCP-uncoupled respiration rates of cells treated with fentanyl at 2 ng/ml compared with controls (data not shown). Conclusion Fentanyl reduces cultured human hepatocyte mitochondrial respiration by a mechanism that is blocked by a KATP channel antagonist. In contrast, antagonism with naloxone does not seem to completely abolish the eff ect of fentanyl.
Introduction Endothelial dysfunction during endotoxemia is responsible for the functional breakdown of microvascular perfusion and microvessel permeability. The cholinergic anti-infl ammatory pathway (CAP) is a neurophysiological mechanism that regulates the infl ammatory response by inhibiting proinfl ammatory cytokine synthesis, thereby preventing tissue damage. Endotoxemia-induced microcirculatory dysfunction can be reduced by cholinergic CAP activation. Clonidine improves survival in experimental sepsis [1] by reducing the sympathetic tone, resulting in the parasympatheticmediated CAP activation. The aim of this study was to determine the eff ects of clonidine on microcirculatory alterations during endotoxemia. Methods Using fl uorescent intravital microscopy, we determined the venular wall shear rate, macromolecular effl ux and leukocyte adhesion in mesenteric postcapillary venules of male Wistar rats. Endotoxemia was induced over 120 minutes by intravenous infusion of lipopolysaccharide (LPS). Control groups received an equivalent volume of saline. Clonidine 10 μg/kg was applied as i.v. bolus in treatment groups. Animals received either (i) saline alone, (ii) clonidine 10 minutes prior to saline administration, (iii) clonidine 45 minutes prior to LPS administration, (iv) clonidine 10 minutes prior to LPS administration, (v) clonidine 30 minutes after LPS administration or (vi) LPS alone. Results All LPS groups (iii to vi) showed a signifi cantly reduced venular wall shear rate compared with the saline group after 120 minutes. There were no signifi cant diff erences between the numbers of adhering leukocytes in the clonidine/LPS groups (iii, iv, v) and the LPS group after 120 minutes. Macromolecular effl ux signifi cantly increased in all groups over the time period of 120 minutes. After 120 minutes there was no diff erence between the LPS group and the clonidine 10 minutes prior to LPS administration group (iv) whereas all other groups (i, ii, iii, v) showed a signifi cantly reduced macromolecular effl ux compared with the LPS group. Conclusion Clonidine has no positive eff ect on microhemodynamic alterations and leukocyte-endothelial interaction during endotoxemia.
The reduction of capillary leakage in clonidine-treated groups depends on the time interval relative to the initiation of endotoxemia. Endothelial permeability and leukocyte activation are regulated by diff erent pathways when stimulated by clonidine during endotoxemia. We conclude that clonidine might have an important time-dependent anti-infl ammatory and protective eff ect on endothelial activation during infl ammation. Introduction Delivering analgesics via conjunctival application could provide rapid and convenient pain relief in disaster medicine. There are sporadic reports from the USA concerning inhalation administration of aerosol with various drugs producing a wide variety of eff ects from anxiolysis, sedation, and loss of aggressiveness to immobilisation. We attempted to determine in an animal experiment whether conjunctival administration of S+ketamine could produce signifi cant eff ect without side eff ects. Methods After ethic committee approval, 10 rabbits were administered conjunctival S+ketamine 2.5 mg/kg. Measured parameters were SpO 2 , blood pressure (BP) and heart rate (HR) before administration and in 1-minute intervals and immobilisation time (loss of righting refl ex [1] . We can speculate that the reason for stability of cardiorespiratory parameters was due to the sympathoadrenergic eff ect of ketamine or due to the method of administration. There were no signs of conjunctival irritation in any animal (S+ketamine is a preservative-free solution). Conclusion Conjunctival S+ketamine 2.5 mg/kg in rabbits produced rapid onset without changes in cardiorespiratory parameters and without signs of irritation of the eye. The results of our project warrant further research to increase the variety of drugs and methods of their administration for anxiolysis, sedation and analgesia in disaster medicine.
Introduction Procedural sedation is used in the emergency department (ED) to facilitate short but painful interventions. Many patients are suitable for discharge after completion. Ideally, the agent used to achieve sedation should not have a prolonged eff ect, allowing safe discharge in the shortest time frame. We hypothesised that propofol, with its short onset and off set, may reduce length of stay (LOS) in comparison with traditional benzodiazepines. Methods Data from a prospective registry were analysed for the period 1 August 2011 to 31 January 2012. Patients who underwent procedural sedation and who were discharged from the ED were identifi ed. Individuals were grouped as having received propofol, midazolam or a combination of the two. All were discharged when fully alert and able to eat and drink. Demographic details and the type of procedure undertaken were extracted. ANOVA was performed to identify diff erences in the length of stay between groups, in addition to descriptive analysis. Results During the study period 75 patients underwent procedural sedation and were discharged from the ED. The median age was 40 years and 57% were male. The commonest procedure performed was shoulder reduction (52%). In the propofol group (n = 20) the mean LOS was 100 minutes compared with 165 minutes in those receiving midazolam (n = 40) and 141 minutes in those receiving a combination (n = 15), P = 0.004. There was no diff erence in adverse events between groups. See Figure 1 .
Conclusion Propofol is increasingly used in EDs for procedural sedation due to its short duration of action. This study suggests that a shorter duration of action and faster recovery may result in a reduced LOS in the ED. 
The use of propofol for sedation in intensive care has been associated with the propofol infusion syndrome (PRIS) characterised by cardiac dysfunction, metabolic acidosis, renal failure, rhabdomyolysis and hyperlipidaemia. We prospectively monitor biochemical markers that we believe demonstrate early signs of this dangerous, often fatal syndrome. When this Pre-PRIS state is identifi ed, propofol is withdrawn whilst the syndrome is still reversible.
Methods We prospectively audited our monitoring of these markers over a 4-month period in propofol-sedated patients: propofol infusion rate, creatine kinase (CK), triglycerides (TG), creatinine, lactate, pH and base defi cit. We defi ned the criteria for Pre-PRIS as requiring a CK ≥320 mmol/l that had doubled from its base level and a rise in TG ≥1.7 IU/l; both that followed a trend with propofol dose. Conclusion We propose that a paired rise in CK and TG that can be attributed to propofol alone represents a Pre-PRIS state that is at risk of developing into full PRIS. We noted this in 22% of our patients, all on modest doses of propofol. It is unclear what proportion of patients will develop the full syndrome as it is not ethically possible to continue propofol in this situation. We advocate daily monitoring of CK and TG to identify Pre-PRIS so that propofol can be reduced or substituted to avoid the morbidity and mortality of the full syndrome. Introduction Until recently there were no guidelines for the reporting of adverse events (AEs) during procedural sedation [1, 2] . A consensus document released in 2012 by the world SIVA International Sedation Task Force proposed a benchmark for defi ning AEs [3] . We analysed 1,008 cases of procedural sedation in the emergency department.
Methods The study is based on 1,008 patients who received procedural sedation with propofol in the emergency department between December 2006 and March 2012. Patients were selected and sedated to a strict protocol by ED consultant staff . We applied the AE tool by performing a search through patient records, discussion with consultants performing the sedation and consensus opinion. Results From 1,008 cases we identifi ed 11 sentinel (six of hypotension, fi ve cases of hypoxia), 34 moderate, 25 minor and three minimal risk adverse events.
The study shows a 1% adverse event rate. This supports use of propofol sedation by emergency physicians but within the limits of a strict governance framework. Our safety analysis using the World SIVA adverse events tool provides a reference point for further studies.
Introduction Physical restraints are used to facilitate essential care and prevent secondary injuries. However, physical restraint may be regarded as humiliating. It may lead to local injury and increase the risk of delirium and post-traumatic stress syndrome. Research on physical restraint is scarce. The aim of this study is to investigate the scope of physical restraint use. Methods Twenty-one ICUs ranging from local hospitals to academic centres were each visited twice and 327 patients were included. We recorded characteristics of restrained patients, motives and awareness of nurses and physicians.
Results Physical restraint was applied in 74 (23%) patients, ranging from 0 to 54% in diff erent hospitals. Frequent motives for restraint use were 'possible threat to airway' (36%) and 'pulling lines/probes' (31%). Restrained subjects more often had a positive CAM-ICU (34% vs. 16%, P <0.001), could less frequently verbally communicate (14% vs. 49%, P <0.001), and received more often antipsychotics (49% vs. 28%, P <0.001), or benzodiazepines (55% vs. 36%, P = 0.003). The use of physical restraint was registered in the patient's fi les in 48% of cases.
Of the 310 interviewed nurses, 258 (83%) were familiar with a physical restraint protocol and 89 (29%) used it in any situation. Thirty percent of the 60 interviewed physicians were aware of the physical restraint status of their patients. Conclusion Physical restraint is frequently used in Dutch ICUs, but the frequency diff ers strongly between diff erent ICUs. Attending physicians are often not aware of physical restraint use.
Introduction Physical restraint (PR) use in critically ill patients has been associated with delirium, unplanned extubation, prolonged ICU length of stay, and post-traumatic stress disorder. Our objectives were to defi ne prevalence of PR use, and to examine patient, treatment, or institutional factors associated with their use in Canadian ICUs. Measures aimed at delirium prevention (psychohygiene and early mobilization) were carried only in a small minority or were not documented. To implement protocolled delirium care in the region at study, a multifaceted tailored implementation program is needed.
Introduction The objective of this study is to investigate the eff ect of intraoperative administration of dexamethasone versus placebo on the incidence of delirium in the fi rst four postoperative days after cardiac surgery.
Methods Within the context of the large multicenter Dexamethasone for Cardiac Surgery (DECS) trial [1] for which patients were randomized to 1 mg/kg dexamethasone or placebo at induction of anesthesia, a monocenter substudy was conducted. The primary outcome of this study was the incidence of delirium in the fi rst four postoperative days. Secondary outcomes were duration of delirium, use of restrictive measures and sedative, antipsychotic and analgesic requirements. Delirium was assessed daily by trained research personnel, using the Richmond Agitation Sedation Scale and the Confusion Assessment Method. Medical, nursing and medication charts were evaluated for signs of delirium and use of prespecifi ed medication. Analysis was by intention to treat. Results Of 768 eligible patients, complete data on delirium could be collected in 738 patients. The incidence of delirium was 14.2% in the dexamethasone group and 14.9% in the placebo group (odds ratio = 0.95, 95% CI = 0.63 to 1.43). No signifi cant diff erence was found on the duration of delirium between the intervention (median = 2 days, interquartile range 1 to 3 days) and placebo (median = 2 days, interquartile range 1 to 2 days) group (P = 0.45). The use of restrictive measures and administration of sedatives, haloperidol, benzodiazepine and opiates were comparable between both groups. Conclusion Intraoperative injection of dexamethasone seems not to aff ect the incidence or duration of delirium in the fi rst 4 days after cardiac surgery, suggesting this regimen is safe to use in the operative setting with respect to psychiatric adverse events. Reference
Introduction The beliefs, knowledge and practices regarding ICU delirium among ICU professionals may vary. This may interfere with the implementation of the Dutch ICU Delirium guideline. We aimed to get insight into potential barriers and facilitators for delirium guideline implementation that may help to fi nd an eff ective implementation strategy.
Methods An online survey was sent to healthcare professionals from the six participating ICUs. Respondents included ICU physicians, nurses and delirium experts (psychiatrists, neurologists, geriatricians, nurse experts). The survey consisted of statements on beliefs, knowledge and practices towards ICU delirium. Agreement with statements by more than 75% of respondents were regarded as facilitating items and agreement lower than 50% as barriers for implementing protocolled care.
Of the 565 surveys distributed, 360 were completed (63.7%). The majority of respondents were ICU nurses (79%). Delirium was considered a major problem (83%) that requires adequate treatment (99%) and is underdiagnosed (81%). Respondents considered that routine screening of delirium can improve prognosis (95%). However, only a minority (20%) answered that delirium is preventable. Only 39% of the respondents had received any training about delirium in the previous 3 years and 77% of them found training useful. The mean delirium knowledge score was 6.6 out of 10 (SD = 1.54). When all groups were mutually compared, nurses scored lower than delirium experts (ANOVA, P = 0.013). The respondents (58%; n = 210) from three ICUs indicated that CAM-ICU assessment was department policy. However, 50% (n = 106) of these respondents felt unfamiliar with CAM-ICU and only 47% (n = 99) of them indicated that a positive CAM-ICU was used for treatment decisions. Haloperidol was the fi rst-choice pharmacological treatment. Only 21% of all respondents knew that a national ICU delirium guideline existed, but in-depth knowledge was generally low. Conclusion Our survey showed that healthcare professionals considered delirium an important but underdiagnosed form of organ failure. In contrast, screening tools for delirium are scarcely used, knowledge can be improved and protocolled treatment based on positive screening is often lacking. These results suggest that the focus of implementation of ICU delirium management should not be on motivational aspects, but on knowledge improvements, training in screening tools and implementation of treatment and prevention protocols. Introduction Delirium is an acute disturbance of consciousness and cognition. It is a common disorder in the ICU and associated with impaired long-term outcome [1, 2] . Despite its frequency and impact, delirium is poorly recognized by ICU physicians and nurses using delirium screening tools [3] . A completely new approach to detect delirium is to use monitoring of physiological alterations. Temperature variability, a measure for temperature regulation, could be an interesting parameter for monitoring of ICU delirium, but this has never been investigated before. The aim of this study was to investigate whether temperature variability is aff ected during ICU delirium.
Methods We included patients in whom days with delirium could be compared with days without delirium, based on the Confusion Assessment Method for the ICU and inspection of medical records. Patients with conditions aff ecting thermal regulation, including infectious diseases, and those receiving therapies aff ecting body temperature were excluded. Twenty-four ICU patients were included after screening 334 delirious ICU patients. Daily temperature variability was determined by computing the mean absolute second derivative of the temperature signal. Per patient, temperature variability during delirious days was compared with nondelirium days using a Wilcoxon signed-rank test. With a linear mixed model, diff erences between delirium and nondelirium days with regard to temperature variability were analysed adjusted for daily mean Richmond Agitation and Sedation Scale scores, daily maximum Sequential Organ Failure Assessment score, and within-patient correlation. Results Temperature variability was increased during delirium days compared with days without delirium (mean diff erence = -0.007, 95% CI = -0.004; -0.011, P <0.001). Adjusting for confounders did not alter our fi ndings (adjusted mean diff erence = -0.005, 95% CI = -0.008; -0.002, P <0.001). Conclusion Temperature variability is increased during delirium in ICU patients, which refl ects the encephalopathy that underlies delirium. Opportunities for delirium monitoring using temperature variability should be further explored. Particularly, in combination with electroencephalography it could provide the input for an objective tool to monitor delirium.
In ICU patients, little research has been performed on the relationship between delirium and long-term outcome, including health-related quality of life (HRQoL), cognitive functioning and mortality. In addition, results seem to be inconsistent. Furthermore, in studies that reported increased mortality in delirious patients, no proper adjustments were made for severity of illness during ICU admission. This study was conducted to investigate the association Introduction We aimed to clarify the diff erences between primary and secondary acute GI injury. Methods A total of 2,690 consecutive adult patients were retrospectively studied during their fi rst week in the ICU. Pathology in the GI system or laparotomy defi ned the primary GI insult. If GI symptoms developed without primary GI insult it was considered secondary GI injury. Absent bowel sounds (BS), vomiting/regurgitation, diarrhoea, bowel distension, GI bleeding, and high gastric residuals (GRV >1,000 ml/24 hours) were recorded daily. Results In total, 2,690 patients (60.4% male), median age 59 years (range 16 to 92), were studied. Eighty-four per cent of them were ventilated, 72% received vasopressor/inotrope. Median (IQR) APACHE II score was 14 (9 to 21) and SOFA on the fi rst day was 6 (3 to 10). A total 35.5% had primary GI pathology. During the fi rst week 34% of patients had absent BS, 38% vomiting/regurgitation, 9% diarrhoea, 7% bowel distension, 6% high GRV and 5% GI bleeding. All symptoms except diarrhoea occurred more often (<0.001) in patients with primary GI insult. Eighty-fi ve per cent of patients with primary GI insult versus 46% without developed at least one GI symptom. The incidence of GI symptoms was signifi cantly higher in nonsurvivors. ICU mortality was lower in patients with primary than secondary GI injury (43.6% vs. 61.2%, P = 0.046). Nonsurvivors without primary GI insult developed GI symptoms later (Figure 1 ). Conclusion Primary and secondary acute GI injury have diff erent incidence, dynamics and outcome. ventilation with relative risk of 9 to 27% and with mortality of 25 to 50% [1, 2] . One of the promoting factors of VAP is the increased pH of the gastric acid, which occurs when H2-receptor antagonists (H2RA) or proton pump inhibitors (PPI) are used for stress ulcer prophylaxis. The results of this pilot study suggest that there may be no diff erence in the incidence of VAP and GI bleeding if stress ulcer prophylaxis is performed by H2RA or PPI. As the latter is more expensive, its use as fi rst choice in critical care should be questioned. Conclusion Depending on resection size liver resection acutely increases portal venous pressure and induces neurohumoral activation resulting in compromised renal function and increased risk of developing AKI. Introduction Severe acute pancreatitis (SAP) requiring admission to an ICU is associated with high mortality (hospital mortality reached 42%) and long lengths of stay [1] . Survival among patients with predicted SAP at admission has been shown to correlate with the duration of organ failure (OF) [2] . The systemic determinant of severity in a new classifi cation of acute pancreatitis (AP) is also based on identifi cation of patients with transient or persistent OF [3] .
Methods The aim of the study was to retrospectively determine the predictors of early persistent OF in ICU patients with SAP. The analysis involved 152 patients. The median time interval between the onset of AP and admission was 24 (9; 48) hours. The patients were divided into two groups: the fi rst group (n = 46) had transient OF and the second group (n = 106) had persistent OF. The ability of the APACHE II score, total SOFA score and number of organ/system failure to discriminate transient from persistent OF was explored with receiver operating characteristic (ROC) curves.
Results Hospital mortality was signifi cantly higher in the second group as compared with the fi rst group (45% vs. 7%, P = 0.000); while infectious complications were 39% versus 11% (P = 0.001) and median lengths of ICU stay were 8 (5; 18) days for the second group and 6 (4; 7) days for the fi rst group (P = 0.001). Optimum cutoff levels (by ROC curve analysis) were APACHE II score ≥12 (sensitivity 0.790; 1 -specifi city 0.119), total SOFA score ≥4 (sensitivity 0.829; 1 -specifi city 0.190), and failure ≥2 organs/systems (sensitivity 0.819; 1 -specifi city 0.214). See Table 1 . Introduction The aim of this study was to evaluate the accuracy of thrombopoietin (TPO) plasma levels as a biomarker of clinical severity in patients with acute pancreatitis (AP). TPO is a humoral growth factor that stimulates megakaryocyte proliferation and diff erentiation [1] . Furthermore, it favors platelet aggregation and polymorphonuclear leukocyte activation [2] . Elevated plasmatic concentrations of TPO have been shown in patients with critical diseases, including ACS, burn injury and sepsis [2] . In particular, clinical severity is the major determinant of elevated TPO levels in patients with sepsis [3] . AP is a relatively common disease whose diagnosis and treatment are often diffi cult, especially in the clinical setting of the emergency department (ED Introduction Renal ischemia-reperfusion injury (IRI) is a common cause of acute kidney injury and occurs in various clinical conditions including shock and cardiovascular surgery. Renal IRI releases proinfl ammatory cytokines within the kidney. Atrial natriuretic peptide (ANP) has natriuretic, diuretic and anti-infl ammatory eff ects [1] and plays an important role of regulating blood pressure and volume homeostasis. The hypothesis was that renal IRI induces infl ammation not only in the kidney but also in remote organs such as the lung and heart and ANP attenuates renal injury and infl ammation in the kidney, lung and heart. Methods Male Sprague-Dawley rats were anesthetized with pentobarbital. Tracheostomy was performed and rats were ventilated at VT 10 ml/kg with 5 cmH 2 O PEEP. The right carotid artery was catheterized for blood sampling and continuous blood pressure measurements. The right femoral vein was catheterized for infusion of saline or ANP. Rats were divided into three groups; IRI group (n = 10), left renal pedicle was clamped for 30 minutes; IRI+ANP group (n = 10), left renal pedicle was clamped for 30 minutes, ANP (0.2 μg/kg/minute, for 3 hours 25 minutes) was started 5 minutes after clamp; and Sham group (n = 6), the shamoperated rats. Hemodynamics, arterial blood gas, and plasma lactate levels were measured at baseline and at 1 hour, 2 hours and 3 hours after declamp. The mRNA expression of IL-6 in the kidney, lung, and heart were measured. The kidney, lung and heart were immunostained to examine the localization of IL-6 and NF-κB and assigned an expression score. The wet/dry ratio of the lung was also measured. Results Renal IRI induced metabolic acidosis, pulmonary edema, mRNA expression of IL-6 in the kidney, lung and heart. Renal IRI increased immunohistochemical localization of IL-6 in the proximal convoluted tubule of the left kidney and NF-κB in the bronchial epithelial cells of the lung. ANP attenuated metabolic acidosis, pulmonary edema and expression of IL-6 mRNA in the kidney, heart, and lung. ANP decreased immunohistochemical localization of IL-6 in the left kidney and NF-κB in the lung. Conclusion These fi ndings suggested that infl ammation within the kidney after renal IRI was extended into the lung and heart. ANP attenuated metabolic acidosis and infl ammation in the kidney, lung and heart in a rat model of renal IRI. ANP may attenuate organ crosstalk between the kidney, lung and heart. Reference increase in urinary NGAL in patients receiving bicarbonate infusion was observed compared with control (P = 0.011). The incidence of postoperative RRT was similar but hospital mortality was increased in patients treated with bicarbonate compared with chloride (11/174 (6.3%) vs. 3/176 (1.7%), OR 3.89 (1.07 to 14.2), P = 0.031). See Figure 1 .
Conclusion On this basis of our fi ndings we do not recommend the use of perioperative infusions of sodium bicarbonate to reduce the incidence or severity of AKI in this patient group. Figure 1 ). An excellent predictive value was found for uNGAL/uhepcidin ratio (AUC 0.90, Figure 1 ). This ratio combines an AKI prediction marker (NGAL) and a marker of protection from AKI (hepcidin), potentiating their individual discriminatory values. Contrarily, at ICU admission, none of the plasma biomarkers was a good early AKI predictor with AUC-ROC ≥0.80.
Conclusion Several urinary markers of acute tubular damage predict AKI after cardiac surgery and the biologically plausible combination of NGAL and hepcidin provides excellent AKI prediction. Introduction Furosemide is one of the most employed diuretics in the ICU for its ability to induce negative water balance. However, one common side eff ect is metabolic alkalosis [1] . We aimed to describe the time course of urinary excretion and changes in plasmatic acid-base balance in response to the administration of furosemide.
Methods We connected the urinary catheter of 39 ICU patients to a quasi-continuous urine analyzer (Kidney INstant monitorinG®), allowing measurement of pH (pHU), sodium, chloride, potassium and ammonium concentrations (Na+U, Cl-U, K+U, NH4+U) every 10 minutes. The study period lasted 3 hours after a single intravenous bolus of furosemide (time 0). In 13 patients receiving two or more administrations over a longer period (46 (26 to 49) hours), according to clinical needs, we reviewed data on fl uid therapy, hemodynamics and acid-base balance from the beginning to the end of the observation. Results Ten minutes after furosemide administration, Na+U and Cl-U rose from 65 ± 6 to 140 ± 5 and from 109 ± 6 to 150 ± 5 mEq/l respectively, while K+U fell from 60 ± 5 to 39 ± 4 mEq/l (P <0.001 for all electrolytes vs. time 0) with a consequent increase in urinary anion gap (AGU = Na+U + Cl-U -K+U). Urinary output increased from 10 (5 to 19) to 53 (29 to 71) ml/10 minutes (P <0.05). After the fi rst hour Cl-U remained higher than Na+U, which progressively decreased, leading to a reduction in AGU and pHU over time. In parallel, a progressive increment in NH4+U was observed. In patients receiving more than one administration we observed an increase in arterial base excess (1.8 ± 0.8 vs. 5.0 ± 0.6 mmol/l, P <0.001) and plasmatic strong ion diff erence (SIDpl) (31 (30 to 33) vs. 35 (34 to 36) mEq/l, P = 0.01) during the study period. These changes were due to a decrease in plasmatic Clconcentration (109.0 ± 1.1 vs. 106.6 ± 0.9 mEq/l, P = 0.009). Plasmatic sodium and potassium concentrations did not change. In these patients, considering the total amount of administered fl uids and urine, a negative water and chloride balance was observed (-460 ± 403 ml and -48 ± 48 mEq, respectively). Conclusion Furosemide acts immediately after administration, causing a rise in urinary output, Na+U and Cl-U concentrations. Loop-diureticinduced metabolic alkalosis may be due to an increased urinary chloride loss and the associated increase in SIDpl. Reference Introduction Given the signifi cant morbidity and mortality associated with acute kidney injury (AKI), there is a need to fi nd factors to help aid decision-making regarding levels of therapeutic support. As a prognostic biomarker, the red cell distribution width (RDW) has attracted interest in the setting of critical care when added to existing scoring systems [1] . By examining RDW in a previously studied AKI cohort, we aimed to evaluate the utility of this routine blood test. Methods A cohort of 209 mixed critical care patients who received renal replacement therapy for AKI had their demographic and biochemical data retrieved from electronic databases. Outcomes were gathered for ICU and hospital mortality. Incomplete datasets were discarded, leading to 153 complete sets. RDW data were taken from the fi rst sample after admission to the ICU, as were all other biochemical values apart from pre-RRT creatinine and potassium. Overall cohort characteristics were gathered, and two groups were created: those with a RDW value within normal range (≤14.5%) and those with a greater than normal value (>14.5%). We then further subgrouped RDW to assess the correlation between rising levels and ICU mortality. Results A total 77.1% of our cohort had a RDW greater than the normal laboratory range at time of ICU admission. Key baseline characteristics (age, APACHE II score, length of stay, ICU mortality) did not diff er signifi cantly between patients with normal and abnormal RDW. When subgroup analysis was performed, no statistically signifi cant correlation between rising RDW and ICU mortality was found (Spearman correlation = 0.426, P = 0.233).
Conclusion In this cohort of critically ill patients with AKI, RDW was not found to be a predictor of mortality. Our results contradict those of recent studies [1, 2] . However, both groups of RDW patients in our study suff ered a higher ICU mortality than in other studies. To further explain these fi ndings, we intend to perform multivariate logistic regression analysis and assess the eff ect of social deprivation on RDW.
Introduction Intra-abdominal hypertension (IAH) is an independent predictor of renal impairment and mortality [1] . Organ dysfunction caused by the pressure eff ect of IAH is well understood, but how this is modifi ed in the presence of bowel obstruction is unclear. The aim of this study was to determine how diff erent IAH models cause renal dysfunction in a pig model. Methods Twenty-four pigs were divided into three groups; a control group (n = 5), a pneumoperitoneum (Pn) (n = 10), and an intestinal occlusion (Oc) model (n = 10). IAP was maintained for 3 hours at 20 mmHg during which time creatinine, urea, urine output, potassium, and glomerular fi ltration pressure (GFP) were measured. Statistical analysis was performed using repeated-measures ANOVA.
Results Over the fi rst 3 hours there was a statistically signifi cant diff erence between the control group and both IAH models for Conclusion As expected the IAH models resulted in signifi cantly worse renal function after 3 hours. This early renal dysfunction may be as a result of an early infl ammatory process that has been associated with the pathophysiology of acute kidney injury. Potassium was signifi cantly elevated in the Pn group as compared with the Oc group. Early changes in potassium levels with IAH may be a marker of early renal dysfunction and the usefulness of other renal biomarkers, such as NGAL, prompts further investigation. Reference
Introduction Oliguria is common in septic patients and is frequently therapeutically addressed with loop diuretics; that is, furosemide. Diuretic treatment in shock and hypovolemia is not rational, but can be tried in oliguric patients with normovolemia or hypervolemia and without hypotension. In such patients it still does not always increase dieresis and can also be harmful. The resistive index is a measure of pulsatile blood fl ow that refl ects the resistance to blood fl ow caused by the microvascular bed distal to the site of measurement. It can refl ect functional status of the tissue distal to the point of measurement. We investigated whether measuring the renal resistive index (RI) could be helpful in determining which patients will respond to furosemide treatment. Methods We included medical ICU patients with sepsis and oliguria (urine output <1 ml/kg/hour) who were prescribed i.v. furose mide. Patients with known chronic renal failure, hypovolemia (CVP <10 mmHg) or severe hypotension (MAP <80 mmHg) were excluded. Resistive index (1 − (end diastolic velocity / maximum systolic velocity)×100) was measured in at least three segmental arteries of both kidneys, the average of all measurements was reported as the result. Repeated assessments were viewed as independent if separated by more than 24 hours. Furosemide was given intravenously in the dose of 40 mg after RI measurement. Positive response to furosemide was defi ned as doubling of hourly dieresis or achieving urine output >1.5 ml/kg/hour after drug administration.
We included 47 patients with a total of 59 measurements. In 28 cases patients had positive response to furosemide. Median RI in responders was 0.67 (range 0.55 to 0.78) and in nonresponders 0.79 (range 0.58 to 0.81); P = 0.027. Construction of receiver operating characteristic curve showed 83% sensitivity and 81% specifi city for the cutoff RI 0.73. No other measured patient characteristic was found to be predictive of response to diuretic treatment. Conclusion Our results show that the RI could be used to guide diuretic treatment in nonhypovolemic, nonhypotensive septic patients. Further studies are needed to confi rm those preliminary results.
Introduction As a proof of concept, the potential added value of chitinase 3-like 1 (CHI3L1) as a more early and specifi c diagnostic parameter for acute kidney injury (AKI) was investigated in adult ICU patients that underwent elective cardiac surgery. . Conclusion SDMA appears to be an accurate and precise estimate of GFR and a more sensitive biomarker of renal dysfunction than SCr. We predict SDMA will perform better than SCr as a biomarker of AKI. This forms the basis of a future study. Introduction Growing evidence hints that bidirectional interaction between heart failure and kidney disease and renal insuffi ciency is a strong predictor of mortality as well as causally linked to the progression of heart failure. Neutrophil gelatinase-associated lipocalin (NGAL) is an early predictor of acute kidney injury (AKI). We evaluated the impact of NGAL on morbidity and mortality in patients with acute heart failure. Methods Seventy-six patients presenting with symptoms consistent with acute heart failure (median age 72 years, 56% male) were enrolled. Plasma NGAL levels were measured by an ELISA at admission and compared with the glomerular fi ltration rate (eGFR) and B-natriuretic peptide (BNP) levels. The primary outcome was AKI development defi ned by RIFLE criteria (fall in GFR >25% or creatinine rise ≥50% from baseline, or a fall in urine output <0.5 ml/kg/hour) and secondary outcomes were duration of hospital stay and in-hospital mortality. Conclusion NGAL is emerging as a promising biomarker of AKI in the setting of acute heart failure and elevated NGAL levels indicate a poor prognosis in this population regarding morbidity and mortality.
Introduction AKI is a common occurrence in sick hospitalized patients, in particular those admitted to intensive care. Published data suggest that 4 to 5% of all critically ill patients develop severe AKI and require initiation of renal replacement therapy (RRT) [1, 2] . Such patients have high mortality rates often exceeding 60% [2] . We aimed to review the outcomes of patients admitted to the ICU and required renal replacement therapy for AKI. We examined whether aetiology of AKI, comorbidity burden, hospital length of stay and treatment in ICU had any signifi cant association with survival in the study cohort. Methods During 2009, 56 patients were identifi ed to have received RRT with AKI who were admitted to the ICU at the Royal Wolverhampton Hospitals NHS Trust. Computerised and paper-based case records were examined for these patients to collect the data. AKIN classifi cation was used to classify the severity of AKI. Conclusion Individuals who develop dialysis-dependent AKI in the ICU setting in general terms either die or recover. Sepsis is the most common association with death. The need for mechanical ventilation and inotropic therapy are both associated with increased incidence of death.
Introduction This study was to evaluate the effi ciency of the early start of intermittent substitutive renal therapy in patients with polytrauma complicated by multiple organ failure syndrome. Methods Forty-two patients with polytrauma complicated by multiple organ failure syndrome were included in the study. The age of the patients was from 20 to 60 years (38.3 ± 1.6 years average). All patients were divided into two equal groups. In the control group (CG) the criteria for the start of the substitutive renal therapy were: hyperkalemia ≥6 mmol/l, plasma creatinine ≥280 μmol/l, diuresis ≤20 ml/hour. In the investigation group (IG) there were subtests to carry out the substitutive renal therapy, allowing one to start it in the earlier period of the multiple organ failure progression. These are increase of Na + >150 mmol/l, osmolarity >300 mOsm/l, elevation of the plasma toxicity according to the average molecule concentration ≥1.0, diuresis decrease ≤40 ml/hour. These were examined: lethality, quantity of the substitutive renal therapy procedures, mechanical lung ventilation duration (MLV), intensive care and hospital duration. The substitutive renal therapy was carried out by AK-200-Ultra apparatus (Gambro, Sweden). The statistical analysis was realized using Statistica 6.1 and the Mann-Whitney U test.
The average quantity of the substitutive renal therapy procedures in the CG was 13.4 ± 0.7, in the IG it was 8 ± 0.6 (P <0.05).
The recuperation of the renal excretory functions was on 19 ± 1 day in 12 patients of the CG, and on 11 ± 1.3 day in the IG, from the moment of substitutive renal therapy start (P <0.05). Lethality in the CG was 43% (nine patients), and in the IG it was 29% (six patients, P <0.05). The duration of the MLV in the CG and IG was 21 ± 1.2 days and 16 ± 1.2, respectively (P <0.05). In the IG the duration of the ICU was lower by 23%, hospitality duration was lower by 17% (P <0.05).
Conclusion The effi ciency of the substitutive renal therapy depends directly on the hydroelectrolytic and metabolic changes and toxicosis degrees in the polytrauma complicated by multiple organ failure syndrome. The early start of the dialysis methods treatment allows one to achieve the earlier recuperation of the renal functions and to decrease the lethality level by 14%.
Can treatment with the molecular adsorbent recirculation system be the solution for type- Introduction It has been suggested that fl uid balance is a biomarker in critically ill patients [1] . There is a paucity of randomized trials examining the eff ect of daily fl uid balance on outcomes in patients on continuous renal replacement therapy (CRRT). The RENAL trial did not fi nd mortality diff erence with higher CRRT dose [2] , but did not investigate the eff ect of daily fl uid balance on patient outcomes. A post hoc analysis suggested survival benefi t in patients with negative fl uid balance [3] . In this study, we hypothesize that daily fl uid balance is an independent predictor of mortality in critically ill patients.
We conducted a retrospective cohort study in eight ICUs of a tertiary academic center. We constructed a robust clustered linear regression model of daily fl uid balance and all-cause hospital mortality among 595 critically ill patients receiving CRRT. We adjusted the model for the Charlson comorbidity score, the daily SOFA scores in the fi rst week after initiation of CRRT as well the type of ICU. Results After adjusting for the type of ICU and the daily severity of illness, patients who died had on average 779 ml higher daily fl uid balance compared with patients who survived (P <0.001, 95% CI = 385 to 1,173 ml, Figure 1 ). Severity of illness predicted daily fl uid accumulation; each additional point of the SOFA score predicted an additional 57 ml of extra daily fl uid (P = 0.002).
balance and intradialytic hypotension with mortality and recovery of renal function. Methods We conducted a retrospective cohort study among patients aged ≥16 years who had RRT initiated and continued for ≥2 days in a level 2 or 3 ICU at two academic centres, and had fl uid balance data available. Patients with end-stage kidney disease, within 1 year of a renal transplant or who had RRT initiated to treat a toxic ingestion were excluded. We used multivariable logistic regression to determine the relationship between mean daily fl uid balance over the fi rst 7 days following RRT initiation and the outcomes of mortality and RRT dependence in survivors. Introduction Acute kidney injury (AKI) is a common complication of critical illness and sepsis [1] . Dosing of antibacterial agents in septic patients is complicated by altered pharmacokinetics due to both acute renal failure and critical illness [2] . Current dosing regimens for administration of gentamicin and vancomycin to septic patients with AKI on continuous venovenous hemofi ltration (CVVH) at a fi ltration rate of 45 ml/kg/hour are missing.
Methods Seventeen septic patients with AKI treated with vancomycin and seven patients with gentamicin on CVVH were included. In the vancomycin group, patients received the fi rst dose of 1.0 g intravenously followed by 1.0 g/12 hours if not adjusted. In the gentamicin group, patients received a loading dose of 240 mg followed by a maintenance dose every 24 hours. The vancomycin maintenance dose was optimized to achieve AUC 0-24 /MIC ≥400 (Cmin >10 mg/l), gentamicin target was Cmax/MIC of 8 to 10. Maintenance doses were adjusted according to drug level simulation using a pharmacokinetic programme.
The median vancomycin total clearance (Cltot) was 0.89 and 0.55 ml/minute/kg on the fi rst and second day of the study. CRRT clearance accounted for about 50 to 60% of vancomycin Cltot found in a population with normal renal function (0.97 ml/minute/kg). Vancomycin serum concentrations after the fi rst dose were below the required target of 10 mg/l as early as 6 hours in 10 patients. AUC 0-24 / MIC ≥400 ratio was achieved in 67% of patients on the fi rst day. The median gentamicin Cltot was 0.68 and 0.79 ml/minute/kg on the fi rst and second day of the study. CRRT clearance accounted for about 50% of gentamicin Cltot found in a population without renal impairment (0.73 ml/minute/kg). The target Cmax/MIC ratio was achieved in 78% of patients after the fi rst dose. Conclusion CVVH at a fi ltration rate of 45 ml/kg/hour leads to high removal of both antibiotics. Due to rapid change in patient's clinical status it was impossible to predict a fi xed dosage regimen. We recommend administration of unreduced loading dose and: blood sampling as early as 6 hours after fi rst vancomycin dose; blood sampling 30 to 60 minutes after gentamicin administration and before the next dose; and the maintenance dose should be based on druglevel monitoring.
CRRT. The aim was to evaluate the eff ects on electrolyte and acid-base status of a new RCA-CVVH protocol using an 18 mmol/l citrate solution combined with a phosphate-containing replacement fl uid, compared with a previously adopted RCA-CVVH protocol combining a 12 mmol/l citrate solution with a conventional replacement fl uid. Methods Until September 2011, RCA-CVVH was routinely performed in our centre with a 12 mmol/l citrate solution and a postdilution replacement fl uid with bicarbonate (HCO 3 -32, Ca 2+ 1.75, Mg 2+ 0.5, K + 2 mmol/l) (protocol A). In cases of metabolic acidosis, not related to inappropriate citrate metabolism and persisting after optimization of RCA-CVVH parameter setting, bicarbonate infusion was scheduled. Starting from September 2011, in order to optimize buff er balance and to reduce the need for phosphate supplementation, a new RCA-CVVH protocol has been designed using an 18 mmol/l citrate solution combined with a recently introduced phosphate-containing replacement fl uid with bicarbonate (HCO 3 - Conclusion Protocol B provided a buff er balance more positive than protocol A and allowed one to adequately control acid-base status without additional bicarbonate infusion and in the absence of alkalosis, despite the use of a standard bicarbonate concentration replacement solution. Furthermore, the combination of a phosphate-containing replacement fl uid appeared eff ective to prevent hypophosphatemia. Introduction The aim of this study was to establish the intraobserver and interobserver variation of ultrasonographic measurements of the rectus femoris muscle cross-section area (RF-CSA). Muscle wasting is frequent in the ICU, aff ecting more than one-half of the patients with severe sepsis [1] . Muscle mass reduces rapidly, and 15 to 20% is lost within the fi rst week [1] . To monitor muscle mass, ultrasound has the benefi ts of being both readily available in the ICU and non-invasive. Ultrasonographic measurement of RF-CSA has an almost perfect correlation with MRI (mean interclass correlation (ICC) = 0.999) [2] and RF-CSA is linearly related to maximum voluntary contraction strength in both healthy subjects and COPD patients (r = 0.78) [3] . Methods The study had two purposes: to determine the intraobserver variation for RF-CSA by one observer scanning 15 healthy adult volunteers three times each at 2-day intervals; and to determine the interobserver variation for RF-CSA by two observers each scanning 15 adult ICU patients on the same day. Patients were in a supine position, legs in passive extension. The transducer was placed perpendicular to the long axis of the right thigh over the RF, two-thirds of the distance from the anterior superior iliac spine to the superior patellar border [1] . RF-CSA was calculated by planimetry. At each scan, three measurements were made. For intraobserver variation, the 3×3 scans were analyzed using the interclass correlation coeffi cient. For interobserver variation, the three measurements from each observer were averaged and compared using Bland-Altman statistics. Results Intraobserver variation: 15 healthy adults, age 39.6 ± 2.4 years, weight 66.8 ± 2.3 kg, sex three male/12 female. ICC: 0.996 (95% CI: 0.990 to 0.998). Interobserver variation: 15 ICU patients, age: 77 ± 8.3 years, weight: 71.3 ± 9.1 kg, sex nine male/six female. Bland-Altman: bias: -0.07 cm 2 , 95% limits of agreement -0.188 to 0.048 cm 2 . Conclusion Ultrasonographic measurement of RF-CSA is easily learned and quickly performed. It has a very low intraobserver and interobserver variation and can be recommended as a reliable method for monitoring muscle wasting in the ICU.
In artifi cially fed critically ill patients, adipose tissue reveals an increased number of small adipocytes and accumulation of M2-type macrophages [1] . We hypothesized that nutrient-independent factors of critical illness explain these fi ndings, and also that M2macrophage accumulation during critical illness may not be limited to adipose tissue. Methods We performed a randomized investigation in a septic mouse model of critical illness and a study of ICU patient biopsies. In the critically ill mouse, we compared the eff ect of parenteral nutrition (n = 13) with fasting (n = 11) on body composition, adipocyte cell size, and macrophage accumulation in adipose tissue, liver and lung. Fed healthy control mice (n = 11) were studied for comparison. In vivo adipose tissue was harvested after 1 week of illness from human patients (n = 40) who participated in a RCT on early parenteral nutrition versus tolerating nutrient restriction [2] , adipose tissue morphology was characterized and compared with healthy controls (n = 13). Results Irrespective of nutritional intake, critically ill mice lost body weight, total fat and fat-free mass. Part of the fat loss was explained by reduced ectopic fat accumulation. Adipocyte cell number and the adipogenic markers peroxisome proliferator-activated receptor γ and CCAT/enhancer binding-protein β increased with illness, again irrespective of nutritional intake. Macrophage accumulation with predominant M2-phenotype was observed in adipose tissue, liver and lungs of critically ill mice, further accentuated by fasting in visceral tissues. Macrophage M2-markers correlated with chemoattractant factor expression in all studied tissues. In human subcutaneous adipose tissue biopsies of critically ill patients, increased adipogenic markers and M2 macrophage accumulation were present irrespective of nutritional intake. Conclusion Adipogenesis and accumulation of M2-macrophages are hallmarks of critical illness, irrespective of nutritional management in humans and mice. Critical illness evokes macrophage polarization to the M2-state not only in adipose tissue but also in liver and lungs, which is further accentuated by fasting.
Introduction Intravenous magnesium sulfate is commonly used in obstetric patients with pre-eclampsia. Following a case of acute symptomatic hypocalcemia we retrospectively examined a cohort of patients to investigate the frequency of hypocalcemia. Methods Obstetric patients were identifi ed from the ICU admissions database and divided into two groups -those treated with magnesium (for suspected pre-eclampsia) and those admitted for other obstetric indications (postpartum hemorrhage, infection, etc.). The baseline calcium values were compared, as well as the lowest and discharge values. Albumin and magnesium values were also compared. All comparisons used Student's t test.
Results Data were collected on 88 parturients admitted over 2 years including 40 (45%) who received magnesium and 48 (55%) who did not. Magnesium-treated women were younger (age: 31 ± 7 vs. 36 ± 5 years, P = 0.02). The baseline calcium concentrations were similar for the two groups (2.2 ± 0.2 vs. 2.2 ± 0.1 mmol/l, P = 0.85). Patients receiving magnesium had signifi cantly higher magnesium concentrations (2.1 ± 0.4 vs. 0.7 ± 0.2 mmol/l, P <0.001), and signifi cantly lower calcium concentrations during therapy (1.6 ± 0.3 vs. 1.9 ± 0.3 mmol/l, P <0.001). At discharge, the calcium levels were closer (magnesium treated 1.9 ± 0.2 vs. untreated 2.1 ± 0.1 mmol/l, P = 0.02). The albumin concentrations did not diff er between the two groups (magnesium treated 27 ± 13 vs. nontreated 33 ± 23 g/l, P = 0.134). Normal values: calcium 2.15 to 2.55 mmol/l, magnesium 0.7 to 0.95 mmol/l, albumin 35 to 50 g/l. Conclusion Magnesium therapy was associated with hypocalcemia. Potential causative mechanisms include a renal excretion interaction and magnesium-induced suppression of parathyroid hormone secretion. Physicians should be aware of the potential for symptomatic hypocalcemia during magnesium therapy. Introduction Disorders of sodium (Na + ) and water homeostasis are common in hospitalised patients. Hyponatremia in particular has been associated with worse hospital outcome and length of stay [1] . We aimed to defi ne the incidence of hyponatremia (serum Na + ≤134 mmol/l) in our intensive care population and to determine whether it was associated with ICU outcome or length of stay. Methods Demographics, APACHE II score, outcome data and admission sodium were retrieved from the Ward Watcher system in the Victoria Infi rmary ICU for 2,440 consecutive admissions from January 2005 to present. We divided patients into three groups depending on serum Na + (≤134 mmol/l, 135 to 144 mmol/l, ≥145 mmol/l) and compared APACHE II score, length of stay and ICU outcome between patients with a low versus a normal serum Na + . Data were analysed using the chi-squared test, Student's t test and the Mann-Whitney test where appropriate. Results Of the 2,440 patients studied, 1,993 had APACHE II data and serum Na + recorded and so were included for analysis. In total, 453 patients (22.7%) had a serum Na + ≤134 mmol/l and 1,388 patients (67.1%) had a serum Na + of 135 to 144 mmol/l. Patients with a low Na + had a higher mortality (OR = 1.48, 95% CI = 1.16 to 1.90, P <0.001), a higher APACHE II score (22 vs. 19, P <0.001) and higher mean age (60 years vs. 58 years, P <0.001) than patients with a normal serum Na + . Mean length of stay of patients with low serum Na + was also longer (5.1 days vs. 4.6 days) although this was not statistically signifi cant (P = 0.09). Conclusion In summary, hyponatremia is a useful index of severity of illness in our ICU population. Whether this is a direct adverse eff ect of low serum sodium levels, or if hyponatremia is simply a marker for 'sicker' patients, is not known. Reference Introduction The anion gap (AG) is used routinely in the assessment of metabolic acidosis, but can be misleading in patients with hypoalbuminemia and other disorders commonly encountered in intensive care. This approach to acid-base analysis relies on assessment of pH, pCO 2 , sodium, bicarbonate and chloride, and can lead to underestimation or overestimation of the true electrochemical status of a patient, as it does not include important ions such as lactate, calcium, magnesium, and albumin. The strong ion gap (SIG) is an alternative to the AG and is based upon Stewart's physical chemistry approach. However, the SIG is cumbersome to calculate. As such, a number of shortcut equations have been developed in an eff ort to approximate the SIG. We sought to compare three such equations, the Kellum corrected anion gap (KellAGc), the Moviat equation, and EZSIG, in an eff ort to evaluate precision and accuracy [1] [2] [3] . Methods We conducted a retrospective chart review of consecutive patients admitted to the ICU of George Washington University Medical Center from September 2010 to March 2011. Of the 1,516 patients screened, 200 met inclusion criteria, which included availability of all laboratory components to calculate the SIG, obtained within 1 hour of each other. Demographic data and serum values for pH, pCO 2 , albumin, lactate, sodium, potassium, chloride, bicarbonate, magnesium, phosphate, and calcium were collected. The AG, SIG, KellAGc, EZSIG, and Moviat equations were subsequently calculated and compared using Pearson correlation and Bland-Altman analysis. Results The mean SIG was 3.25 ± 3.5. Mean values for KellAGc, Moviat, and EZSIG were 4.5 ± 5.0, 1.77 ± 2.2, and 3.6 ± 3.7, respectively. Pearson correlation coeffi cients for KellAGc, Moviat, and EZSIG when compared with the SIG were r = 0.77, P = 0.0001; r = 0.88, P = 0.001; and r = 0.89, P = 0.001, respectively. In Bland-Altman analysis, the mean bias for the test equations versus the SIG were: KellAGc (1.25), Moviat (-1.48), and EZSIG (0.40). Conclusion While all three equations correlated highly with the SIG, the EZSIG and Moviat outperformed the KellAGc in Pearson and Bland-Altman analysis. The EZSIG had a smaller bias than the Moviat equation and a slightly better correlation (0.89 vs. 0.88). In the assessment of critically ill patients, EZSIG is a candidate scanning equation for the measurement of the SIG when all SIG components are not available. university-affi liated teaching hospital in Tunis. Patients admitted within the fi rst 24 hours post burn with greater than 10% total body surface area (TBSA) burned were enrolled in this study from 1 January 2009 to 30 June 2010. Exclusion criteria were pregnancy, history of adrenal insuffi ciency, or steroid therapy within 6 months prior to burns. A short corticotrophin test (250 μg) was performed, and cortisol levels were measured at baseline (CS T0) and 60 minutes post test. Adrenal insuffi ciency was defi ned by a response ≤9 μg/dl. Relative adrenal insuffi ciency was further defi ned by a baseline cortisol >20 μg/dl. Results Patients were assigned into two groups: G1 (RAI, n = 7) and G2 (absence AI, n = 11). Comparative study of the two groups shows the results presented in Table 1 . Conclusion RAI is common in severely burned patients during the acute phase, and is associated with shock. Further prospective controlled studies will be necessary to establish risk factors of RAI in severely burned patients and its impact on their prognosis.
Albumin-adjusted calcium concentration should not be used to identify hypocalcaemia in critical illness T Steele 1 , R Introduction Hypocalcaemia is common in critical illness and accurate assessment is crucial. Small studies have shown that albumin-adjusted calcium (adjCa) does not accurately predict the ionised calcium (iCa) concentration in critically ill patients, yet adjCa continues to be widely used [1] . We investigated the reliability of using adjCa to identify hypocalcaemia in a large, diverse population requiring intensive care. Methods In a retrospective study of patients admitted to the ICUs of a tertiary care hospital between January 2008 and 2012, iCa and pH were extracted from routine blood gas results and total calcium, albumin and phosphate from routine biochemistry results. adjCa was calculated using a formula derived from and validated on the local population [2] . Sensitivity, specifi city, positive and negative predictive values (PPV and NPV) and area under the curve (AUC) of adjCa for predicting hypocalcaemia (iCa <1.1 mmol/l) were calculated. Results In total, 18 patients were included. The mean age was 69 ± 2 years, mean weight 76 ± 2 kg, APACHE II score 23 ± 2 and most patients suff ered from pneumosepsis. On the fi rst day of intubation, total and free testosterone levels were extremely low in most patients and remained low during the fi rst week (Figure 1 ). 17β-Estradiol levels were elevated on day 1 and decreased during the fi rst week. LH and FSH levels were inappropriately low. All lipoprotein fractions and their apo-proteins were reduced as well as 17-OH-progesterone, DHEA and DHEAS. In contrast, androstenedione (adione) levels were elevated. This suggests preferential and stimulated synthesis of androstenedione ( Figure 2 ). The high 17β-estradiol levels indicate that androstenedione is shunted into the estrogen pathway, a process that requires high aromatase activity. The high estradiol/total testosterone ratio supports this conclusion. Conclusion Hyperestrogenic hypotestosteronemia is a frequent fi nding in the acute phase of severe sepsis in male patients with respiratory failure. It is suggested to be caused by decreased androgen production and shunting of androgen to estrogen synthesis as a result of increased aromatase activity. The clinical relevance of gonadal hormone substitution needs further study. Introduction Melatonin could have a meaningful role in critically ill patients, because of its immunomodulatory, antioxidant and sleep regulation properties; it is reduced in critical illness. The purpose of this study is to describe the endogenous blood melatonin values in ICU patients and their correlation with clinical parameters. Methods Seventy-three high-risk critically ill patients mechanically ventilated for >48 hours were enrolled. Blood samples for melatonin assay were collected between the 3rd and the 8th day of the ICU stay. Melatonin was determined by radioimmunoassay and ELISA. The peak and the area under the curve (AUC) calculated for each patient were correlated with the clinical parameters using the regression for quantiles test.
Results Endogenous melatonin was found lower in critically ill patients compared with healthy subjects (Figure 1) , although it showed a great individual variability and it generally maintained a night-time increase. In the univariate analysis the peak was found related to: blood creatinine (P = 0.034); patients in coma (P = 0.024); hospital mortality (P = 0.016). The AUC was found related to: SAPS II (P = 0.047); creatinine (P <0.001); AST (P <0.001); ALT (P <0.001); hospital mortality (P <0.022). Peak and AUC were found higher in nonsurvivor patients.
In accord with previous studies, the endogenous blood melatonin was found reduced in ICU patients. The higher melatonin peak in renal failure may be due to an increased distribution volume; greater AUC in patients with liver failure could be due to a less effi cient removal of the hormone from the systemic circulation. The fi nding of increased peak and AUC in nonsurvivor patients could be due to a hormonal response increased by the body stress reaction, potentially similar to cortisol [1], or to a higher production of a physiological antioxidant [2] with a decreased ability to use it.
Introduction Metformin intoxication inhibits mitochondrial complex I and oxygen consumption (VO 2 ). Succinate bypasses complex I by donating electrons to complex II. The aim of this study was to clarify whether succinate ameliorates mitochondrial VO 2 of metforminintoxicated human platelets.
Methods Platelet-rich-plasma was incubated for 72 hours with metformin at a fi nal concentration of 0 mg/l (control), 1.66 mg/l (therapeutic dose) or 166 mg/l (toxic dose). Platelet VO 2 was then measured with a Clark-type electrode, in the presence of glutamate plus malate (complex I electron donors) (fi nal concentration: 20 mmol/l for both) or succinate (complex II electron donor) (30 mmol/l), before and after adding cyanide (40 mmol/l). Mitochondrial (cyanide-sensitive) and extra-mitochondrial (cyanide-insensitive) VO 2 were corrected for platelet count.
The main results, from four preliminary experiments, are shown in Figure 1 . In the presence of glutamate plus malate, only platelets incubated with a high dose of metformin had a mitochondrial VO 2 signifi cantly lower than controls. In the presence of succinate, mitochondrial VO 2 of controls did not change signifi cantly whereas that of platelets incubated with metformin did. The eff ect of succinate tended to become larger as the dose of metformin was increased from 0 up to 166 mg/l (0.3 ± 0.2 vs. 0.6 ± 0.3 vs. 1.0 ± 0.3 nmol/minute*10 6 cells) (P = 0.068). Even so, mitochondrial VO 2 of platelets incubated with the highest dose of metformin did not return to the levels of controls. Extra-mitochondrial VO 2 was always the same. Introduction Metformin, widely used as an antidiabetic drug, activates the AMP activated protein kinase, a key regulator of the metabolism providing protection under fuel defi ciency. Chronic metformin therapy has been shown in long-term follow-up clinical studies to reduce cardiovascular mortality [1] . In animal experiments, acute metformin pretreatment has been shown to reduce ischemia-reperfusion injury on cardiomyocytes [2] . We want to evaluate whether outcomes are aff ected in coronary artery bypass grafting (CABG) surgery. Introduction Metformin, an oral hypoglycemic drug, belongs to the biguanide class and is now generally accepted as fi rst-line treatment in type 2 diabetes mellitus, especially in overweight patients [1] . In some predisposing conditions, the use of metformin may result in metforminassociated lactic acidosis (MALA), a rare adverse event associated with a high mortality rate [2] . The aim of this study is to assess risk factors and prognostic factors in patients with MALA. [4] . In our study, a higher plasma concentration of lactate represents the main negative prognostic factor, as pointed out by other studies [5] . The prothrombin activity, which is considered to be a decisive prognostic factor in the study of Peters and colleagues [6] , was not impaired in patients with poor outcome.
Introduction Stress hyperglycemia in the critically ill is a complex process in which insulin signaling is systematically hijacked to provide energy substrate for metabolic priorities such as cell healing or infection containment. Fluctuating levels of plasma glucose are associated with increased mortality in the ICU [1] . We develop a multiscale mathematical model that can characterize the severity of stress hyperglycemia based on a fundamental understanding of the signaling molecules involved. Methods Insulin resistance following insult has been shown to be driven primarily by the immune response via the cytokine IL-6 [2] . We created a multiscale mathematical model that links circulating glucose and insulin concentration dynamics from the extended minimal model [3] to a cellular insulin response model [4] that captures insulinmediated glucose uptake in an insulin-responsive cell. Results Inhibitory dynamics driven by IL-6 were incorporated into the cellular model to attenuate an insulin signaling intermediate (insulin receptor substrate 1) according to the proposed biological mechanisms. The percentage reduction in glucose uptake as a function of IL-6 concentration was fi t to data from patients who underwent elective abdominal surgery [2] , shown in Figure 1 . The overall multiscale model captures decreased insulin signaling as a result of increased IL-6 levels and the subsequent hyperglycemia that may ensue.
Introduction Hyperglycemia is frequently encountered in critically ill patients, and associated with adverse outcome. Improvement of glucose protocol adherence may be accomplished using electronic alerts. We confi gured a non-intrusive real-time electronic alert, called a GLYC sniff er, as part of our Intensive Care Information System (ICIS) that continuously evaluates the occurrence of persistent hyperglycemia and hypoglycemia. Conclusion A real-time electronic persistent glycemia sniff er resulted in a signifi cantly higher proportion of normoglycemia, without increasing the variability. Furthermore, hypoglycemic events occurred less frequently, and were resolved more timely. Smart alerting is able to improve quality of care, while diminishing the problem of alert fatigue.
Introduction A recent study showed that hyperglycaemia (blood glucose ≥7.8 mmol/l) in nondiabetic patients hospitalised in a medical ICU is associated with increased risk of diabetes [1] . We investigated a large mixed ICU population to confi rm these results. Methods This study retrospectively included patients with negative history of diabetes admitted to ICUs during the year 2007. We excluded patients receiving steroids, with newly diagnosed diabetes and those with end-stage disease. Patients were followed-up 5 years after index admission. Diagnosis of diabetes within 6 months from the index admission was presumed as revealing DM at inclusion, which excluded the patient. Patients who were taking glucocorticoids during the followup period were excluded. Diabetics were identifi ed from ICD-9 documentation. Propensity score for death (pDead) was computed from either SAP1 (MIMICII) or APACHE III (HIDENIC) to assess the risk of death. Hypoglycemia was defi ned as AVG ≤60 mg/dl. AVG was computed as the area under the glucose curve throughout ICU admission. Mortality was examined within bins (each bin is categorized by a 10 mg/dl increase in AVG) and was compared between adjacent categories using a chi-square test. The same method was repeated among diabetics, nondiabetics, patients with lower (pDead greater than median) and higher (pDead lower than 
The presence of decubiti on admission to the ICU is associated with longer hospitalizations even after adjusting for age, acuity, and organ supportive therapies. DU on admission to ICU provide a unique, unambiguous marker of increased resource utilization. Introduction The aim was to analyze the prognosis of AIDS patients with organ dysfunctions at ICU admission. Methods A prospective cohort study, including all patients with HIV/ AIDS diagnosis, who were admitted to a specialized ICU from November 2009 until May 2012. Patients with less than 24 hours of ICU stay were excluded. Demographics and nutritional status were collected. The organ dysfunctions were classifi ed according to the SOFA score, and categorized as absent (0 SOFA point), mild (1 to 2 points) and severe (3 to 4 points). We expressed numeric variables as median and interquartile interval (25% to 75%). We performed a multivariate analysis of possible variables associated with hospital mortality (P <0.2), and we explored the 7-day, 28-day and 60-day survival of patients with and without independent risk factors.
We included 139 patients with HIV/AIDS admitted to the ICU. Median age was 40 (31 to 48) years and 71% were male. Severe malnutrition was common (34%). The CD4 cell count was 84 (25 to 274) cells/mm 3 and viral load was 17,733 (67 to 174,214) copies/ml; 57% had at least one opportunistic infection; 55% had used antiretroviral therapy previous to ICU admission. Mechanical ventilation was used by 46% of patients and hospital mortality was 42%. Total SOFA score was 5 (2 to 9) points. Cardiovascular dysfunction was the most common on the fi rst day of stay (51%), followed by respiratory (42%), neurological (40%), renal (35%), hematological (27%) and hepatic (17%). Cardiovascular and renal dysfunctions presented with higher rate of severe dysfunction (30% and 15%, respectively). Rates of neurological (P = 0.002), renal (P = 0.009) and hematological (P = 0.003) dysfunctions were higher in nonsurvivors. Age, CD4 cell count, malnutrition, and opportunistic infections were included in the multivariate analysis. Neurological dysfunction was the independent risk factor for hospital mortality (odds 3.2 (1.4 to 7.2)). The presence of neurological dysfunction was dichotomized: associated or not with primary neurological diagnosis; survival was lower in the patients with neurological dysfunction and without primary neurological diagnosis (log-rank test 0.001 in the 7-day and 0.02 in the 28-day analysis). Sixty-day survival was similar in primary and secondary neurological dysfunction, but it remained lower than in patients without neurological impairment. Conclusion Neurological dysfunction was independently associated with hospital survival, mainly in those AIDS critically patients without primary neuropathy. Results A total of 273 charts were reviewed. In total, 242 were categorized into A (n = 126), B (n = 79) or C (n = 37). D (n = 31) consisted mainly of patients with hematological malignancies (n = 12) and patients with chemotherapy or immunosuppressive treatment (n = 14). The groups diff ered in length of stay with A< B< C. During the fi rst 3 days the SOFA score was higher in A compared with C and in B compared with C. The duration of antibiotic therapy was longer in both B and C compared with A. There were no diff erences in 28-day mortality (A: 34/126 = 27%, B: 30/79 = 38%, C: 15/37 = 40%); however, the proportions of patients dying between days 8 and 28 were higher in B (14/63 = 22%) and C (7/29 = 24%) compared with A (5/95 = 5%). Conclusion In this retrospective material it was possible to categorize 88.6% of all patients as having primary, secondary or tertiary sepsis. The categories diff ered in clinical picture at presentation as well as in outcome. A prospective study is warranted to validate the results of this study. Conclusion Older people represent a growing proportion of the population although their representation in the critical care population remained constant in this 10-year study. These patients had a slightly higher median APACHE II score and 14.5% greater critical care mortality than the younger patients. The majority of survivors were able to go home; however, 31% died within 22 months with signifi cant life expectancy curtailment, surviving on average only 7.4 months after discharge; this has not changed in the last 10 years. Those who survived this initial period (69%) had a much better outlook. This information may be vital to patients and physicians when discussing admission to critical care. Reference
Methods Potential risk factors for psychological problems were prospectively collected at ICU discharge. Two months after ICU discharge 252 ICU survivors received the questionnaires Post-Traumatic Stress Symptom scale-10 (PTSS-10) and Hospital Anxiety and Depression Scale (HADS) to estimate the degree of post-traumatic stress, anxiety and depression.
Of the 150 responders, 31% had adverse psychological outcome, defi ned as PTSS-10 >35 and/or HADS subscales ≥8. After analysis, six predictors with weighted risk scores were included in the screening instrument: major pre-existing disease, being a parent to children younger than 18 years of age, previous psychological problems, in-ICU agitation, being unemployed or sick-listed at ICU admission and appearing depressed in the ICU. Each predictor corresponded to a given risk score. The total risk score, the sum of individual risk scores, was related to the probability for adverse psychological outcome in the individual patient. The predictive accuracy of the screening instrument, as assessed with area under the receiver operating curve, was 0.77. When categorizing patients in three risk probability groups -low (0 to 29%), moderate (30 to 59%) and high (60 to 100%) risk -the actual prevalence of adverse psychological outcome in respective groups was 12%, 50% and 63%.
Conclusion The preliminary screening instrument may aid ICU clinicians in identifying patients at risk for adverse psychological outcome after critical illness. Prior to wider clinical use, external validation is needed. 
The multiorgan dysfunction syndrome (MODS) is a dynamic process involving simultaneously or consecutively two or more organ systems [1] . The organ dysfunction's degree can be assessed by three severity scores (SOFA [2] , MODS [3] , LODS [4] ), but they have some limitations: they do not allow the evaluation of the clinical course of a patient, they are not reliable in populations diff erent from the reference one, and they do not support clinicians' decisions. Because MODS implies a systemic infl ammatory reaction leading to microcirculatory dysfunction, our hypothesis was that organ failures follow a predictable sequence of appearance. Our aims were to verify the presence of more likely organ failure sequences and to assess an online method to predict the evolution of MODS in a patient. The high mortality and morbidity rate of MODS in ICUs can in fact be reduced only by a prompt and well-timed treatment [5] .
Methods We selected 73 patients consecutively admitted to the ICU of Sant'Andrea Hospital from January to June 2012. The inclusion criteria were at least two organ systems with SOFA ≥2, ICU length of stay >48 hours. For each patient we calculated the SOFA since the beginning of the inclusion criteria and daily for 8 days. For the statistical analysis we used Dynamic Bayesian Networks (DBNs) [6] . DBNs were applied to model SOFA changes in order to identify the most probable sequences of organs failures in a patient who experienced a fi rst known failure.
We created a DBN for the analysis of MODS studying the relations between organ failures at diff erent times. The DBN was made so that each organ failure is dependent on the previous one. We also considered a corrective factor to take account that not all patients completed the observation. Using software (GeNie) we obtained the probabilities of the organ failure sequences. Conclusion The use of DBNs, although with our limited set of data, allowed us to identify the most likely organ dysfunction sequences associated with a fi rst known one. Capability to predict these sequences in a patient makes DBNs a promising prognostic tool for physicians in order to treat patients in a timely manner, or to test a treatment effi cacy.
Introduction Assessing whether a critically ill patient should be admitted to an ICU remains diffi cult and mortality amongst ICU patients is high. To render intensive care with no prospect of success is an immense emotional burden for both patient and relatives, and a great socioeconomic burden for society as well. Therefore, validated strategies that can help identify patients who will benefi t from intensive care are in demand. This study seeks to investigate whether preadmission quality of life can act as a predictor of mortality amongst patients admitted to the ICU. Methods All patients (>18 years) admitted to the ICU for more than 24 hours are included. In order to assess preadmission quality of life, the patient or close relatives complete the Short-Form 36 (SF-36) within 72 hours after ICU admission. Mortality is evaluated from ICU admission until 30 days hereafter. Logistic regression and receiver operating characteristic analyses are employed to assess predictive value for mortality using fi ve models: Introduction Long-term compromise after traumatic injury is signifi cant; however, few modifi able factors that infl uence outcome have been identifi ed. The aim of this study was to identify acute and early post-acute predictors of long-term recovery amenable to change through intervention. Methods Adults (>17 years) admitted to the ICU, Princess Alexandra Hospital, Australia following injury were prospectively followed. Data were collected on demographics, pre-injury health, injury characteristics and acute care factors. Psychosocial measures (selfeffi cacy (SE), illness perception (IP), post-traumatic stress disorder (PTSD) symptoms and psychological distress) and health status (SF-36) were collected via questionnaire 1, 6, 12, and 24 months post injury.
Outcomes of interest were the Physical Function (PF) and Mental Health (MH) subscales of the SF-36. Regression models were used to estimate predictors of physical function and mental health over a 2-year period. A subject-specifi c intercept in a mixed model was used to account for repeated data from participants over time.
Results Participants (n = 123) were young (median 37, IQR 22 to 55 years), predominantly male (83%) and spent on average 3 days in the ICU and 3 weeks in hospital. Response rates were over 55% at each follow-up, with responders similar to nonresponders except for being generally older. PF and MH scores improved over time, although the averages remained below the Australian norms at 24 months.
Predictors of PF included IP (β = -1.5, 95% CI = -3.1 to -1.1, P <0.01), SE (β = 1.8, 95% CI = 1.3 to 2.6, P <0.01), hospital length of stay (β = -1.7, 95% CI = -2.0 to -0.8, P <0.01), never having been married (β = 1.8, 95% CI = 0.3 to 5.5, P = 0.03), and having injury insurance (β = -2.7, 95% CI = -6.9 to -1.9, P <0.01). Predictors of MH included PTSD symptoms (β = -2.4, 95% CI -3.4 to -1.4, P <0.01), psychological distress (β = -6.9, 95% CI = -8.9 to -5.2, P <0.01), SE (β = 0.6, 95% CI = 0.2 to 1.1, P <0.01), and unemployment (β = -2.3, 95% CI = -5.0 to -0.2, P = 0.04). Conclusion Trauma ICU patients experience compromised physical function and mental health 24 months after injury. Psychological distress, self-effi cacy and illness perception infl uence outcomes and are potentially amenable to change in response to interventions initiated during hospital stay. Introduction Swiss Diagnosis Related Groups (SwissDRG) have been eff ective since 1 January 2012. The infl uence of this new system on patients' discharge characteristics from a large ICU is not known. With the introduction of the DRG we expect patients to be discharged after a shorter length of stay on the ICU and with higher severity of illness.
Methods The ICU of the City Hospital Triemli in Zurich has an interdisciplinary organization with surgical and internal medical patients, with a maximum occupancy of 18 beds and a center function for the surrounding hospitals. In this ongoing prospective observational study, we collect and analyze the anonymized data of all patients discharged from our ICU prior to and after the introduction of the SwissDRG. The primary endpoint was the length of stay on the ICU in hours. The secondary endpoints were the severity of illness of the patients at the time of discharge, detected by the scoring system SAPS II as well as measured by the number of readmissions to the ICU. Initially all patients were analyzed and in a second step only patients within percentiles 6 to 94 were considered. We also analyzed the subgroups of patients referred internally, patients sent back to referring hospital and patients regionalized to a homebase hospital. The statistics have been done with SPSS and P <0.05 was considered signifi cant. Results We present the results of an 18-month period, 9 months prior to and 9 months after the introduction of the SwissDRG. Data of 1,491 and 1,492 patients were analyzed, respectively. When all patients were included, we found prior to and after the introduction of the DRGs a comparable length of stay on the ICU (mean ± SD of 52.1 ± 2.2 hours vs. 50.82 ± 2.2 hours), no diff erence in the severity of illness at discharge detected by the SAPS II (mean ± SD of 27.9 ± 0.3 vs. 28.4 ± 0.3) and the number of readmissions (91 vs. 92). There was also no signifi cant diff erence when only percentiles 6 to 94 were included or when the three subgroups were analyzed. Conclusion Up to now, the introduction of the SwissDRG has no infl uence on patients' discharge characteristics from a large ICU. Data assessment will continue and further data analysis has to be performed. There are only few data on the infl uence of DRG on ICU patients [1, 2] . We expect that the introduction of the DRG in Switzerland will change the number of admissions from external hospitals to a large ICU with a centre function and will infl uence the severity of disease of the admitted patients.
The ICU of the Triemli City Hospital in Zurich has an interdisciplinary organisation with surgical and internal medical patients, with a maximum occupancy of 18 beds and a centre function for the surrounding hospitals of the region. In this prospective ongoing observational study, we collect and analyse the anonymised data of all patients admitted to our ICU from an external hospital during 12 months prior to (1 January to 31 December 2011) and after (1 January to 31 December 2012) the introduction of the DRG in Switzerland. Exclusion criteria are admissions by the emergency department, self-assignments into the hospital and internal relocations. The primary endpoint is the number of admissions from an external hospital to our ICU. Secondary endpoints are the severity of the disease of the admitted patients, detected by the scoring systems SAPS II and APACHE II as well as the length of stay in external hospitals before admission. The statistical analysis is descriptive.
Results We present the preliminary data for 10 months (in each case January to October) before and after the introduction of the DRG. We observed an increase of 9.2% (391 vs. 427 patients) of admissions to our ICU after the introduction of the DRG. The severity of disease determined by the SAPS II score is unchanged (mean 26.7 vs. 26.0 points, P = 0.466). The severity of disease determined by the APACHE II score is signifi cantly lower (15.4 vs. 14 points, P = 0.017). We also noted that after the introduction of the DRG the patients were earlier transferred from an external hospital to our ICU (mean time until transfer 29.9 vs. 18.7 hours), but this value was not signifi cant (P = 0.55). Conclusion Up to now the introduction of the DRG in Switzerland has had a complex infl uence on the number and the kind of patients (LWP, n = 300); and patients whose waiting time was equal to or less than that period, short waiting period (SWP, n = 113). Results In total, 413 patients were included, 300 of which belonged to the LWP group (65.4%). For the entire cohort, the mean APACHE II score was 19 ± 7, the mean age was 52 ± 22 years, and 211 patients were male (51.1%). The LWP group did not show diff erence in the APACHE II score (19 ± 7 vs. 18 ± 8, P = 0.13), but was older (55 ± 20 vs. 49 ± 23, P = 0.01). LWP also had a higher incidence of primary bloodstream infection (23.8% vs. 10.4%, P = 0.01) and catheter-associated urinary tract infection (10.2% vs. 1.9%, P = 0.01). LWP patients had higher mortality (37.8% vs. 25.9%, P = 0.02) and longer ICU LOS (21 ± 47 vs. 14 ± 18 days, P = 0.01). Relative risk for death in the LWP was 1.74 (95% CI: 1.11 to 2.72). Conclusion Despite showing no signifi cant diff erences on APACHE II scores from the SWP group, patients from the LWP group presented greater incidence of primary bloodstream infection, catheterassociated urinary tract infection, higher mortality outcomes and longer ICU LOS. References
Intensivists are expected to have many roles during and after a major disaster/catastrophe; that is, triage, intensive care, education for people, and so forth. The roles of intensivists against special disaster or nuclear disaster are studied based on actual experiences. Methods Several disasters are studied. The Fukushima Daiichi Nuclear plant explosion after the Higashinihon earthquake 2011 was medically reviewed based on the total 30-day stay on-site in addition to several days around the site. The Chernobyl incident 1986 was inspected 15 years after the incident. Other nuclear disasters are included.
Results Many serious problems were revealed in the medical teams, which are as follows: inappropriate basic preparedness against large special disasters, including nuclear disaster; lack of appropriate education and training for medical teams against nuclear disasterthat is, most members of Japan DMAT or the disaster medical assistance team are still laypersons; incorrect standard/rules of Japan DMAT, which were excessively focused upon cure of the usual type of injury and planned short period or nearly 48 hours, which should be abandoned; and insuffi cient consideration to the weak/vulnerable people or CWAP, children, (pregnant) women, aged people, and the poor people/sicker patients. Many of them died because of an insuffi cient emergency transportation system from their contaminated houses or hospital. Conclusion In order to cope with the special disasters, such as NBC or nuclear, biological and chemical disaster, it is insuffi cient to take makeshift measures or use cheap tricks. Working out the systematization of disaster medicine, based upon the academic viewpoints and philosophy/reliability, is essential to protect the people and the nation.
Variation in acute care burden and supply across diverse urban settings S Murthy 1 , S Austin 2 , H Wunsch 3 , NK Adhikari 4 , V Karir 2 , K Rowan 5 , ST Jacob 6 , J Salluh 7 , F Bozza 8 , B Du 9 , Y An 10 , B Lee 2 , F Wu 2 , C Oppong 11 , R Venkataraman 12 , V Velayutham 13 , D Angus 2
The World Bank has warned that the rapid growth of the world's urban population can only be accommodated safely if cities adequately develop key infrastructure, such as the provision of acute care resources. Yet, even basic descriptive information on urban acute care supply and demand is extremely limited. We therefore conducted a pilot assessment across seven diverse urban settings across the world.
We selected a convenience sample of seven large cities with varying geographical and socioeconomic characteristics: Boston, Paris, Bogota, Recife, Liaocheng, Chennai, and Kumasi. To estimate acute care supply, we developed an instrument to collect data on acute and critical care infrastructure. We collected data from municipal authorities and local research collaborators. We expressed the burden of acute disease as the number of deaths due to acute illnesses, estimated from the 2008 Global Burden of Disease Study. Results were expressed as acute care supply and acute deaths per 100,000 population and acute care supply per 100 acute deaths.
The supply of hospital beds varied from 72.4/100,000 population in Kumasi to 245.8/100,000 in Boston. ICU beds with capacity for invasive mechanical ventilation and intensive nursing services ranged from 0.4/100,000 in Kumasi to 19/100,000 population in Boston. The number of ambulances varied 70-fold between cities. The gap between cities widened when demand was estimated based on disease burden, with a 70-fold diff erence between cities in ICU beds/acute deaths. In general, most of the data were unavailable from municipal authorities. Conclusion The provision of acute care services, a key aspect of urban infrastructure, varied substantially across the seven diverse urban settings we studied. Furthermore, the local municipal authorities generally appeared to have little knowledge of their acute care infrastructure, with implications for future planning and development. Resources may not always be allocated by severity of illness, but by custom or habit, particularly if diff erent groups administer bed control and triage. Specialty-specifi c diff erences may exist even when a single team controls triage. Variability in resource utilization has important implications for cost-containment and triage.
Methods Patients admitted to a single, closed medical/surgical ICU with full-time intensivists and unifi ed triage control in a large, university-affi liated hospital were evaluated during 2011 to 2012. Patients who died in the ICU were excluded. The day of discharge (D/C) and severity using APACHE IV and its related Acute Physiology Score (APS) component were calculated daily for the fi rst 7 days. Trend was assessed across days by Cuzick's test. Results A total of 719 surgical and 925 medical patients met inclusion criteria. In total, 20.2% of surgical and 21.3% of medical patients had an ICU LOS <1; P = 0.58. Admission severity was correlated with length of stay, P = 0.014 for both medical and surgical patients. Medical patients are sicker on admission and D/C from the ICU than surgical patients (P <0.05) (Figure 1 ). Conclusion ICU utilization diff ered by patient type even with unifi ed triage control within a single unit. Surgical patients were less severely ill on admission to and D/C from the ICU. A signifi cant percentage of medical and surgical patients are D/C within 1 day and may be more effi ciently served in a less resource-intensive environment. The reasons for the diff erences in ICU utilization for surgical versus medical patients require clarifi cation and may have implications for both resource utilization and cost. Introduction Interest in safety and clinical outcomes of inpatients has been growing in Japan, because the 100,000 Lives Campaign was introduced under the Japanese Patient Safety Act in 2008. In this act, an introduction of the Rapid Response System (RRS) was one of the mainstreams to inpatients' care. However, many Japanese healthcare providers cannot understand how to achieve the introduction of the RRS, because there are few who have knowledge of the system. Therefore, we developed a new introductory training course for the RRS. The educational eff ectiveness was analyzed through the surveillance questionnaires after the course. Methods The educational program includes a lecture series con cerning the outline and management methods, introduction of facilities that have already deployed, small group discussions, and teaching methods-of-training for the medical emergency team using a simulator. Evaluation was made in the fi ve-point scale by 82 participants (58 physicians, 16 nurses and eight other professions) throughout seven courses. The questionnaires are: A. understanding of RRS, B. knowledge acquisition about patient safety, C. expectation for decreasing the cardiopulmonary arrest by RRS, and D. expectation for decreasing the psychological burden by RRS. Results Seventy-three participants (89.0%) answered the questionnaires. The numbers of participants who scored more than four points were as follows: A. was 71 (97.2%), B. was 70 (95.9%), C. was 64 (87.7%), and D. was 68 (93.2%), respectively. The majority of participants obtained the correct knowledge, and had a solid understanding for the RRS. It was evident that providing abundant material and didactic lectures traced from the introduction to management, and collecting and resolving the questions, promoted comprehension. However, there is a limitation of whether or not the participants introduce the RRS into their own institutions. It is essential to improve the course and continue to support the activities of the participants. Conclusion Our training course may promote the introduction and dissemination of the RRS in Japan.
Introduction Teaching of medical ethical issues including confi dentia lity and consent have long been a small part of the medical curriculum. These issues are more complex in an ICU where patients may lack capacity. Documents such as Good Medical Practice 1995, Confi dentiality 2009 and the Mental Capacity Act 2005 give guidance to medical professionals in these matters in the UK. Methods A questionnaire was distributed amongst staff in four ICUs in South London. Results were analysed according to level of experience and background (medical/nursing or allied health professional (AHP)).
Of 225 questionnaires distributed, the response rate was 66% (31% doctors, 56% nurses and 13% AHP). Staff with either less than 1 year experience or greater than 10 years experience had the greatest exposure to the Mental Capacity Act and Data Protection Act, suggesting a gap in knowledge in staff with intermediate experience.
Knowledge of the Caldicott principles were unaff ected by experience, with many experienced respondents having 'No Idea' . The majority of respondents (unaff ected by experience) felt that when giving information to relatives face to face, relatives should be kept fully informed. When giving information over the telephone, most doctors felt the response should be tailored to the knowledge of the person being spoken to whilst nurses were split between tailoring the response, giving full information, setting up a password system and not giving any information at all. Most respondents felt date of birth and hospital number constituted 'Patient Identifi able Information' . However, experienced staff did not appreciate the importance of unusual diagnosis and clinical photographs as also being able to identify patients. Similarly, the majority knew that the patient themselves identifi ed the 'Next of Kin' but 7% (unaff ected by experience) felt this was decided by the family and felt the family could decide on resuscitation status. When consent is required for an elective procedure in a patient who lacks capacity, doctors tended to have a better understanding of the need to delay the procedure where possible than nurses, the majority of which felt this could be decided by the next of kin or two consultant doctors. Most doctors felt that 'Acting in the Patient's Best Interests' would mean doing what would give the patient the best outcome rather than doing what the patient would have wanted (unaff ected by experience). The majority of staff , on answering this questionnaire, felt that they lacked suffi cient knowledge on the subject and most felt annual reminders would be useful.
The ICU is an environment where issues of consent, confi dentiality and disclosure of information occur daily. Staff feel they lack knowledge in these areas that is unaff ected by their experience. We need to ensure that all staff have the necessary knowledge to deal with these situations. Introduction Alcohol-related hospital and ICU admissions are known to have a huge impact on healthcare resources in the UK. Excessive use of alcohol is independently associated with sepsis, septic shock and hospital mortality among ICU patients. This study assesses the relationship between alcohol abuse and intensive care resource utilisation in a mixed medical, surgical and neurosurgical ICU. Methods A prospective survey of emergency alcohol-related admissions over a 1-year period was undertaken at a tertiary university adult general and neurosurgical ICU. All patients were screened for acute and chronic alcohol abuse on admission. Acute alcohol abuse was defi ned as being intoxicated with alcohol at the time of admission and chronic alcohol abuse was defi ned as chronic alcohol use exceeding recommended UK national guidelines on consumption. The amount of alcohol consumption was obtained, diagnosis on admission, ICU and hospital mortality, length of stay, and total cost were recorded. All patients were screened for alcohol-related comorbidities. Comparative retrospective data were obtained for the same time period for nonalcohol-related emergency ICU admissions. Data were analyzed using SPSS. Results In total, 7.7% of patients were admitted with a history of acute/chronic alcohol excess. Sixty-seven per cent of alcoholrelated admissions were due to acute alcohol excess. Neurosurgical patients admitted due to alcohol excess had higher ITU mortality than nonalcohol-related neurosurgical patients: 32.1% versus 14.39% (P = 0.02), respectively. Ninety-three per cent of alcohol-related neurosurgical admissions were caused by acute alcohol intoxication. The intensive care cost was signifi cantly higher for alcohol-related (£12,396 per patient) compared with nonalcohol-related neurosurgical admissions (£7,284 per patient). Of the medical patients admitted, 60% of these admissions were due to acute alcohol excess. The cost of intensive care treatment was lower for alcohol-related medical admissions.
Conclusion This is one of the largest studies of alcohol-related admissions to critical care. Our survey confi rms that alcohol-related admissions to the ICU are commonplace; however, our frequency is signifi cantly less than previously reported. Our study reveals interspecialty variations in demographic data, APACHE II scores, mortality and cost of admission. Neurosurgical alcohol-related admissions bear higher mortality and result in greater resource utilisation relative to nonalcohol-related neurosurgical admissions. Alcohol continues to burden both our patients and critical care. During the fi rst three postoperative days, preoperative AHI >30 was associated with a prolonged weaning time, a reduced oxygenation index (arterial pO 2 /FiO 2 ), an impaired kidney function, an augmented infl ammatory response and an overall increased length of stay in the ICU. The observed association of high preoperative AHI values with postoperative clinical characteristics remained statistically signifi cant throughout the fi rst three postoperative days. Conclusion Undiagnosed SDB is highly prevalent among cardiac surgical patients. Clinical trajectories of individuals with severe SDB are described by a prolonged recovery of pulmonary function, delayed weaning and a pronounced infl ammatory response after surgery. Screening for SDB might identify patients that are susceptible for a complicated postoperative course.
Introduction A literature review was performed to assess whether massage benefi ts patients postoperatively following coronary bypass grafts (CABG) and or valve replacement/repair. A case study on a patient who had suff ered a hypoxic brain post cardiac arrest was conducted. Methods A review on MEDLINE and Cochrane using search terms massage, cardiac and ICU identifi ed nine research papers on the benefi ts of massage postoperatively for the aforementioned patient group. Other papers were listed but unrelated to cardiac surgery. None of the nine papers identifi ed for this review were ICU specifi c in the title but the ICU was mentioned in the main text body. For the purpose of this review the selected papers are researching the eff ects of massage on physiological parameters, anxiety, pain, calm and perceived stress indicators in the CABG and/or valve repair/replacement. Out of these nine papers, one is British (2002). Five are American (2006 to 2012), two are Brazilian (2010) and one is an Indian paper (2010). All papers are randomised control trials (RCTs). Papers written prior to 1999 were excluded from this literature review.
Introduction VAP has continued to be a major cause of morbidity and mortality in critically ill patients in Thailand for decades. Previous research found that the implementation of VAP care bundles and the educational program can reduce VAP incidence in the ICU [1] . In this research we aimed to observe the reduction of VAP incidence after the implementation of VAP care bundles to ICU medical personnel. Methods Inclusion criteria: all adult surgical patients (>18 years old) who are on ventilatory support in the surgical ICU at Siriraj Hospital. There are two groups, divided into pre-educational group (group I) and post-educational group (group II) (n = 220/group). We also observed the adherence rate to VAP care bundles according to the educational program. The pretest and post-test to determine the effi cacy of the educational program were done. The VAP care bundles consisted of weaning according to weaning protocol, sedation vacation, headof-bed elevation, measurement of cuff pressures four times/day, 2% chlorhexidine use for mouth care and emptying of ventilator circuit condensate.
Results There were 45.38 and 25.25 episodes of VAP per 1,000 ventilatordays in group I and group II, respectively (P = 0.020). The incidence of VAP was 21.82% in group I and 9.09% in group II (P = 0.000). There was signifi cant reduction in the length of ventilatory support per person (group I = 2, group II = 1 (median), P = 0.013, 95% CI = 0.319 to 0.936) and mortality rate (group I = 15.5%, group II = 8.2%, P = 0.017). There was no signifi cant diff erence in LOI, LOH and ATB cost. The pretest scores were 15.53 and 17.53 on average from 40 medical personnel in group I and group II, respectively (P = 0.000). The head-of-bed elevation adherence rate was improved after the educational program (group I = 50.1%, group II = 70.36%, P = 0.017). But the adherence to other bundles was not improved. See Tables 1 and 2 . Introduction Following our study of severe sepsis care across three centres [1] , we aimed to introduce a rapid feedback mechanism into our rolling audit programme. Whilst previous audits raised awareness of severe sepsis, only whole organisation performance was reported and no feedback was given to individual clinicians. It is recognised that such feedback loops can improve clinical practice [2] . Methods Patients admitted to critical care (58 beds, four units) with a primary admission diagnosis of infection were screened for severe sepsis. Pre-ICU care was then audited against the Surviving Sepsis Guidelines [3] . Time zero is defi ned as when criteria for severe sepsis were fi rst met. An individualised traffi c-light report was then generated and emailed to the patient's consultant and other stakeholders involved in care (Figure 1 ). We aimed to report cases within 7 days of critical care admission. A cumulative report is generated monthly to track organisation-wide performance.
Since November 2011, 153 cases of severe sepsis have been audited and reported back to clinicians. Compliance with antibiotics in <1 hour has risen from 35 to 75% and compliance with the pre-ICU elements of the resuscitation bundle has risen from 20 to 70% ( Figure 2 ). Feedback from clinicians has been encouraging as our reports highlight both positive and negative examples of practice. Conclusion Individualised feedback on sepsis care has led to substantial improvements in guideline compliance. This concept could be translated to other time-dependent patient pathways.
Introduction When we talk about safety culture, we speak of being aware that things can go wrong. We must be able to recognize mistakes and learn from them, sharing that information fairly and impartially to try to prevent its recurrence. Organizations such as the Agency for Healthcare Research and Quality (AHRQ) have developed tools to help organizations measure their safety culture and there is little information about our country. Methods A descriptive survey study. We sent the Spanish version of the questionnaire on patient safety culture (AHRQ) to the nursing staff of a polyvalent ICU of 42 beds in a tertiary hospital.
The questionnaire was sent to 179 nurses, receiving correctly answered 88 surveys (response rate of 49.16%). On a scale of 0 to 10, 6.97 points was obtained to estimate the safety climate for staff respondents. The item best scored was teamwork in the unit (65.9%). Detected as a fortress, 'communication between nurses at shift changes' (76.1% positive responses). The worst rating was obtained in the section on human resources, followed by management support in the fi eld of patient safety. Conclusion The perception of safety culture in an ICU by nursing staff is far from optimal levels. The team work dimension was identifi ed as the most valued by workers, with the transmission of information on shift changes the most valued item. Methods To compare our number of admissions, related activity and case-mix indicators 1 year before and after the geographical change was done. We analyzed our whole number of patients admitted to the ICU. We used the chi-square test for categorical variables and one-way analysis of variance for quantitative data. Minitab and Statbas statistical programs were used. We plotted activity data using the Barber-Johnson 1 diagram. Results A total of 2,774 cases (63% males; mean age 61 years) were admitted to our ICU during the period (1 year before and after the transfer). No diff erences between both groups were founded in demographic data, Knaus score and NYHA status. Regarding their origin, we found more patients admitted from other hospital centers (20 vs. 29%; P <0.001). APACHE II score increased from 17.24 to 19.08% (P <0.001) and a slight increase change in SAPS 3 score was also found (52.29 to 53.75; P <0.01 
There are several defi nitions of level 1 (L1) care, all refer to a group at risk of clinical deterioration on the ward [1] [2] [3] . There is evidence that ward patients who become acutely unwell often receive suboptimal care [4] . A regional study commissioned by Norfolk, Suff olk & Cambridgeshire Critical Care Network (NSCCCN) found that a majority of ward patients may be of L1 dependency and death rates appear to be correlated with L1 status. We aim to examine the relationship between the ward distribution of illness acuity, staffi ng and patient outcome. Methods Data were collected as part of NSCCCN's observational prevalence study in 2010. Ward surveys included acuity of illness, staffi ng levels and skill mix. Secondary data were obtained from the Patient Administration System. Emergency, oncology, paediatric and maternity units were excluded. Results Complete datasets were obtained from 1,402 patients in 22 wards in our university hospital over two seasons. This constitutes 98.3% of inpatients from those wards. The mean ward occupancy rate was 94% (10th to 90th percentile: 85% to 100%). At least one L1 acuity criterion was scored by 898 (64%) patients, with 25% from geriatrics followed by orthopaedics (17%) and general surgery (10%). Each ward had an average of eight qualifi ed nursing staff (range: 4 to 12) equating to an average staff :patient ratio (SPr) of 0.253. There was no correlation between ward occupancy and nursing staff (Pearson correlation, corr: 0.55), nor between prevalence of L1 criteria and staffi ng (corr: 0.34). The admission rate to intensive care was noted to be higher if the patients were nursed in a ward with lower than average SPr compared with higher SPr (2.7% vs. 1.2%, P = 0.058 Fisher's exact), but this was not statistically signifi cant. Senior nursing (Band 6) staff were part of the skill mix on only nine of 44 ward surveys. Conclusion Better outcome with improved SPr may be unsurprising, although if proven conclusively would signifi cantly inform workforce planning. Lack of correlation between staffi ng levels and occupancy or acuity is also interesting given that we know L1 criteria are associated with worse outcome.
Introduction Prolonged shifts, workload, stress, and diff erent confl icts are associated with burnout, loss of psychological wellbeing, and probably with an inadequate sleep quality (ISQ). This relevant disturbance leads to deterioration of the work performance, may impair quality of care provided to patients and increases the incidence of serious adverse events. The objective was to determine the prevalence of ISQ and sleepiness among Uruguayan ICU workers, and to evaluate risk factors associated with ISQ. Methods A survey was conducted in six Uruguayan ICUs. The sleep quality was evaluated on the basis of the Pittsburgh score (PS), and the sleepiness was identifi ed by the Epworth scale. ISQ was defi ned as PS greater than 5 points and sleepiness by an Epworth scale higher than 6 points. ICU's, patient's, and clinician's characteristics were assessed for their association with the prevalence of ISQ. All variables with P <0.2 in univariate analysis were included in a model of ordinal regression. P <0.05 was considered statistically signifi cant.
Results The survey was completed by 129 ICU workers. The global prevalence of ISQ in ICU was 67.4%. ISQ was observed in 45% of physicians and 82% of nurses and nurses assistant (P <0.001). Sleep medication was used by 13.3% of the ICU team. Univariate analysis showed that ISQ was signifi cantly associated with sex (73% vs. 43%, P = 0.03 in women and men, respectively), marital status (84% vs. 61%, P = 0.01 in single and couple workers, respectively), more than 60 hours working in the last week (76% vs. 61%, P = 0.07) and less than 6 sleeping hours (95% vs. 54%, P <0.0001). Multivariable analysis demonstrated that a sleep duration less than 6 hours was independently associated with ISQ (OR = 24.5; 95% CI = 5.2 to 115.8; P <0.0001). Furthermore, pathologic sleepiness was present in 59.3% of ICU workers. Sleepiness was independently associated with use of sleep medication (OR = 5.9; 95% CI = 1.2 to 28.5; P = 0.025). Conclusion The prevalence of ISQ and sleepiness is very high among ICU workers. Those disturbances are independently associated with a sleep duration less than 6 hours, and sleep medication use, respectively. These results highlights that strategies to decrease ISQ and sleepiness in ICU clinicians are urgently needed to improve work performance, improve quality of care provided and prevent adverse events.
Introduction Work-related stress is a potential problem among doctors and is associated with anxiety, depression, reduced job satisfaction, days off work, errors and near misses [1] . To compare stress levels between diff erent groups of doctors and identify causes of stress, we conducted a survey at University Hospital Lewisham using the UK Health and Safety Executive's Management Standards (HSEMS). HSEMS is a validated tool developed to identify work conditions that warrant interventions to reduce stress levels across organisations [2] . Methods We conducted an anonymous survey of doctors working in anaesthetics, intensive care, general medicine and accident and emergency (A&E) departments over 6 weeks using the HSEMS question naire. We also surveyed awareness of the Trust's stress management services and whether staff had a designated supervisor or mentor. Results were analysed using the HSEMS Analysis Tool, which rates stressors with a score from 1 to 5 (5 represents the lowest amount of stress). We compared the Trust's results against HSEMS national standards.
Results Seventy-two doctors completed the survey. Lowest stress levels were found in doctors working in intensive care (n = 12, mean 3.63, SD 0.39). This was followed by medicine (n = 26, mean 3.55, SD 0.47), anaesthetics (n = 27, mean 3.40, SD 0.44), and A&E (n = 7, mean 3.11, SD 0.65), which had the highest stress levels. There was no signifi cant diff erence in stress levels between diff erent grades of doctors. When compared with HSEMS targets, staff relationships and peer support exceeded national standards. However, management of organisational change and demands at work need improvement. The majority of doctors (82%) had no idea what stress management services were provided by the Trust. Seventy-nine per cent of doctors had an allocated supervisor or mentor, 91% of those felt able to approach their supervisor. Conclusion These survey results provide reassurance that stress levels in intensive care compare well, despite critically unwell patients and higher mortality rates. We identifi ed areas that need improvement within the Trust and will present these results to all relevant departments. With the support of hospital management we will initiate HSEMS-validated measures to reduce stress.
Introduction Although recent reports show an improvement in outcomes for pediatric hematology patients requiring intensive care [1, 2] , respiratory failure remains one of the major risks of pediatric mortality. This study was conducted to assess our hypothesis that mortality associated with respiratory failure is higher than that for other organ failures in pediatric hematology patients admitted to our ICU. Methods A retrospective study analyzed children with hematological disorders admitted to our ICU between April 2005 and June 2012. All of the included children required emergency admission and invasive mechanical ventilation. Those who did not need intubation, or required intubation only for therapeutic intervention and died within 24 hours of ICU admission were excluded. The survival group was defi ned as patients who were discharged from the ICU, and the nonsurvival group was defi ned as those who died in the ICU or within 7 days after discharge from the ICU. The PELOD score and PIM-II were applied as morbidity scoring systems Results Twenty-seven patients, including 18 males and nine females, with a median age of 6.1 years (range, 0.2 to 16.6 years) were analyzed. Sixteen patients had leukemia, fi ve had hemophagocytic syndrome, six had solid tumors. The average predicted mortality rate was 31.3% in PIM-II. The survival group included 15 patients (56%) and the nonsurvival group included 12 patients (44%). When the survival group was compared with the nonsurvival group, there were no signifi cant diff erences in the systolic blood pressure (101.3 ± 13.9 mmHg vs. 92.8 ± 25.4 mmHg; P = 0.15), the proportion of patients requiring continuous renal replacement therapy (33.3% vs. 50.0%; P = 0.30), and PELOD score (15.5 ± 10.4 vs. 21.8 ± 15.4; P = 0.22). In the nonsurvival group, the PIM-II was higher than that in the survival group (27.9 ± 10.4 vs. 35.7 ± 9.0; P = 0.06); the PaO 2 /FiO 2 (272.5 ± 136.7 vs. 153.3 ± 123.3; P = 0.03) and oxygenation index (6.7 ± 8.1 vs. 14.1 ± 9.5; P = 0.04) were signifi cantly worse in the nonsurvival group than in the survival group.
Conclusion The data show that respiratory failure is more strongly associated with mortality than other organ failures in pediatric hematology patients requiring intensive care. These results also suggest that mechanical ventilation intervention in patients with respiratory failure must occur earlier to improve the outcomes for these patients.
Introduction Critically ill patients with haematological malignancies (HM) have high hospital mortality [1] . Severity of illness scores may underestimate mortality in such patients [2] . Methods Data collection was conducted at three hospitals from 2008 to 2011. Patients with any active HM condition were matched with two control patients at two hospitals and with one control at Christie Hospital. Control patients had the same APACHE II (within 2 points) and admission diagnosis, but no HM. Readmissions and planned surgical cases were excluded. Results A total of 163 patients with HM were compared with 237 control patients. Seventy-four admissions with HM were identifi ed at two hospitals, and each was matched with two control patients. Eightynine admissions with HM from Christie Hospital were identifi ed. These were matched with 89 controls. Patients with HM spent signifi cantly longer in hospital before ICU admission (Table 1) . Unit and hospital mortality rates were not statistically diff erent between patients with HM and without HM ( Table 2) . Conclusion Unit mortality of critically ill patients with HM was similar to those without HM. Hospital mortality in patients with HM was higher than those without HM, although not statistically signifi cant. Severity of illness at presentation to critical care is the main determinant of outcome in patients with HM.
group when requiring emergency admission to the ICU in a tertiary cancer centre. Methods A retrospective review of medical notes between 2004 and 2012. Results A total of 249 patients were admitted, of whom 54 had more than one admission. There were 310 episodes in total. Leukaemia n = 85; lymphoma n = 90; myeloma n = 36. We compared the characteristics of those who survived ICU admission with those who failed to survive to discharge from ICU. The two populations were similar (age 51 vs. 57; males 59% vs. 57%). Those who survived had a lower APACHE II score on admission (19 vs. 23; P <0.001), lower mean organ failure scores (1 vs. 2; P <0.05), lower requirements of inotropes (26% vs. 50%; P = 0.001), ventilation (31% vs. 64%; P = 0.001) and fi ltration (11% vs. 26%; P = 0.004). There was no diff erence in the prevalence of sepsis at the time of admission (64% vs. 70%). Both groups included patients with prior bone marrow transplant (38% vs. 40%). Of note, ICU and 6-month survival were 27% and 50%, respectively. These values are lower than those reported in the literature to date. Conclusion ICU and 6-month mortalities were 27% and 50%, respectively. Patients with haematological malignancy stand to benefi t from intensive care, and should be off ered admission based on clinical need.
Introduction Many evidence-based interventions are not delivered to patients [1] . This may not be due to a clinician's intentional decisions. The aim of this project was to compare the use of starch before and after removing it as an option from an e-prescribing template.
Methods Our e-prescribing software enables users to prescribe intravenous fl uids from a series of menus. One of these is a template that has several fl uids available to use as a bolus when instructed by a clinician. We removed starch as an option from the template in April 2009. Starch could still be prescribed elsewhere on the prescribing system. Data on the use of starch from November 2008 to November 2012 were analysed as the mean volume of starch infused per patient per month. The mean of each set of parameters was then compared using a Student's t test.
Results The mean volume of starch per patient administered before and after electronic prescription options were altered was 480 ml and 21 ml, respectively (P = 0.004). See Figure 1 .
Conclusion Despite clinicians intending to reduce the use of starch it was still regularly administered on our ICU. The removal of a default prescribing option dramatically reduced the volume of starch used whilst not restricting the ability to make a conscious choice to prescribe it. Adjusting default options has potential to infl uence clinical decisions and ensure more reliable, evidence-based care. Introduction Early detection of sepsis is important for a suffi cient treatment to reduce mortality. We hypothesized that using modifi ed systemic infl ammatory response syndrome criteria over 1 hour using an electronic software program facilitates the clinical diagnosis of sepsis. Methods After IRB approval and informed consent we enrolled in this prospective, observational, single-center study 1,119 consecutive patients (age 68.6 ± 16.4, female/male 476/649) admitted over a 6-month period to a surgical ICU. A total 149 of them met modifi ed systemic infl ammatory response criteria. Patients were monitored by an electronic software program using live data from the laboratory and bedside monitors to detect modifi ed systemic infl ammatory response syndrome criteria persisting over 1 hour. The physicians were blinded to the software program alerts that notifi ed in real time when modifi ed systemic infl ammatory response syndrome criteria were detected and persisted over 1 hour, but did not provide treatment recommendations. Results There was a total of 149 modifi ed systemic infl ammatory response syndrome criteria alerts. Seventy-four were confi rmed as true sepsis cases by physicians. The overall incidence of sepsis was 7%. Patients were categorized into length of stay <24 hours, 24 to 96 hours and >96 hours. The overall sensitivity of our system for detecting sepsis was 68% and the specifi city was 91%. The positive predictive value is 34% and the negative predictive value is 98%. Conclusion Real-time alerts using an automated, electronic monitoring of modifi ed systemic infl ammatory response syndrome criteria facilitate the clinical diagnosis of sepsis. beds. Intentional rounds or proactive patient rounds were recognised by the Royal College of Physicians and the Royal College of Nursing [1] as structured, evidence-based processes for nurses to carry out regular checks with individual patients at set intervals. The senior nursing team decided to adapt this initiative to the intensive care setting in order to address clinical challenges and provide guidance for shift leaders to focus on key elements of care. Methods Our intentional rounds, performed once per shift (twice daily), include two components. First, pressure area care -this component involves the shift leader checking whether key elements of pressure sore prevention have been performed. These include completion of the Waterlow risk assessment tool [2] , noting the frequency of repositioning, use of lateral positioning and pressure-relieving pads. Second, renal replacement therapy rates -this element was identifi ed as an area for focus after we established that our haemofi ltration fl uid use per hour of therapy was twice that of a near identical clinical setting. This pattern continued even after adopting similar therapy guidelines. The shift leader was guided to check whether therapy rates had been adjusted in line with latest biochemical results.
The incidence of pressure ulcers in the 4 months since the initiative began has averaged 2.25 per month compared with 7.8 per month prior to commencement of intentional rounding. Added to the rounding tool at the end of September 2012, RRT rates in the preceding 4 months averaged 31.5 ml/kg/hour over 24 hours, an 11.9% reduction from the previous average of 35.75 ml/kg/hour. If the pattern of RRT was to continue, this could equate to a cost saving of UK£40,000 per annum.
Conclusion The use of a modifi ed targeted intentional rounding tool by the nursing shift leader can help ensure that best practice guidelines are adhered to. This strategy can improve patient outcomes and provide potentially signifi cant fi scal benefi ts. References Introduction Handovers are often associated with poor communi cation. ICU patients with multiple complex problems are ideal to study naturally occurring handovers. However, few studies have been conducted in the ICU. Methods We conducted questionnaires of physicians and nurses involved and observed handovers in real time of medical ICU patients over 1 month.
We interviewed 580 of 672 physicians and nurses involved (86.3%) and observed 90 real-time handovers (45 patients, 26.8%) of 168 patients. Mean duration of handover was 391.3 (± 263.6) seconds, 78.5% were face to face and 1.26 (± 1.75) distractions per handover were noted, person-to-person calling being the commonest mode of distraction (46.7%). Nurses received training during induction in signifi cantly higher numbers, covered allied specialties more and reviewed the patients early (all P <0.05). Perception of the relative importance of diff erent components of the handover varied signifi cantly between donors, recipients, physicians and nurses. Both physicians and nurses seldom (39.7%) reviewed the available electronic past medical records of the patient before handover, which in addition to training in handover and overall confi dence level in the management following handover are signifi cantly associated with better satisfaction in univariate analysis; only the confi dence level in patient management remained signifi cant after multivariate analysis. However, agreement between donor and recipient on overall satisfaction was poor (P >0.05). Nursing handovers were signifi cantly longer than physicians' (572.08 ± 214.68 vs. 168.6 ± 97.27 seconds, P <0.001) but are also associated with higher distractions particularly during evening shifts. Conclusion A higher percentage of nurses received handover training; nursing handovers are longer and more inclusive of other components of patient management; perceived importance of components of handover varies among healthcare professionals; distractions are common during handovers and associated with longer duration, by nurses and in the evening shifts; and higher confi dence level in patient's management following the handover is associated with better satisfaction.
using telemedicine to provide acute burn and critical care consultation on pediatric and adult burn patients in Lviv, Ukraine, as well as in triage and transport of critically ill patients from Lviv to a tertiary-care facility in the USA for further management. Methods Using a new telemedicine learning center established at City Hospital #8 in Lviv, Ukraine, consultations regarding acutely injured burn victims occurred between physicians in Ukraine and physicians at Shriners Hospital and Massachusetts General Hospital in Boston. After the initial presentation, each patient was reviewed on a daily basis by physicians in Boston. Skype, an Internet-based communication tool, was used in communication with the Burn Center in Lviv. Radiographic images were scanned and digitalized using an electronic scanner, and JPEG image compression was used to facilitate the transmission of radiographic images and patient charts. Informed consent and HIPPA guidelines were followed in transmitting any patient-related information.
Results Since 2011 we have provided consultation on 14 patients in Lviv, Ukraine, ranging in age from 15 months to 63 years. Each patient had an average of six consultations. We present two of these cases as examples of the capabilities of our telemedicine program. The fi rst case involved a 15-month-old female with 40% TBSA from scald injury, where telemedicine was instrumental in the primary assessment as well as to arrange a direct assessment from a nearby burn surgeon. The second case resulted from a house fi re with multiple casualties, where physicians in Boston were able to utilize telemedicine to guide the initial resuscitation and airway management of three critically burned children, as well as to arrange for transport of one of the victims, an 11-year-old male with 87% TBSA, from Ukraine to the USA for acute management. Multiple diffi culties were overcome in implementing the system between the two countries including: time zone diff erences, language barrier, and diff erent approaches to patient care. Conclusion We have established a telemedicine program linking physicians in Boston, MA, USA with City Hospital #8 in Lviv, Ukraine to improve care in pediatric and adult burn patients. Our program has provided consultation on 14 patients since 2011, and it highlights the capabilities of telemedicine for acute consultation as well as triage and transport of critically ill patients to tertiary-care facilities. Introduction During the last few years the frequency of end-oflife decisions (EOLD) signifi cantly increased in ICUs. The method of nurse involvement in making EOLD is diff erent worldwide [1, 2] . The purpose of this study was to analyze opinions of nurses about therapy restriction. We have examined with a multicenter study the opinions of the medical stuff about end-of-life care in Hungarian ICUs. Methods We performed a questionnaire evaluation among physicians and nurses of ICUs about infl uencing factors of therapy restriction, the method of the decision-making process, and the frequency of diff erent EOLD. The questionnaire, containing 21 questions, was delivered electronically to Hungarian ICUs, and then we analyzed the responses anonymously. The retrieved 302 answers (191 physicians, 102 nurses) were analysed using a nonparametric Student's test.
Results A total 71% of the nurse responders work in university clinics, 2% in regional centrum, 24% in municipal hospital, 3% in other ICUs. The nurses found both human (2.72/5 vs. 1.98/5) and material (2.81/5 vs. 2.12/5) resources more restrictive factors during patient admission than physicians (P = 0.025, P = 0.0024). Nurses working in municipal hospital were more strongly infl uenced by lack of material and human resources (3.34/5, 3.3/5) than nurses working in university clinics (2.2/5, 2.43/5), P = 0.01, P = 0.025. Younger nurses (working between 6 and 10 years) were more interested in the patient's or surrogate's wishes than older nurses (working more than 10 years). Religion did not infl uence patient admission and forego therapy; however, religious nurses compared with atheists and nonpracticing believers preferred to prolong therapy against the patient's will (P = 0.04). Nurses felt that physicians slightly involved them in the end-of-life decision-making process (2.1/5 vs. 2.4/5 P = 0.0001). Conclusion We found that the workplace, level of medical attendance, godliness, work experience, and position in medical staff strongly infl uenced making EOLD. While limitation of the therapy should be team work, nurses felt their opinions were hardly taken into consideration, although nurses seemed to be more realistic in the decision-making process.
Introduction More than one in fi ve people admitted to an ICU will die there.
Research has highlighted concerns about support for patients and families and decision-making in this context [1, 2] . Here, we describe the development and evaluation of a tool to improve palliative care in a 32-bed general ICU in a central London teaching hospital. Methods Medical Research Council guidance for complex interventions Phase 0 to I comprised literature review, theoretical modelling, observation and qualitative interviews and focus groups with staff and families exploring concerns and views of interventions identifi ed in the literature review. Phase II comprised intervention development, implementation and evaluation of tool feasibility and eff ects using staff survey, observation, audit of records and relative survey. Results Phase I: 47 staff and 24 family members were interviewed. The short time between decisions for treatment withdrawal and death, plus concerns for support management, communication and decision-making, highlighted a need to ensure excellent psychosocial assessment for all. Phase II: as part of integrated care guidelines, we developed the King's Psychosocial Assessment and Care tool (K-PACE). K-PACE is used for all patients entering the ICU, completed within 24 hours of admission. It contains psychosocial assessment of the family and patient needs, and identifi es key individuals for contact. Educational training was supported by K-PACE and was implemented in two waves. Post-implementation survey of 95 ICU staff found that most (80%) were aware of K-PACE. Eighty-two per cent of nurses but only 17% of doctors had completed the tool. In total, 158/213 (74%) family members responded to the survey (additionally three patients responded). There were high levels of satisfaction for symptom control and psychosocial care but concerns continued regarding explanation of treatment and care. Conclusion K-PACE is a feasible tool to improve the palliative care of patients and their families in the ICU. Further refi nement is needed and planned, with consideration of roll-out into the wider medical centre.
be concerned involving the family's will. Especially, stopping or withdrawing therapy is a quite diffi cult operation in Japan because of legal issues. Our hypothesis is that some diff erence exists in thoughts between physicians and nurses for terminal patients in the ICU. The aim of this study is to know their real thoughts. Methods A questionnaire survey was performed on physicians and nurses in our medico-surgical ICU. The questionnaire consists of 11 questions with fi ve optional answers related to the thoughts of participants about treatment of hopeless or brain death patients. Concretely, the questions were; whether to withhold therapy or not, whether to accept to withdraw therapy or not and with family's will, whether to accept to immediately stop therapy and with family's will, whether to positively or not donate organs from a brain death patient, necessity of ICU care for brain death patients, and feeling guilty and stress for stopping or withdrawing therapy. The optional answer has fi ve gradations from 'Yes' to 'No' for all questions. The participants were asked to answer the questionnaire by expressing themselves without regarding legal issues or the consensus. It was guaranteed to be anonymous for them in the data analysis. The answers were compared between physicians and nurses. The Mann-Whitney U test was used for statistical analysis. P <0.05 was considered statistically signifi cant. Results There were in total 52 participants (response rate 98.1%) with 20 physicians and 32 nurses. Withdrawing therapy was signifi cantly accepted in nurses than in physicians (83% vs. 55%, P = 0.039), when the family well understood. Withholding therapy should not be operated for brain death patients for physicians (65%), while it seemed a diffi cult judgement for nurses (23%, P = 0.021). ICU care for brain death patients is less necessary for physicians than nurses (80% vs. 53%, P = 0.016). There were no signifi cant diff erences in other questions between physician and nurses such as feeling guilty or stress for stopping or withdrawing therapy. Conclusion Some of end-of-life thoughts in the ICU showed diff erences between physicians and nurses.
Introduction Optimal patient evaluations of ICU rehabilitation therapy remain unclear. Methods One hundred ICU patients with acute respiratory failure were randomized to receive early rehabilitation (ER) or usual-care (UC). Cohort 1 (n = 50) received ER as one physical therapy (PT) session/day versus UC; Cohort 2 (n = 50) received ER as 2 PT/day with the second session resistance training, versus UC. UC was without ER. Blood was drawn for cytokines through day 7. Cohort 2 underwent strength and physical functional assessments using the Short Physical Performance Battery (SPPB), a valid and reliable measure of physical function consisting of walking speed, balance, and repeated chair stands. It is a well-studied composite measure in older persons, but has not been used in ICU survivors. Small changes of 0.5 to 0.6 points in the SPPB have been shown to be clinically meaningful. Conclusion In this pilot study, early ICU rehabilitation was safe, and was associated with numerically although not statistically shorter hospital stay, greater strength and improved functional scores. Particularly, the SPPB demonstrated discriminatory ability in groups of ICU survivors with low physical function. Future early ICU rehabilitation studies should consider ICU survivor assessments using the SPPB due to its ease, reproducibility and discriminatory ability following ICU and hospital discharge.
of the demographic variables such as sex, age, education, race and length of stay had an eff ect on perceived quality of care. Conclusion The CQI 'R-ICU' turned out to be a valid, reliable, sensitive and feasible instrument. Large-scale implementation is recommended.