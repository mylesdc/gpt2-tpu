{
    "paper_id": "c751c8772ce889d767d8896227d82d3f8f5f3bab",
    "metadata": {
        "title": "Disease dynamics in a stochastic network game: a little empathy goes a long way in averting outbreaks Supplementary Material",
        "authors": [
            {
                "first": "Ceyhun",
                "middle": [],
                "last": "Eksin",
                "suffix": "",
                "affiliation": {},
                "email": ""
            },
            {
                "first": "Jeff",
                "middle": [
                    "S"
                ],
                "last": "Shamma",
                "suffix": "",
                "affiliation": {},
                "email": ""
            },
            {
                "first": "Joshua",
                "middle": [
                    "S"
                ],
                "last": "Weitz",
                "suffix": "",
                "affiliation": {},
                "email": ""
            }
        ]
    },
    "abstract": [
        {
            "text": "We recall the utility function of individual i defined in the Methods section below,",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Abstract"
        },
        {
            "text": "(S1)",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Abstract"
        },
        {
            "text": "Define the space of probability distributions on the action space (0 \u2264 a i \u2264 1) [0, 1] as ([0, 1]). A mixed strategy profile \u03c3 (\u00b7) is a function that maps the state s \u2208 {0, 1} n to the space of probability distributions on the actions space, i.e., \u03c3 i : {0, 1} n \u2192 ([0, 1]). The definition of a mixed MMPE strategy profile \u03c3 * := {\u03c3 * i : {0, 1} n \u2192 [0, 1]} is a distribution on the action space that satisfies the following, for all t = 1, 2, . . . , and i \u2208 N ,",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Abstract"
        }
    ],
    "body_text": [
        {
            "text": "for any \u03c3 i \u2208 {0, 1} n \u2192 ([0, 1]) where \u03c3 * N i := {\u03c3 * j : j \u2208 N i }. From the existence of a mixed Nash equilibrium in games with continuous payoffs, we know that a mixed MMPE exists for the game with payoffs (S1).",
            "cite_spans": [],
            "ref_spans": [],
            "section": ""
        },
        {
            "text": "In the following we constructively show that in the stochastic disease network game with payoffs in (S1), a degenerate (pure) MMPE strategy profile \u03c3 * := {\u03c3 * i : {0, 1} n \u2192 [0, 1]} that satisfies the above relation in (S2) exists. Note that a degenerate distribution puts weight one on a single action value, that is, the strategy profile corresponds to a single action profile for any state.",
            "cite_spans": [],
            "ref_spans": [],
            "section": ""
        },
        {
            "text": "Since in the stochastic game population response is determined by the current state of the disease only, i.e., equilibrium is stationary, it suffices to show the existence of a pure Nash equilibrium strategy for the stage game with state s \u2208 {0, 1} n . A pure Nash equilibrium (NE) strategy profile \u03c3 * : {0, 1} n \u2192 [0, 1] n of the stage game with payoffs (S1) and state s satisfies",
            "cite_spans": [],
            "ref_spans": [],
            "section": ""
        },
        {
            "text": "for any \u03c3 i \u2208 {0, 1} n \u2192 ([0, 1]). We define the corresponding equilibrium action profile as a * := \u03c3 * (s) for a given state s. For a given individual i with state s i \u2208 {0, 1}, neighbors' state s N i := {s j } j\u2208N i , and neighbor action profile a N i we have the best response of individual i as follows,",
            "cite_spans": [],
            "ref_spans": [],
            "section": ""
        },
        {
            "text": "where 1(\u00b7) is the indicator function. Since the payoffs are linear in self-actions, the actions that maximize the payoffs are in the extremes -a i = 1 or a i = 0 -depending on the states and actions of their neighbors. We can equivalently represent the NE definition in (S3) as a fixed point equation by using the best response definition,",
            "cite_spans": [],
            "ref_spans": [],
            "section": ""
        },
        {
            "text": "In the following we define the concept of strictly dominated action which will be a useful in finding the Nash equilibrium. . Since all individuals have singleton non-dominated action spaces, i.e., N \\ (n(1) n(2)) = / 0, the process ends. Furthermore, the corresponding action profile is a Nash equilibrium of the stage game in (S1) by Lemma 2.",
            "cite_spans": [],
            "ref_spans": [],
            "section": ""
        },
        {
            "text": "If an action a i is strictly dominated then there exists a more preferable action a i for any circumstance. It is clear that if an action is strictly dominated then it cannot be a rational action from (S6). In a game we can iteratively remove the strictly dominated actions, this process is called the iterated elimination of strictly dominated strategies and is defined below.",
            "cite_spans": [],
            "ref_spans": [],
            "section": ""
        },
        {
            "text": "Definition 2 (Iterated elimination of strictly dominated actions) Set the initial set of actions A 0 i = [0, 1] for all i, and for any k \u2208 N set",
            "cite_spans": [],
            "ref_spans": [],
            "section": ""
        },
        {
            "text": "We denote the set of actions of individual i that survive the iterated elimination by A \u221e i := \u221e k=0 A k i . When A \u221e i has a single element, we say A \u221e i is a singleton. We formally state an iterated elimination process for the game in (S1).",
            "cite_spans": [],
            "ref_spans": [],
            "section": ""
        },
        {
            "text": "Definition 3 (An iterated elimination process for the epidemic game) Begin with action spaces A 0 i = [0, 1] for all i \u2208 N . Fix s \u2208 {0, 1} n . Iterate k = 1, 2, . . .",
            "cite_spans": [],
            "ref_spans": [],
            "section": ""
        },
        {
            "text": "If n(k) = / 0 then stop iteration of k.",
            "cite_spans": [],
            "ref_spans": [],
            "section": ""
        },
        {
            "text": "Our next result shows that the process described above eliminates all strictly dominated strategies in finite time.",
            "cite_spans": [],
            "ref_spans": [],
            "section": ""
        },
        {
            "text": "Lemma 1 Consider the process described in Definition 3. There exists an iteration step k \u2264 n such that {A k i } i\u2208N are the set of actions that are not strictly dominated for the game with payoffs (S1) and state s \u2208 {0, 1} n .",
            "cite_spans": [],
            "ref_spans": [],
            "section": ""
        },
        {
            "text": "Proof: First consider the odd iteration steps, k = odd. If the condition inside the bracket in (S9) holds then for any action that individual j \u2208 N i \\ l=even n(l) takes, a i = 1 dominates any other action a i \u2208 [0, 1] \\ {1} by Definition 1. To see this first 2/14 recall the best response of individual i (S5). Note that by the even steps, individuals j \u2208 l=even n(l) have a j = 0 as the only not dominated action. Hence the best response of individual i can be rewritten as",
            "cite_spans": [],
            "ref_spans": [],
            "section": ""
        },
        {
            "text": "where we remove neighbors that only have zero as the not dominated action. If the inequality inside the indicator function is true even when the remaining neighbors of i take action 1, then a i = 1 dominates all the other actions [0, 1) of i, that is, for all a j \u2208 [0, 1] for j \u2208 N \\ i,",
            "cite_spans": [],
            "ref_spans": [],
            "section": ""
        },
        {
            "text": "Assuming the left hand side of the above inequality is one then by the definition of strictly dominated action in Definition 1, a i = 1 is the only action that is not dominated. Next, we consider the even iteration steps, k = even. If the condition inside the bracket in (S10) holds then for any action that individual j \u2208 N i \\ l=odd n(l) takes, a i = 0 dominates a i \u2208 [0, 1] \\ {0} by Definition 1. To see this first recall the best response of individual i (S5). Note that by the odd steps, individuals j \u2208 l=odd n(l) have a j = 1 as the only not dominated action. Hence the best response of individual i can be rewritten as",
            "cite_spans": [],
            "ref_spans": [],
            "section": ""
        },
        {
            "text": "where we separate neighbors j \u2208 N i l=odd n(l) that only have a j = 1 as the not dominated action from the other neighbors j \u2208 N i \\ l=odd n(l). If the inequality inside the indicator function is false even when the remaining neighbors of i take action 0, then a i = 0 dominates all the other actions (0, 1] of i, that is, for any a j \u2208 [0, 1] for j \u2208 N i \\ l=odd n(l)",
            "cite_spans": [],
            "ref_spans": [],
            "section": ""
        },
        {
            "text": "Assuming the left hand side of the above inequality is one then by the definition of strictly dominated action in Definition 1, a i = 0 is the only action that is not dominated. Further if no individual can eliminate a dominated strategy n(k) = / 0 at an iteration k then at following iterations k + 1, k + 2, . . . there won't be any individuals that eliminate any actions as strictly dominated. To see this, assume the opposite is true, that is, n(k + 1) = / 0. Since n(k) = / 0 then the condition in n(k + 1) is identical to the conditions inside (S9) or (S10) for n(k \u2212 1) depending on whether k + 1 is odd or even, respectively. There cannot be an individual that satisfies the conditions at iteration k + 1 because n(k + 1) is selected among individuals that have not been previously in any set, i.e., i \u2208 N \\ k l=1 n(l). This contradicts n(k + 1) = / 0. As a result, at each iteration k the number of individuals in n(k) has to be positive until an iteration step n(k) at which either there is no individual left N \\ k l=1 n(l) = / 0 or there is no individual that satisfies the condition after the colon in (S9) or (S10).",
            "cite_spans": [],
            "ref_spans": [],
            "section": ""
        },
        {
            "text": "Suppose now that the process stops at iteration k, i.e., n(k) = / 0, but there exists an individual i \u2208 N \\ k\u22121 l=1 n(l) with strictly dominated action (S7) a i \u2208 A k i . Suppose k is odd then by (S12), any action a i \u2208 A k i \\ {1} must be dominated by a i = 1. Suppose k is even, then by (S14) any action a i \u2208 A k i \\ {0} must be dominated by a i = 0. Then n(k) = / 0 which is a contradiction. This",
            "cite_spans": [],
            "ref_spans": [],
            "section": ""
        },
        {
            "text": "together with the fact that if n(k) = / 0 then n(l) = / 0 for l > k implies that if the process in Definition 3 stops all the strictly dominated strategies are eliminated.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "3/14"
        },
        {
            "text": "Finally, the fact that the number of elements of n(k) is positive at every iteration except the last iteration in Definition 3 implies that the iteration ends in at most n iterations.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "3/14"
        },
        {
            "text": "The following Lemma shows that once all the strictly dominated strategies are eliminated, the action profile that assigns socialize to susceptible individuals and do not socialize to infected individuals is a pure Nash equilibrium strategy profile.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "3/14"
        },
        {
            "text": "Lemma 2 Consider the game defined by the payoffs in (S1) given state s \u2208 {0, 1} n and the iterated elimination process in Definition 3. Denote the action space of i \u2208 N that is not strictly dominated by",
            "cite_spans": [],
            "ref_spans": [],
            "section": "3/14"
        },
        {
            "text": "is not a singleton then let a * i = 1 for s i = 0 and a * i = 0 for s i = 1. The resulting action profile a * is a pure Nash equilibrium of the game.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "3/14"
        },
        {
            "text": "Proof: We will use the following equivalent definition in (S6) of Nash equilibrium for the stage game in (S1),",
            "cite_spans": [],
            "ref_spans": [],
            "section": "3/14"
        },
        {
            "text": "Note that at the end of the elimination process in Definition 3, the action space of an individual i is either a singleton or is equal to A * i = [0, 1]. A strictly dominated action cannot satisfy the above equation, hence the equilibrium action of an individual with a singleton non-dominated action space is given by a",
            "cite_spans": [],
            "ref_spans": [],
            "section": "3/14"
        },
        {
            "text": "is not a Nash equilibrium action. Then individual i can deviate and by the above equation it must be that a * i = 0 is a Nash equilibrium action because",
            "cite_spans": [],
            "ref_spans": [],
            "section": "3/14"
        },
        {
            "text": "Note that by our assumption a * j = 0 for all the infected individuals (s j = 1) at which 0 is not a strictly dominated action, i.e., when A j = [0, 1]. Hence in the right hand side of the above inequality only individuals that do not have 0 in their not dominated action set will matter. Furthermore, if 0 is a strictly dominated action, socialize action 1 is the only remaining not strictly dominated action. Hence, we can write the above inequality as follows,",
            "cite_spans": [],
            "ref_spans": [],
            "section": "3/14"
        },
        {
            "text": "Now note that if the above inequality is true then 1 should be strictly dominated by action 0 for individual i. Hence, it is a contradiction to the fact that the action space is not a singleton, A * i \u2208 [0, 1]. A similar contradiction argument can be made for the infected individuals (s i = 1) and the equilibrium action a * i = 0. Therefore, the action profile described in the statement must be a pure Nash equilibrium of the game.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "3/14"
        },
        {
            "text": "Lemma 1 shows that the process in Definition 3 eliminates all dominated actions in finite time. Furthermore, if all individuals are included in the process, i.e., if N = n k=1 n(k) then we end up with a singleton action profile that is not strictly dominated. This means the game has an unique pure Nash equilibrium by definition of strictly dominated action. If at the end of the process in Definition 3, if all individuals are not included in the process, i.e., N \\ n k=1 n(k) = / 0 then the set of not strictly dominated actions is not a singleton. Lemma 2 proposes a pure strategy profile that is a Nash equilibrium of the game in (S1) for the case that action spaces of individuals that survive strict elimination process are not all singleton.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "3/14"
        },
        {
            "text": "Lemmas 1 and 2 considered the stage game with payoffs (S1) given state s \u2208 {0, 1} n . An MMPE strategy profile \u03c3 is a mapping from any state to the action space. We can obtain a pure MMPE strategy profile using Lemma 2 for all possible states s \u2208 {0, 1} n . That is, we use the elimination process in Definition 3 and the action assignment given in Lemma 2 for all the states to construct an MMPE strategy profile. In our simulations, in this paper, we construct the MMPE equilibrium strategy profiles following this process -see Fig. S1 for an example.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 530,
                    "end": 537,
                    "text": "Fig. S1",
                    "ref_id": "FIGREF2"
                }
            ],
            "section": "3/14"
        },
        {
            "text": "In this section, we consider the sub-optimality of the decisions of individuals that play according to an MMPE strategy profile. By the definition of MMPE strategy, individuals consider current payoffs and play according Nash equilibrium strategy of that stage game. In the following we show the worst stage game Nash equilibrium strategy can be n fold worse than the optimal action profile given a network and state.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "B Price of Anarchy of the stage game"
        },
        {
            "text": "Define the stage game \u0393(s, G , c) with payoffs in (S1) parametrized by the disease state s, contact network G and utility constants c := {c 0 , c 1 , c 2 }. The welfare value of the action profile a in this game is the sum of utilities of the individuals,",
            "cite_spans": [],
            "ref_spans": [],
            "section": "4/14"
        },
        {
            "text": "(S18)",
            "cite_spans": [],
            "ref_spans": [],
            "section": "4/14"
        },
        {
            "text": "Note that the first summation is over all the individuals and the second summation is over all the edges E in the network G . We define the optimum action profile as the maximizer of the welfare function above, i.e., a opt = argmax a W (a, s(t)). We denote the set of Nash equilibrium action profiles by A * and define the price of anarchy (PoA) as the ratio of the worst possible Nash action profile to the optimum action profile,",
            "cite_spans": [],
            "ref_spans": [],
            "section": "4/14"
        },
        {
            "text": ".",
            "cite_spans": [],
            "ref_spans": [],
            "section": "4/14"
        },
        {
            "text": "(S19)",
            "cite_spans": [],
            "ref_spans": [],
            "section": "4/14"
        },
        {
            "text": "Obviously, PoA \u2264 1. In the following we provide a lower bound on the price of anarchy.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "4/14"
        },
        {
            "text": "Proposition 1 For a game \u0393(s, G , c), the price of anarchy (S19) has the following lower bound",
            "cite_spans": [],
            "ref_spans": [],
            "section": "4/14"
        },
        {
            "text": "where |N i | is the degree connectivity of i.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "4/14"
        },
        {
            "text": "Proof: Consider the derivatives of u i in (S1) and W in (S18) with respect to a i , respectively,",
            "cite_spans": [],
            "ref_spans": [],
            "section": "4/14"
        },
        {
            "text": "First note that \u2202 u i \u2202 a i \u2265 \u2202W \u2202 a i for any s and a \u2212i because",
            "cite_spans": [],
            "ref_spans": [],
            "section": "4/14"
        },
        {
            "text": "Therefore, given actions of others a \u2212i and state s, it could be that \u2202 u i \u2202 a i > 0 while \u2202W \u2202 a i < 0. Consider the optimal and equilibrium action profiles where a opt j = a * j for j \u2208 N \\ i, and a opt i = 0 but a * i = 1. That is, we have \u2202 u j \u2202 a j < 0 and \u2202W \u2202 a j < 0 for all j = i but \u2202 u i \u2202 a i > 0 while \u2202W \u2202 a i < 0. Then, the difference between the equilibrium profile welfare W (a * , s) and optimal action profile welfare W (a opt , s) is equal to (S23), that is,",
            "cite_spans": [],
            "ref_spans": [],
            "section": "4/14"
        },
        {
            "text": "We divide the difference above by W (a opt , s) to get",
            "cite_spans": [],
            "ref_spans": [],
            "section": "4/14"
        },
        {
            "text": "A trivial lower bound on the term for the numerator above is \u2212 max(c 1 , c 2 )|N i | for any state s. Furthermore a trivial upper bound for W (a opt , s) is nc 0 . Using these bounds we obtain a bound for the above equality, Now note that the choice of the individual i is arbitrary. When this individual is the individual with the largest number of neighbors we get a lower bound on the right hand side of the above inequality which is the worst case lower bound on the price of anarchy in (S20). Note that this upper bound can be arbitrarily bad, i.e., in the order of 1/n. In the discussion following Figure 2 (d) in the main text we show that this bound is tight. We repeat this example below. The first Nash equilibrium is also the optimal action profile a opt yielding a welfare of (n \u2212 1)c 0 . The second Nash equilibrium a * obtains a welfare of c 0 resulting in a PoA of 1 n\u22121 . Another example on a star network follows.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 604,
                    "end": 612,
                    "text": "Figure 2",
                    "ref_id": null
                }
            ],
            "section": "4/14"
        },
        {
            "text": "Example 2 Consider the star network shown in Figure S2 . The center individual 1 has n \u2212 1 neighbors and is the only infected individual. Constants are such that c 0 = c 1 + \u03c1 1 and c 0 = (n \u2212 1)c 2 + \u03c1 2 for arbitrarily small positive constants \u03c1 1 > 0 and \u03c1 2 > 0, so that c 0 < (n \u2212 1)(c 1 + c 2 ). The unique NE action profile is that all individuals are social, a * i = 1 for all i \u2208 N . The welfare for the NE is W (a * ) = nc 0 \u2212 (n \u2212 1)(c 1 + c 2 ) = (n \u2212 1)\u03c1 1 + \u03c1 2 . The optimal action profile is that only the susceptible nodes are social a opt i = 1 for i = 1 and a opt 1 = 0. The welfare for the a opt is W (a opt ) = (n \u2212 1)c 0 . Then PoA \u2248 \u03c1 1 /c 0 for large n.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 45,
                    "end": 54,
                    "text": "Figure S2",
                    "ref_id": "FIGREF1"
                }
            ],
            "section": "4/14"
        },
        {
            "text": "Here, we see that PoA is determined by the closeness of c 1 to c 0 . Note that in this example, equilibrium is unique and, unlike Example 1, the optimal action profile is not a Nash equilibrium.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "4/14"
        },
        {
            "text": "Another notion that enables us to gauge the optimality of equilibria is the Price of Stability (PoS). PoS is the ratio of the best possible Nash action profile to the optimum action profile,",
            "cite_spans": [],
            "ref_spans": [],
            "section": "4/14"
        },
        {
            "text": "Note that PoS \u2264 1 by definition of a opt . We remark that in Example 2, the equilibrium is unique, meaning PoS = PoA. Therefore, not only the PoA but also the PoS can be arbitrarily bad.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "4/14"
        },
        {
            "text": "We formally define the reproductive ratio R 0 in the following.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "C R 0 bound"
        },
        {
            "text": "Definition 4 Let the initial state of the population be given by s(1) where s i (1) = 1, otherwise s j (1) = 0 for all j = i for some randomly selected individual i. Then R 0 is the expected number of individuals that contract the disease from the randomly selected individual i until i heals,",
            "cite_spans": [],
            "ref_spans": [],
            "section": "6/14"
        },
        {
            "text": "where 1(s j (t + 1) \u2212 s j (t) = 1, i \u2192 j) is the indicator function that is one if individual j transitions to an infected state at time t and i is the one infecting j, and 1 (s i (l) = 1 for l < t) is the indicator function that is one if individual i has not healed yet. The outside expectation is with respect to the uniform distribution that selects the initial infected individual, and the inside expectation is with respect to the transition probabilities of the Markov chain.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "6/14"
        },
        {
            "text": "In the following, we derive bounds for R 0 given that individuals act according to an MMPE strategy profile and we select the initial infected individual randomly from the network.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "6/14"
        },
        {
            "text": "Theorem 1 Consider a network with degree distribution P(k). Assume the infected individual is chosen from the population uniformly at random. Scale c 0 with \u03b2 for convenience, i.e., c 0 := \u03b2 c 0 , and assume c 0 > c 1 .Assume c 0 > c 1 . Then R 0 defined in (S29) has the following upper bound,",
            "cite_spans": [],
            "ref_spans": [],
            "section": "6/14"
        },
        {
            "text": "where K := min( c 0 /c 2 , n).",
            "cite_spans": [],
            "ref_spans": [],
            "section": "6/14"
        },
        {
            "text": "Proof: We start by moving the second expectation inside the sum in the definition of R 0 (S29) to get the following,",
            "cite_spans": [],
            "ref_spans": [],
            "section": "6/14"
        },
        {
            "text": "Now consider the conditional expectation inside the summation which we can equivalently represent as the following conditional probability E[1(s j (t + 1) \u2212 s j (t) = 1, i \u2192 j)1 (s i (l) = 1 for l < t) s(1)]",
            "cite_spans": [],
            "ref_spans": [],
            "section": "6/14"
        },
        {
            "text": "= P s j (t + 1) \u2212 s j (t) = 1, i \u2192 j, s i (l) = 1 for l < t s(1) .",
            "cite_spans": [],
            "ref_spans": [],
            "section": "6/14"
        },
        {
            "text": "Note that the above conditional probability is the probability that j is infected by i at time t and i remained infected until time t given i is infected at t = 0. Using the chain rule and law of total probability, we can write the above conditional probability as follows P(s j (t + 1) \u2212 s j (t) = 1, i \u2192 j, s i (l) = 1 for l < t s(1)) = P s j (t + 1) \u2212 s j (t) = 1, i \u2192 j s i (t) = 1, s j (t) = 0, s(1) P(s i (t) = 1, s j (t) = 0 s i (l) = 1 for l < t, s(1))",
            "cite_spans": [],
            "ref_spans": [],
            "section": "(S32)"
        },
        {
            "text": "Note that the first four lines on the right hand side equal to the probability that i infects j at time t given that i remained infected until t \u2212 1 by law of total probability. The last line is the probability that i remained infected given that i started infected since s i (1) = 1. Observe that the probability that i infects j is zero if individual i is susceptible at time t or individual j is infected, i.e., P s j (t + 1) \u2212 s j (t) = 1, i \u2192 j s i (t) = 0 = 0 or P s j (t + 1) \u2212 s j (t) = 1, i \u2192 j s j (t) = 1 = 0. Hence only the first line of the four line expression is nonzero which simplifies the identity above as follows P(s j (t + 1) \u2212 s j (t) = 1, i \u2192 j, s i (l) = 1 for l < t s(1)) = P s j (t + 1) \u2212 s j (t) = 1, i \u2192 j s i (t) = 1, s j (t) = 0, s(1) P(s i (t) = 1, s j (t) = 0 s i (l) = 1 for l < t, s(1)) P s i (l) = 1 for l < t s(1) (S34)",
            "cite_spans": [],
            "ref_spans": [],
            "section": "(S32)"
        },
        {
            "text": "The probability of healing at each step is independent, hence the last conditional probability is equal to (1 \u2212 \u03b4 ) t\u22121 . The conditional probability that i remains infected and j is infected at time t given that i is infected until time t is less than 1 \u2212 \u03b4 by the argument below, P(s i (t) = 1, s j (t) = 0 s i (l) = 1 for l < t, s(1)) = P(s j (t) = 0 s i (t) = 1, s i (l) = 1 for l < t, s(1))P(s i (t) = 1 s i (l) = 1 for l < t, s(1)) (S35)",
            "cite_spans": [],
            "ref_spans": [],
            "section": "7/14"
        },
        {
            "text": "(S37)",
            "cite_spans": [],
            "ref_spans": [],
            "section": "7/14"
        },
        {
            "text": "The first equality above follows by chain rule. The inequality follows by the fact that the probability is less than one. The second equality is true by the transition probability of individual i from an infected state. Substituting these identities inside the equation (S34) we obtain the following",
            "cite_spans": [],
            "ref_spans": [],
            "section": "7/14"
        },
        {
            "text": "Now consider the conditional probability on the right hand side of (S38), the probability that j is infected at time t + 1 by i given that j is susceptible and i is infected at time t. We have the following upper bound,",
            "cite_spans": [],
            "ref_spans": [],
            "section": "7/14"
        },
        {
            "text": "The first equality is by the fact that s j (t + 1) = 1 if s j (t) = 0 and s j (t + 1) \u2212 s j (t) = 1. The second equality above is by the law of total probability and by the fact that if i or j takes an action to self-isolate, i.e., a j (t) = 0 or a i (t) = 0, then i cannot infect j. The inequality follows by the fact that P a i (t) = 1, a j (t) = 1 s i (t) = 1, s j (t) = 0 \u2264 P a i (t) = 1 s i (t) = 1, s j (t) = 0 . The last equality follows because if both agents socialize at normal levels then the infection probability is \u03b2 when agent j and i are connected, j \u2208 N i . Now we consider a * i (t). We know that the MMPE action of individual i is a best response to best response actions of neighbors from (S6) where best response function is given by (S5). Specifically from the perspective of an infected individual i, the best response action is given by the following",
            "cite_spans": [],
            "ref_spans": [],
            "section": "7/14"
        },
        {
            "text": ". Also note that if a * i (1) = 1 then a * i (t) = 1 for all t > 1. Moreover, if a * i (1) = 0 then i never infects another node if it is the only initially infected individual. Hence, we have a * i (t) = a * i (1) for all t. That is, the action of agent i at time t is determined by the initial action and is independent of the state at time t. Therefore, we can write",
            "cite_spans": [],
            "ref_spans": [],
            "section": "7/14"
        },
        {
            "text": "Substituting the above identity in (S39) and using (S39) in (S38), which then we substitute to (S32), we get the following upper bound for R 0 in (S31),",
            "cite_spans": [],
            "ref_spans": [],
            "section": "7/14"
        },
        {
            "text": "Now note the indicator function terms depend on the network of individual i but they do not depend on the identities of the neighbors. In fact, \u2211 n j=1 1( j \u2208 N i ) is equal to the number of neighbors of i, i.e., |N i |. In addition, we have \u2211 \u221e t=1 (1 \u2212 \u03b4 ) t = 1/\u03b4 . Using these identities, we can write the above bound as follows,",
            "cite_spans": [],
            "ref_spans": [],
            "section": "8/14"
        },
        {
            "text": "Individual i is chosen uniformly random and it has k neighbors with probability P(k), therefore, the expectation on the right hand side is equal to the following,",
            "cite_spans": [],
            "ref_spans": [],
            "section": "8/14"
        },
        {
            "text": "Bound in (S30) follows.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "8/14"
        },
        {
            "text": "Corollary 1 Consider a random scale free network where the degree distribution follows P(k) \u223c k \u2212\u03b3 for \u03b3 = 2. Then R 0 defined in (S29) has the following upper bound,",
            "cite_spans": [],
            "ref_spans": [],
            "section": "D R 0 bound scale-free"
        },
        {
            "text": "Proof: We directly use the bound in (S30) and substitute in P(k) = L(\u22122, n)k \u22122 where L(\u22122, n) = (\u2211 n k=1 k \u22122 ) \u22121 is the normalization constant for the scale-free distribution",
            "cite_spans": [],
            "ref_spans": [],
            "section": "D R 0 bound scale-free"
        },
        {
            "text": "We first upper bound the normalization constant L(\u22122, n) by the fact that \u2211 n k=1 k \u22122 > 2 \u2212 1 n . Hence, L(\u22122, n) \u2264 n 2n\u22121 . Next we note that the summation behaves logarithmically which yields the desired bound in (S45).",
            "cite_spans": [],
            "ref_spans": [],
            "section": "D R 0 bound scale-free"
        },
        {
            "text": "We formally define the reproductive ratio R * in the following.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "E R * bound"
        },
        {
            "text": "Definition 5 Let the initial state of the population be given by s(1) where s i (1) = 1, otherwise s j (1) = 0 for all j = i for some randomly selected individual i. The probability that we select an initial sick individual with degree k is given by Q(k) := kP(k) \u2211 k kP(k) . Then R * is the expected number of individuals that contract the disease from a randomly selected individual i until i heals,",
            "cite_spans": [],
            "ref_spans": [],
            "section": "E R * bound"
        },
        {
            "text": "is the indicator function that is one if individual j transitions to an infected state at time t and i is the one infecting j, and 1 (s i (l) = 1 for l < t) is the indicator function that is one if individual i has not healed yet. The outside expectation is with respect to the probability distribution Q(k), and the inside expectation is with respect to the transition probabilities of the Markov chain.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "E R * bound"
        },
        {
            "text": "Below we present a bound for R * for a generic network. Proof: First note that the only difference between the R 0 definition and R * is the difference in probability distributions of selecting the initial infected individual. Hence, the bound in (S43) applies to R * which we repeat here for convenience,",
            "cite_spans": [],
            "ref_spans": [],
            "section": "E R * bound"
        },
        {
            "text": "where now the expectation is with respect to the distribution Q(k). Therefore, the expectation on the right hand side is equal to the following,",
            "cite_spans": [],
            "ref_spans": [],
            "section": "E R * bound"
        },
        {
            "text": "Bound in (S48) follows by using the definition of Q(k) = kP(k) \u2211 k kP(k) .",
            "cite_spans": [],
            "ref_spans": [],
            "section": "E R * bound"
        },
        {
            "text": "Corollary 2 Consider a random scale free network where the degree distribution follows P(k) \u223c k \u2212\u03b3 for \u03b3 = 2. Then R * given by Definition 5 has the following upper bound,",
            "cite_spans": [],
            "ref_spans": [],
            "section": "F R * bound scale-free"
        },
        {
            "text": "Proof: We directly use the bound in (S48) and substitute in P(k) = L(\u22122, n)k \u22122 where L(\u22122, n) = (\u2211 n k=1 k \u22122 ) \u22121 is the normalization constant for the scale-free distribution",
            "cite_spans": [],
            "ref_spans": [],
            "section": "F R * bound scale-free"
        },
        {
            "text": "The bound in (S51) follows by noting that \u2211 n l=1 l \u22121 \u2248 log(n).",
            "cite_spans": [],
            "ref_spans": [],
            "section": "F R * bound scale-free"
        },
        {
            "text": "See Figure S3 for the effect of parameters c 1 , c 2 , \u03b2 on eradication time.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 4,
                    "end": 13,
                    "text": "Figure S3",
                    "ref_id": null
                }
            ],
            "section": "G Mean eradication time"
        },
        {
            "text": "See Figure S4 for the effect of parameters c 1 , c 2 , \u03b2 on infectivity level. ",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 4,
                    "end": 13,
                    "text": "Figure S4",
                    "ref_id": "FIGREF4"
                }
            ],
            "section": "H Average infectivity levels"
        },
        {
            "text": "In the preferential attachment model, used in Figures 4-6 of the main text, a node is added at each step and is connected to an existing node selected randomly proportional to their degree-see the Barabasi-Albert algorithm 1 . A modification to this algorithm is when each added node is connected to two existing nodes selected randomly proportional to their degree. This modified algorithm generates scale-free networks with an expected scaling degree of \u03b3 = 2.3 while the non-modified preferential attachment algorithm generates an expected scaling degree of \u03b3 = 2.",
            "cite_spans": [
                {
                    "start": 223,
                    "end": 224,
                    "text": "1",
                    "ref_id": "BIBREF0"
                }
            ],
            "ref_spans": [
                {
                    "start": 46,
                    "end": 57,
                    "text": "Figures 4-6",
                    "ref_id": null
                }
            ],
            "section": "I Critical empathy thresholds for modified scale-free networks"
        },
        {
            "text": "In Figure S5 , we test the accuracy of the bounds for R 0 and R * in equations (S30) and (S48) (equations (1) and (4) in the main text) for scale-free networks generated according to the modified preferential attachment algorithm. Using these inequalities and substituting for P(k) = L(\u22122.3, n)k \u22122.3 where L(\u22122.3, n) is a normalizing constant, we solve for empathy constant values c 2 that make the right hand side of equations (1) and (4) less than one -see the caption of Fig. S5 for the critical empathy values. We note that the closed form critical thresholds of the empathy constant c 2 in equations (S45) and (S51) (equations (3) and (6) of main text) are obtained when the network is scale-free with scaling degree \u03b3 = 2. The critical empathy values obtained for \u03b3 = 2.3 are greater than the critical empathy values when \u03b3 = 2. Numerical experiments show that the critical empathy constants that make R * < 1 are still good indicators of fast disease eradication.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 3,
                    "end": 12,
                    "text": "Figure S5",
                    "ref_id": "FIGREF7"
                },
                {
                    "start": 475,
                    "end": 482,
                    "text": "Fig. S5",
                    "ref_id": "FIGREF7"
                }
            ],
            "section": "I Critical empathy thresholds for modified scale-free networks"
        },
        {
            "text": "We assess the robustness of the critical empathy threshold to initial conditions by considering initial infectivity levels of {5%, 20%, 50%} in Figure S6 . Besides the difference in initial conditions, the numerical setup is identical to Figure 6 in main text. The numerical experiments confirm that the critical empathy threshold for R * in (S51) is a good indicator for disease eradication. There are only a few runs which fail to eradicate the disease within 200 time steps when the critical empathy for R * < 1 is exceeded -refer to the caption of Figure S6 (S53)",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 144,
                    "end": 153,
                    "text": "Figure S6",
                    "ref_id": "FIGREF8"
                },
                {
                    "start": 238,
                    "end": 246,
                    "text": "Figure 6",
                    "ref_id": null
                },
                {
                    "start": 552,
                    "end": 561,
                    "text": "Figure S6",
                    "ref_id": "FIGREF8"
                }
            ],
            "section": "J Sensitivity to initial level of infectivity"
        },
        {
            "text": "That is, the infected will socialize normally at all times and a susceptible agent will socialize if the number infected around is less than c 0 c 1 . Given the MMPE profile above, the infection probability of a susceptible individual i is given by . The rate of disease eradication is above 95% among 50 trials when the value of the empathy constant is above the threshold that makes R * < 1. That is, for c 2 that make R * < 1 there are only a few trials that fail to eradicate the disease within 200 steps which are observed when \u03b2 = 0.1 and 20% initially infected, and when \u03b2 = {0.2, 0.3}.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "J Sensitivity to initial level of infectivity"
        }
    ],
    "bib_entries": {
        "BIBREF0": {
            "ref_id": "b0",
            "title": "Emergence of scaling in random networks",
            "authors": [
                {
                    "first": "A.-L",
                    "middle": [],
                    "last": "Barab\u00e1si",
                    "suffix": ""
                },
                {
                    "first": "R",
                    "middle": [],
                    "last": "Albert",
                    "suffix": ""
                }
            ],
            "year": 1999,
            "venue": "Science",
            "volume": "286",
            "issn": "",
            "pages": "509--512",
            "other_ids": {}
        },
        "BIBREF1": {
            "ref_id": "b1",
            "title": "Epidemic spread over networks with agent awareness and social distancing",
            "authors": [
                {
                    "first": "K",
                    "middle": [],
                    "last": "Paarporn",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [],
                    "last": "Eksin",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [
                        "S"
                    ],
                    "last": "Weitz",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [
                        "S"
                    ],
                    "last": "Shamma",
                    "suffix": ""
                }
            ],
            "year": 2015,
            "venue": "Proceedings of the 53rd Annual Allerton Conference on Communications, Control, and Computing",
            "volume": "",
            "issn": "",
            "pages": "51--57",
            "other_ids": {}
        },
        "BIBREF2": {
            "ref_id": "b2",
            "title": "Virus spread in networks",
            "authors": [
                {
                    "first": "P",
                    "middle": [],
                    "last": "Van Mieghem",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Omic",
                    "suffix": ""
                },
                {
                    "first": "R",
                    "middle": [],
                    "last": "Kooij",
                    "suffix": ""
                }
            ],
            "year": 2009,
            "venue": "IEEE/ACM Transactions on Networking",
            "volume": "17",
            "issn": "",
            "pages": "1--14",
            "other_ids": {
                "DOI": [
                    "10.1109/TNET.2008.925623"
                ]
            }
        }
    },
    "ref_entries": {
        "FIGREF0": {
            "text": "Strictly dominated action) For a given state s \u2208 {0, 1} n , an action a i \u2208 [0, 1] is strictly dominated if and only if there exists an action a i \u2208 [0, 1] such thatu i (a i , a N i , s) > u i (a i , a N i , s) \u2200a N i(S7) The iterated elimination process in Definition 3 on a 5 individual network. The payoff constants are such that 2c 1 > c 0 > c 1 and c 0 > 2c 2 . Individuals 1 and 2 are sick (s 1 = 1, s 2 = 1) and the rest are healthy. The contact network is shown at k = 0. At step k = 1, individuals in n(1) = {1, 2, 4, 5} eliminate all actions except 1 by (S9) given c 0 > c 1 and c 0 > 2c 2 . Individual 3 cannot eliminate any action from the space. At step k = 2, individual 3 eliminates all actions except 0 by (S10)",
            "latex": null,
            "type": "figure"
        },
        "FIGREF1": {
            "text": "Price of anarchy in the star network (Example 1). We consider the strong empathy (c 0 < (n \u2212 1)c 2 ) and strong averseness (c 0 < c 1 ) case inFigure 2(d) of the main manuscript. Left and right figures show the two stage game pure Nash equilibria. The Nash equilibrium action profile shown by the right figure is optimal with welfare equal to (n \u2212 1)c 0 . The Nash equilibrium action profile shown by the left figure attains a welfare equal to c 0 . The PoA is equal to 1/n \u2212 1.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF2": {
            "text": "Consider a star network with n individuals. The center individual with n \u2212 1 neighbors is the only infected individual. Payoff constants are such that c 0 < c 1 and c 0 < (n \u2212 1)c 2 . There are two stage game Nash equilibria: 1) a opt i = 1 if s i = 0 and a opt i = 0 if s i = 1, and 2) a * i = 0 if s i = 0 and a * i = 1 if s i = 1.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF3": {
            "text": "Consider a network with degree connectivity distribution P(k). Assume c 0 > c 1 . Then R * defined in Definition 5 has the following upper bound, R * \u2264 \u03b2 \u03b4 min( c 0 /c 2 ,n) Effect of risk averseness c 1 and empathy c 2 constants on mean eradication time. We fix the healing rate to \u03b4 = 0.2 and the population size to n = 100. The infection rate \u03b2 values equal to 0.1,0.2, and 0.3 for figures left, middle, and right, respectively. We let c 0 = 1 for each figure. The axes in the figures correspond to the constant values of c 1 and c 2 . For a given value of c 1 and c 2 , we generate 50 scale-free networks using the preferential attachment algorithm and run the stochastic disease network game for 200 steps for each network. Each run starts with all individuals infected. The grid color represents the average eradication time among runs where we let eradication time be equal to 200 if the disease is not eradicated. The eradication time decreases as c 1 or c 2 increases in the region where disease is eradicated fast, i.e., R * > 1.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF4": {
            "text": "Effect of risk averseness c 1 and empathy c 2 constants on average infectivity level. We fix the healing rate to \u03b4 = 0.2 and the population size to n = 100. The infection rate \u03b2 values equal to 0.1,0.2, and 0.3 for figures left, middle, and right, respectively. We let c 0 = 1 for each figure. The axes in the figures correspond to the constant values of c 1 and c 2 . For a given value of c 1 and c 2 , we generate 50 scale-free networks using the preferential attachment algorithm and run the stochastic disease network game for 200 steps for each network. Each run starts with all individuals infected. The grid color represents the average of infectivity level at time t = 200 among runs. Increasing c 1 reduces the average number of infected in the population whether the disease is endemic or not.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF5": {
            "text": "for details. K Epidemic threshold when c 2 = 0 If c 2 = 0, an MMPE action profile at time t can simply be written as follows, a * i (t) = 1 c 0 > c 1 (1 \u2212 s i (t)) \u2211",
            "latex": null,
            "type": "figure"
        },
        "FIGREF7": {
            "text": "(t) = P(s i (t + 1) = 1|s i (t) = 0) = 1 \u2212 \u220f j\u2208N i (1 \u2212 \u03b2 a * i (t))s j (t).(S54) Effect of risk averseness c 1 and empathy c 2 constants on disease eradication with scale-free networks with higher scaling degree. We consider n = 100 individuals, and let \u03b4 = 0.2, and c 0 = 1. The scale-free network is generated according to the modified preferential attachment algorithm resulting in a scale-free network with average scaling degree of \u03b3 = 2.3. The average node degree is 3.94. The critical values of c 2 {0.04, 0.35, 1.01} that make R 0 < 1 for \u03b2 = {0.1, 0.2, 0.3} calculated using equation (S30) are marked with white dotted dashed lines. The critical values of the empathy constant, c 2 = {0.12, 0.27, 0.35} (marked with red solid lines), that make R * < 1 for \u03b2 = {0.1, 0.2, 0.3} calculated using equation (S48) are still accurate in determining fast eradication.",
            "latex": null,
            "type": "figure"
        },
        "FIGREF8": {
            "text": "Effect of risk averseness c 1 and empathy c 2 constants on disease eradication when initial number of infected is 5% (top), 20% (middle), and 50% (bottom) of the population. We consider n = 100 individuals, and let \u03b4 = 0.2, and c 0 = 1. For a given value of c 1 and c 2 , we generate 50 scale-free networks using the preferential attachment algorithm and run the stochastic disease network game for 200 steps for each network. The grid color represents the ratio of runs in which disease is eradicated within 200 steps. For figures left, middle, and right the eradication of threshold \u03b2 \u03bb max (A)/\u03b4 is equal to 2.65, 5.3, and 8, respectively. The critical values of c 2 {0.02, 0.16, 0.36} that make R 0 < 1 for \u03b2 = {0.1, 0.2, 0.3} calculated using equation (S45) are marked with white dotted dashed lines. The critical values of the empathy constant, c 2 = {0.11, 0.22, 0.33}, that make R * < 1 for \u03b2 = {0.1, 0.2, 0.3} calculated using equation (S51) are accurate in determining fast eradication for any value of c 1 (marked with red solid lines)",
            "latex": null,
            "type": "figure"
        }
    },
    "back_matter": [
        {
            "text": "We can bound the above conditional probability as follows by using the relation 1 \u2212 \u220f j\u2208N i (1 \u2212 \u03b2 a i a j s j (t)) \u2264 \u03b2 a i \u2211 j\u2208N i a j s j (t),Next we obtain a n-state approximation of the Markov chain dynamics -see for a similar approximation 2 . Define the probability of infection of individual i at time t p i (t) = E[s i (t)]. Let p(t) \u2208 [0, 1] n be n infection probability vector of the population. By law of total probability,Using the upper bound above for p i 01 (t) and noting that aWe now approximate the above bound by replacing s j (t) terms with p j (t) to getWhen we linearize the dynamics of p(t) defined by (S58) around the fixed point origin 0 n , i.e., the state of disease eradication p i (t) = 0 for all i, we get the following,where I n is the n \u00d7 n identity matrix and A \u2208 [0, 1] n\u00d7n is the adjacency matrix of the contact network, i.e., its i jth elementThe approximate disease dynamics in (S59) is linear. Furthermore, the origin 0 n is a globally stable fixed point if and only if \u03bb max ((1 \u2212 \u03b4 )I n + \u03b2 A) < 1 3 . We can equivalently write this condition as \u03b2 \u03b4 \u03bb max (A) < 1. This result implies that the state of disease eradication is unstable if \u03b2 \u03b4 \u03bb max (A) > 1. Note that this threshold does not depend on the risk averseness constant c 1 . This implies that risk aversion cannot by stop an outbreak when c 2 = 0, except in the extreme case of c 0 < c 1 .",
            "cite_spans": [],
            "ref_spans": [],
            "section": "annex"
        }
    ]
}