{
    "paper_id": "15a1cb9e790f884e0513e9f0112a909e95a96c67",
    "metadata": {
        "title": "Supplementary Material for: Avoidable errors in the modeling of outbreaks of emerging pathogens, with special reference to Ebola Appendix A. Simulation study",
        "authors": [
            {
                "first": "Aaron",
                "middle": [
                    "A"
                ],
                "last": "King",
                "suffix": "",
                "affiliation": {
                    "laboratory": "",
                    "institution": "University of Michigan",
                    "location": {
                        "settlement": "Ann Arbor",
                        "region": "Michigan",
                        "country": "USA"
                    }
                },
                "email": ""
            },
            {
                "first": "Matthieu",
                "middle": [],
                "last": "Domenech De Cell\u00e8s",
                "suffix": "",
                "affiliation": {
                    "laboratory": "",
                    "institution": "University of Michigan",
                    "location": {
                        "settlement": "Ann Arbor",
                        "region": "Michigan",
                        "country": "USA"
                    }
                },
                "email": ""
            },
            {
                "first": "Felicia",
                "middle": [
                    "M G"
                ],
                "last": "Magpantay",
                "suffix": "",
                "affiliation": {
                    "laboratory": "",
                    "institution": "University of Michigan",
                    "location": {
                        "settlement": "Ann Arbor",
                        "region": "Michigan",
                        "country": "USA"
                    }
                },
                "email": ""
            },
            {
                "first": "Pejman",
                "middle": [],
                "last": "Rohani",
                "suffix": "",
                "affiliation": {
                    "laboratory": "",
                    "institution": "University of Michigan",
                    "location": {
                        "settlement": "Ann Arbor",
                        "region": "Michigan",
                        "country": "USA"
                    }
                },
                "email": ""
            }
        ]
    },
    "abstract": [
        {
            "text": "To demonstrate the differences between fitting to raw incidence vs. cumulative incidence data, we performed a simulation study in which we fit the deterministic model variant to both types of data at three different levels of observation overdispersion: k \u2208 {0, 0.2, 0.5}. For each overdispersion treatment, 500 simulated 39-week time series were generated from the stochastic model variant. The basic reproduction number was set to R 0 = 1.4; the incubation and infectious periods were fixed as in Table B1 ; the assumed population size was taken to be that of the Republic of Guinea. We assumed a reporting probability of \u03c1 = 0.2 and that, at outbreak initiation, 10 individuals were infected. This set of parameter values yields a sample mean simulation visually comparable to the WHO data from Guinea, which display initally slow growth in the number of cases and later acceleration.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 499,
                    "end": 507,
                    "text": "Table B1",
                    "ref_id": null
                }
            ],
            "section": "Abstract"
        },
        {
            "text": "For each simulated data set, we estimated the basic reproduction number, R 0 , the reporting probability, \u03c1, and the negative binomial overdispersion parameter, k. All other model parameters were fixed at their true values. Parameter estimation was accomplished using the trajectory matching algorithm (traj.match) from the R package pomp (King et al., 2010). We constructed likelihood profiles over R 0 and, from these, obtained maximum likelihood point estimates and likelihood-ratio confidence intervals. The full process of obtaining likelihood profiles on model parameters by trajectory matching took approximately 1.2 hr on a 40-cpu cluster.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Abstract"
        },
        {
            "text": "A second simulation study was performed, in which the deterministic variant of the model was fit to cumulative incidence data by ordinary least squares. This common procedure in effect assumes that measurement errors are independent and identically normally distributed. Results of this exercise are shown in Fig. A2 in a form comparable to that of Fig. 1 . As in the results shown in the main text, confidence interval widths are erroneously under-estimated with the result that achieved coverage is far smaller than its nominal value.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 309,
                    "end": 316,
                    "text": "Fig. A2",
                    "ref_id": null
                },
                {
                    "start": 349,
                    "end": 355,
                    "text": "Fig. 1",
                    "ref_id": null
                }
            ],
            "section": "Abstract"
        },
        {
            "text": "1 Figure A1 : Twelve randomly-selected simulated datasets from the 1500 used in the simulation study. Four simulations are shown for each of three values of the negative binomial overdispersion parameter, k.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 2,
                    "end": 11,
                    "text": "Figure A1",
                    "ref_id": null
                }
            ],
            "section": "Abstract"
        },
        {
            "text": "2 Figure A2 : Results from simulation study fitting the deterministic model to cumulative incidence data using the method of least squares. The model was fit to both raw (blue) and accumulated (red) simulated incidence data. The same 1500 simulated data sets of length 39 wk used in Fig. 1 were used here. (A) Estimates of R 0 . True value used in generating the data is shown by the dashed line. (B) Estimates of reporting probability, \u03c1. The dashed line shows the value used to generate the data. (C) Widths of nominal 99% profile likelihood confidence intervals (CI) for R 0 . (D) Actual coverage of the CI, i.e., probability that the true value of R 0 lay within the CI. Ideally, actual coverage would agree with nominal coverage (99%, dashed line). 3",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 2,
                    "end": 11,
                    "text": "Figure A2",
                    "ref_id": null
                },
                {
                    "start": 283,
                    "end": 289,
                    "text": "Fig. 1",
                    "ref_id": null
                }
            ],
            "section": "Abstract"
        },
        {
            "text": "Model parameters were initially estimated using trajectory matching. As in the simulation study, we initially fitted R 0 , \u03c1, k and the initial conditions. However, profile likelihoods over \u03c1 were flat, indicating a lack of identifiability in the reporting rate due to a trade-off between this parameter and initial conditions. Accordingly, we fixed \u03c1 = 0.2. The flatness of the likelihood profiles indicates that this assumption has no effect on the quality of fit. All other model parameters were fixed at the known values given in Table B1 .",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 534,
                    "end": 542,
                    "text": "Table B1",
                    "ref_id": null
                }
            ],
            "section": "Abstract"
        },
        {
            "text": "Trajectory matching was used to compute likelihood profiles over R 0 and k. For each point in the profile, the other parameters and initial conditions were initialized at 40 points according to a latin hypersquare (Sobol') design. In all, the trajectory matching calculations required approximately 21 cpu hr of computation. Full details of the trajectory matching codes are provided in the Supplementary Material. Table B1 : Model parameters, with their interpretations, and their assumed values (parameters estimated from incidence data are so indicated) together with the source of evidence for the assumption.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 415,
                    "end": 423,
                    "text": "Table B1",
                    "ref_id": null
                }
            ],
            "section": "Abstract"
        }
    ],
    "body_text": [
        {
            "text": "To demonstrate the differences between fitting to raw incidence vs. cumulative incidence data, we performed a simulation study in which we fit the deterministic model variant to both types of data at three different levels of observation overdispersion: k \u2208 {0, 0.2, 0.5}. For each overdispersion treatment, 500 simulated 39-week time series were generated from the stochastic model variant. The basic reproduction number was set to R 0 = 1.4; the incubation and infectious periods were fixed as in Table B1 ; the assumed population size was taken to be that of the Republic of Guinea. We assumed a reporting probability of \u03c1 = 0.2 and that, at outbreak initiation, 10 individuals were infected. This set of parameter values yields a sample mean simulation visually comparable to the WHO data from Guinea, which display initally slow growth in the number of cases and later acceleration.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 499,
                    "end": 507,
                    "text": "Table B1",
                    "ref_id": "TABREF0"
                }
            ],
            "section": "Appendix A. Simulation study"
        },
        {
            "text": "For each simulated data set, we estimated the basic reproduction number, R 0 , the reporting probability, \u03c1, and the negative binomial overdispersion parameter, k. All other model parameters were fixed at their true values. Parameter estimation was accomplished using the trajectory matching algorithm (traj.match) from the R package pomp (King et al., 2010) . We constructed likelihood profiles over R 0 and, from these, obtained maximum likelihood point estimates and likelihood-ratio confidence intervals. The full process of obtaining likelihood profiles on model parameters by trajectory matching took approximately 1.2 hr on a 40-cpu cluster.",
            "cite_spans": [
                {
                    "start": 339,
                    "end": 358,
                    "text": "(King et al., 2010)",
                    "ref_id": "BIBREF16"
                }
            ],
            "ref_spans": [],
            "section": "Appendix A. Simulation study"
        },
        {
            "text": "A second simulation study was performed, in which the deterministic variant of the model was fit to cumulative incidence data by ordinary least squares. This common procedure in effect assumes that measurement errors are independent and identically normally distributed. Results of this exercise are shown in Fig. A2 in a form comparable to that of Fig. 1 . As in the results shown in the main text, confidence interval widths are erroneously under-estimated with the result that achieved coverage is far smaller than its nominal value. Figure A1 : Twelve randomly-selected simulated datasets from the 1500 used in the simulation study. Four simulations are shown for each of three values of the negative binomial overdispersion parameter, k. Figure A2 : Results from simulation study fitting the deterministic model to cumulative incidence data using the method of least squares. The model was fit to both raw (blue) and accumulated (red) simulated incidence data. The same 1500 simulated data sets of length 39 wk used in Fig. 1 were used here. (A) Estimates of R 0 . True value used in generating the data is shown by the dashed line. (B) Estimates of reporting probability, \u03c1. The dashed line shows the value used to generate the data. (C) Widths of nominal 99% profile likelihood confidence intervals (CI) for R 0 . (D) Actual coverage of the CI, i.e., probability that the true value of R 0 lay within the CI. Ideally, actual coverage would agree with nominal coverage (99%, dashed line).",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 309,
                    "end": 316,
                    "text": "Fig. A2",
                    "ref_id": null
                },
                {
                    "start": 349,
                    "end": 355,
                    "text": "Fig. 1",
                    "ref_id": null
                },
                {
                    "start": 537,
                    "end": 546,
                    "text": "Figure A1",
                    "ref_id": null
                },
                {
                    "start": 743,
                    "end": 752,
                    "text": "Figure A2",
                    "ref_id": null
                },
                {
                    "start": 1024,
                    "end": 1030,
                    "text": "Fig. 1",
                    "ref_id": null
                }
            ],
            "section": "Appendix A. Simulation study"
        },
        {
            "text": "Model parameters were initially estimated using trajectory matching. As in the simulation study, we initially fitted R 0 , \u03c1, k and the initial conditions. However, profile likelihoods over \u03c1 were flat, indicating a lack of identifiability in the reporting rate due to a trade-off between this parameter and initial conditions. Accordingly, we fixed \u03c1 = 0.2. The flatness of the likelihood profiles indicates that this assumption has no effect on the quality of fit. All other model parameters were fixed at the known values given in Table B1 .",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 534,
                    "end": 542,
                    "text": "Table B1",
                    "ref_id": "TABREF0"
                }
            ],
            "section": "Appendix B. Model-based inference Trajectory matching"
        },
        {
            "text": "Trajectory matching was used to compute likelihood profiles over R 0 and k. For each point in the profile, the other parameters and initial conditions were initialized at 40 points according to a latin hypersquare (Sobol') design. In all, the trajectory matching calculations required approximately 21 cpu hr of computation. Full details of the trajectory matching codes are provided in the Supplementary Material. ",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Appendix B. Model-based inference Trajectory matching"
        },
        {
            "text": "Model parameters were estimated using the Iterated Filtering algorithm (IF2) (Ionides et al., 2015) , implemented as mif in the R package pomp (King et al., 2010) . For each country and each type of data, the parameter estimates along the trajectory-matching profiles were used to initialize the IF2 runs. From each initial point, we performed 60 IF2 iterations using 2\u00d710 3 particles, hyperbolic cooling, and a random walk standard deviation (on the log scale) of 0.02 for all parameters and 1 for initial conditions. For the parameters estimated in each IF run, the log-likelihood was computed as the log of the mean likelihoods of 10 replicate filters, each with 5 \u00d7 10 3 particles. Approximate confidence intervals were then computed using the profile log-likelihood (Raue et al., 2009) . All details of these computations are provided in the Supplementary Material. Computing each of the profile likelihoods in Fig. 2 using iterated filtering took approximately 34 cpu hr of computation; all profile computations were accomplished in roughly 3.6 hr on a 100-cpu cluster. Table B2 shows the maximum likelihood parameter estimates (MLE). Fig. B1 shows a comparison of various summary statistics (\"probes\") computed both on the data and on model simulations.",
            "cite_spans": [
                {
                    "start": 77,
                    "end": 99,
                    "text": "(Ionides et al., 2015)",
                    "ref_id": "BIBREF15"
                },
                {
                    "start": 143,
                    "end": 162,
                    "text": "(King et al., 2010)",
                    "ref_id": "BIBREF16"
                },
                {
                    "start": 771,
                    "end": 790,
                    "text": "(Raue et al., 2009)",
                    "ref_id": "BIBREF17"
                }
            ],
            "ref_spans": [
                {
                    "start": 916,
                    "end": 922,
                    "text": "Fig. 2",
                    "ref_id": null
                },
                {
                    "start": 1076,
                    "end": 1084,
                    "text": "Table B2",
                    "ref_id": "TABREF1"
                },
                {
                    "start": 1141,
                    "end": 1148,
                    "text": "Fig. B1",
                    "ref_id": null
                }
            ],
            "section": "Iterated filtering"
        },
        {
            "text": "To tease apart the consequences of failing to account for stochasticity from those of improperly fitting to cumulative data, we fit both deterministic and stochastic models each to both actual incidence and accumulated incidence data. It is important to recognize that the exercise of fitting the stochastic model to accumulated data is not something one would ever actually do. Indeed, at the outset incompatibility of model assumptions with the data becomes evident. To see this, let H t be the true incidence (i.e., actual number of new infections) in reporting interval t and C t be the number of reported cases in that interval. Because of measurement error, C t = H t +\u03b5 t , where \u03b5 t is the error. Let h t = t s=1 H t and c t = t s=1 C t be the accumulated true and reported incidence, respectively. Because c t = t s=1 (H t + \u03b5 t ), the errors c t \u2212 h t are not independent, which is the fundamental problem associated with fitting to cumulative incidence data, irrespective of whether the model for H t is deterministic or stochastic. If one attempts to fit a stochastic model to c t by modeling c t = h t + \u03be t , where \u03be t are measurement errors, one is confronted with the fact that, even though the accumulated data, c t , and simulations of h t are guaranteed to increase with time, simulations of c t under this model will not in general be monotonically increasing.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Results"
        },
        {
            "text": "Nevertheless, one naturally wonders about the relative importance of the choice to use a deterministic or stochastic model vs. using raw or accumulated incidence data. Although the answer will certainly depend on both model and data, and therefore vary from situation to situation, we present the comparison in the present case to partially satisfy this natural curiosity. For the SEIR model fit to the Sierra Leone outbreak data, Fig. B2 shows likelihood profiles for the four model-data combinations and Fig. B3 shows the corresponding forecasts. Figure B1 : Additional summary statistics, or probes, computed on both stochastic model simulations and the data. In each panel, the probability density of the probes on the simulated data are shown in grey; the blue line indicates the value of the probe on the data. Probes include autocorrelation at lag 1 (ACF), standard deviation (SD) on log-transformed data, 90th percentile, the autocorrelation at lags 1, 2, and 3 wk after removing an exponential trend (DACF), and the exponential growth rate as obtained by log-linear regression (TREND). Figure B2 : R 0 likelihood profiles for four model-data combinations for the SEIR model fit to the Sierra Leone outbreak. Figure B3 : Forecast uncertainty for the Sierra Leone EBVD outbreak as a function of the model used and the data to which the model was fit. The ribbons show the median and 95% envelope of model simulations for the various models fit to raw and cumulative incidence data from the Sierra Leone outbreak. The data used in model fitting are shown using black triangles.",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 431,
                    "end": 438,
                    "text": "Fig. B2",
                    "ref_id": null
                },
                {
                    "start": 506,
                    "end": 513,
                    "text": "Fig. B3",
                    "ref_id": null
                },
                {
                    "start": 549,
                    "end": 558,
                    "text": "Figure B1",
                    "ref_id": null
                },
                {
                    "start": 1095,
                    "end": 1104,
                    "text": "Figure B2",
                    "ref_id": null
                },
                {
                    "start": 1217,
                    "end": 1226,
                    "text": "Figure B3",
                    "ref_id": null
                }
            ],
            "section": "Results"
        },
        {
            "text": "This appendix describes and displays the data and codes needed to fully reproduce all the results of the paper. All referenced files are provided on datadryad.org: doi:10.5061/dryad.r5f30.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Appendix C. Data and codes"
        },
        {
            "text": "The following installs the necessary packages, if they are not already installed.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "R packages"
        },
        {
            "text": "pkgs <-c(\"plyr\",\"pomp\",\"reshape2\",\"magrittr\",\"ggplot2\",\"scales\", \"foreach\",\"doMC\",\"doMPI\",\"iterators\")",
            "cite_spans": [],
            "ref_spans": [],
            "section": "R packages"
        },
        {
            "text": "The following are the version numbers of these packages used in performing the study.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "R packages"
        },
        {
            "text": "R.version.string sapply(pkgs,function(x)as.character(packageVersion(x))) ## [1] \"R version 3.1.2 (2014-10-31)\" ## plyr pomp reshape2 magrittr ggplot2 scales foreach ## \"1.8.1\" \"0.62.5\" \"1.4.1\" \"1.5\" \"1.0.0\" \"0.2.4\" \"1.4.2\" ## doMC doMPI iterators ## \"1.3.3\" \"0.2.1\" \"1.0.7\"",
            "cite_spans": [],
            "ref_spans": [],
            "section": "R packages"
        },
        {
            "text": "Most of the codes described below are designed to be run on a cluster using OpenMPI. It will be assumed that an MPI hostfile, hosts, exists, which specifes the hosts to be used. An example hosts file has contents localhost slots=1 max-slots=1 node1 slots=0 max-slots=10 node2 slots=0 max-slots=10 node3 slots=0 max-slots=10 node4 slots=0 max-slots=10",
            "cite_spans": [],
            "ref_spans": [],
            "section": "MPI"
        },
        {
            "text": "Here, node1, node2, node3, and node4 are the compute nodes in this small cluster. Each has at least 10 cores.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "MPI"
        },
        {
            "text": "The file ebola.R, shown below, will be sourced by the each of the codes below. It defines a function ebolaModel that constructs a pomp object holding the Ebola data and the model codes.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Model implementation"
        },
        {
            "text": "Contents of the file ebola.R:",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Model implementation"
        },
        {
            "text": "week,Guinea,Liberia,SierraLeone 1,2.244,, 2,2.244,, 3,0.073, , 4,5.717,, 5,3.954,, 6,5.444,, 7,3.274,, 8,5.762,, 9,7.615,, 10,7.615,, 11,27.392,, 12,17.387,, 13,27.115,, 14,29.29,, 15,27.84,, 16,16.345,, 17,10.917,, 18,11.959,, 19,11.959,, 20,8.657,, 21,26.537,, 22,47.764 ebolaModel <-function (country=c(\"Guinea\", \"SierraLeone\", \"Liberia\", \"WestAfrica\"), data = NULL,",
            "cite_spans": [
                {
                    "start": 61,
                    "end": 272,
                    "text": ", 4,5.717,, 5,3.954,, 6,5.444,, 7,3.274,, 8,5.762,, 9,7.615,, 10,7.615,, 11,27.392,, 12,17.387,, 13,27.115,, 14,29.29,, 15,27.84,, 16,16.345,, 17,10.917,, 18,11.959,, 19,11.959,, 20,8.657,, 21,26.537,, 22,47.764",
                    "ref_id": null
                }
            ],
            "ref_spans": [],
            "section": "Model implementation"
        },
        {
            "text": "## Incubation period is supposed to be Gamma distributed with shape parameter 3 ## and mean 11.4 days. The discrete-time formula is used to calculate the ## corresponding alpha (cf He et al., Interface 2010). ## Case-fatality ratio is fixed at 0.7 (cf WHO Ebola response team, NEJM 2014) incubation_period <-11.4/7 infectious_period <-7/7 index_case <-10/pop dt <-timestep nstageE <-as.integer(nstageE) globs <-paste0(\"static int nstageE = \",nstageE,\";\"); theta <-c(N=pop,R0=1.4, alpha=-1/(nstageE*dt)*log(1-nstageE*dt/incubation_period), gamma=-log(1-dt/infectious_period)/dt, rho=0.2,cfr=0.7, k=0, S_0=1-index_case,E_0=index_case/2-5e-9,",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Model implementation"
        },
        {
            "text": "dat <-mutate(dat,cases=cumsum(cases),deaths=cumsum(deaths)) } ## Create the pomp object pomp( data=dat[c(\"week\",\"cases\",\"deaths\")], times=\"week\", t0=0, params=theta, globals=globs, obsnames=c(\"cases\",\"deaths\"), statenames=c(\"S\",\"E1\",\"I\",\"R\",\"N_EI\",\"N_IR\"), zeronames=if (type==\"raw\") c(\"N_EI\",\"N_IR\") else character(0), paramnames=c(\"N\",\"R0\",\"alpha\",\"gamma\",\"rho\",\"k\",\"cfr\", \"S_0\",\"E_0\",\"I_0\",\"R_0\"), nstageE=nstageE, dmeasure=if (least.sq) dObsLS else dObs, rmeasure=if (least.sq) rObsLS else rObs, rprocess=discrete.time.sim(step.fun=rSim,delta.t=timestep), skeleton=skel, skeleton.type=\"vectorfield\", parameter.transform=partrans, parameter.inv.transform=paruntrans, initializer=function (params, t0, nstageE, ...) { all.state.names <-c(\"S\",paste0(\"E\",1:nstageE),\"I\",\"R\",\"N_EI\",\"N_IR\") comp.names <-c(\"S\",paste0(\"E\",1:nstageE),\"I\",\"R\") x0 <-setNames ( ",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Model implementation"
        },
        {
            "text": "The codes in file, simstudy.R, shown below, perform the simulation study computations. Note that it assumes that file ebola.R (contents displayed above) is present in the working directory. On a cluster with R and OpenMPI installed, in a directory containing simstudy.R, ebola.R, and the hostfile hosts, execute the computations with a command such as: po <-ebolaModel(country=\"Guinea\",timestep=0.1) params <-parmat(coef(po),3) params[\"k\",] <-c(0,0.2,0.5) paramnames <-names(coef(po)) nsims <-500 bake(file=\"sims.rds\",{ simulate(po,params=params,nsim=nsims,seed=208335746L, as.data.frame=TRUE,obs=TRUE) %>% rename(c(time=\"week\")) %>% mutate(k=params[\"k\",((as.integer(sim)-1)%%ncol(params))+1]) }) -> simdat pompUnload(po) bake(file=\"tm-sim-profiles-R0.rds\",{ foreach(simul=1:nsims, .combine=rbind,.inorder=FALSE, .noexport=noexport, .options.mpi=list(chunkSize=10,seed=1598260027L,info=TRUE)) %:% foreach(type=c(\"raw\",\"cum\"),.combine=rbind,.inorder=FALSE) %dopar% { dat <-subset(simdat,sim==simul,select=c(week,cases,deaths)) tm <-ebolaModel(country=\"Guinea\",data=dat,type=as.character(type)) st <-params[,(simul-1)%%3+1] true.k <-unname(st[\"k\"]) true.R0 <-unname(st[\"R0\"]) true.rho <-unname(st[\"rho\"]) st[\"k\"] <-st[\"k\"]+1e-6 tm <-traj.match(tm,start=st,est=c(\"R0\",\"k\",\"rho\"),transform=TRUE) if (coef(tm,\"rho\")==1) coef(tm,\"rho\") <-0.999 if (coef(tm,\"rho\")==0) coef(tm,\"rho\") <-0.001 tm <-traj.match(tm,est=c(\"R0\",\"k\",\"rho\"),method='subplex',transform=TRUE) pompUnload(tm) data.frame(sim=simul,type=as.character(type), true.k=true.k,true.R0=true.R0,true.rho=true.rho, as.list(coef(tm)),loglik=logLik(tm), conv=tm$convergence) } -> fits foreach (fit=iter(fits,by=\"row\"), .noexport=noexport, .combine=rbind,.inorder=FALSE) %:% foreach (r0=seq(from=0.7,to=3,length=200), .combine=rbind,.inorder=FALSE, .options.mpi=list(chunkSize=200,seed=1598260027L,info=TRUE)) %dopar% { dat <-subset(simdat,sim==fit$sim,select=c(week,cases,deaths)) tm <-ebolaModel(country=\"Guinea\",data=dat,type=as.character(fit$type)) coef(tm) <-unlist(fit[paramnames]) coef(tm,\"R0\") <-r0 if (coef(tm,\"rho\")==1) coef(tm,\"rho\") <-0.999 if (coef(tm,\"rho\")==0) coef(tm,\"rho\") <-0.001 tm <-traj.match(tm,est=c(\"k\",\"rho\"),transform=TRUE) if (coef(tm,\"rho\")==1) coef(tm,\"rho\") <-0.999 if (coef(tm,\"rho\")==0) coef(tm,\"rho\") <-0.001 tm <-traj.match(tm,est=c(\"k\",\"rho\"),transform=TRUE,method='subplex') pompUnload(tm) data.frame(sim=fit$sim,type=fit$type, true.k=fit$true.k,true.R0=fit$true.R0,true.rho=fit$true.rho, as.list(coef(tm)),loglik=logLik(tm), conv=tm$convergence) } }) -> profiles bake(file=\"tm-sim-fits.rds\",{ ddply(profiles,~type+sim+true.k,subset,loglik==max(loglik)) -> starts foreach(fit=iter(starts,by=\"row\"),.combine=rbind,.inorder=FALSE, .noexport=noexport, .options.mpi=list(chunkSize=30,seed=1598260027L,info=TRUE)) %dopar% { dat <-subset(simdat,sim==fit$sim,select=c(week,cases,deaths)) tm <-ebolaModel(country=\"Guinea\",data=dat,type=as.character(fit$type)) coef(tm) <-unlist(fit[paramnames]) if (coef(tm,\"rho\")==1) coef(tm,\"rho\") <-0.999 if (coef(tm,\"rho\")==0) coef(tm,\"rho\") <-0.001 tm <-traj.match(tm,est=c(\"k\",\"rho\"),transform=TRUE) if (coef(tm,\"rho\")==1) coef(tm,\"rho\") <-0.999 if (coef(tm,\"rho\")==0) coef(tm,\"rho\") <-0.001 tm <-traj.match(tm,est=c(\"k\",\"rho\"),transform=TRUE,method='subplex') pompUnload(tm) data.frame(sim=fit$sim,type=fit$type, true.k=fit$true.k,true.R0=fit$true.R0,true.rho=fit$true.rho, as.list(coef(tm)),loglik=logLik(tm), conv=tm$convergence) } }) -> fits toc <-Sys.time() print(toc-tic) ## trajectory matching with least squares simulation study tic <-Sys.time() bake(file=\"ls-sim-profiles-R0.rds\",{ foreach(simul=1:nsims, .combine=rbind,.inorder=FALSE, .noexport=noexport, .options.mpi=list(chunkSize=10,seed=1598260027L,info=TRUE)) %:% foreach(type=c(\"raw\",\"cum\"),.combine=rbind,.inorder=FALSE) %dopar% { dat <-subset(simdat,sim==simul,select=c(week,cases,deaths)) tm <-ebolaModel(country=\"Guinea\",data=dat,type=as.character(type), least.sq=TRUE) st <-params[,(simul-1)%%3+1] true.k <-unname(st[\"k\"]) true.R0 <-unname(st[\"R0\"]) true.rho <-unname(st[\"rho\"]) st[\"k\"] <-10 tm <-traj.match(tm,start=st,est=c(\"R0\",\"k\",\"rho\"),transform=TRUE) if (coef(tm,\"rho\")==1) coef(tm,\"rho\") <-0.999 if (coef(tm,\"rho\")==0) coef(tm,\"rho\") <-0.001 tm <-traj.match(tm,est=c(\"R0\",\"k\",\"rho\"),method='subplex',transform=TRUE) pompUnload(tm) data.frame(sim=simul,type=as.character(type), true.k=true.k,true.R0=true.R0,true.rho=true.rho, as.list(coef(tm)),loglik=logLik(tm), conv=tm$convergence) } -> fits foreach (fit=iter(fits,by=\"row\"), .noexport=noexport, .combine=rbind,.inorder=FALSE) %:% foreach (r0=seq(from=0.7,to=3,length=200), .combine=rbind,.inorder=FALSE, .options.mpi=list(chunkSize=200,seed=1598260027L,info=TRUE)) %dopar% { dat <-subset(simdat,sim==fit$sim,select=c(week,cases,deaths)) tm <-ebolaModel(country=\"Guinea\",data=dat,type=as.character(fit$type), least.sq=TRUE) coef(tm) <-unlist(fit[paramnames]) coef(tm,\"R0\") <-r0 if (coef(tm,\"rho\")==1) coef(tm,\"rho\") <-0.999 if (coef(tm,\"rho\")==0) coef(tm,\"rho\") <-0.001 tm <-traj.match(tm,est=c(\"k\",\"rho\"),transform=TRUE) if (coef(tm,\"rho\")==1) coef(tm,\"rho\") <-0.999 if (coef(tm,\"rho\")==0) coef(tm,\"rho\") <-0.001 tm <-traj.match(tm,est=c(\"k\",\"rho\"),transform=TRUE,method='subplex') pompUnload(tm) data.frame(sim=fit$sim,type=fit$type, true.k=fit$true.k,true.R0=fit$true.R0,true.rho=fit$true.rho, as.list(coef(tm)),loglik=logLik(tm), conv=tm$convergence) } }) -> profiles bake(file=\"ls-sim-fits.rds\",{ ddply(profiles,~type+sim+true.k,subset,loglik==max(loglik)) -> starts foreach(fit=iter(starts,by=\"row\"),.combine=rbind,.inorder=FALSE, .noexport=noexport, .options.mpi=list(chunkSize=30,seed=1598260027L,info=TRUE)) %dopar% { dat <-subset(simdat,sim==fit$sim,select=c(week,cases,deaths)) tm <-ebolaModel(country=\"Guinea\",data=dat,type=as.character(fit$type), least.sq=TRUE) coef(tm) <-unlist(fit[paramnames]) if (coef(tm,\"rho\")==1) coef(tm,\"rho\") <-0.999 if (coef(tm,\"rho\")==0) coef(tm,\"rho\") <-0.001 tm <-traj.match(tm,est=c(\"k\",\"rho\"),transform=TRUE) if (coef(tm,\"rho\")==1) coef(tm,\"rho\") <-0.999 if (coef(tm,\"rho\")==0) coef(tm,\"rho\") <-0.001 tm <-traj.match(tm,est=c(\"k\",\"rho\"),transform=TRUE,method='subplex') pompUnload(tm) data.frame(sim=fit$sim,type=fit$type, true.k=fit$true.k,true.R0=fit$true.R0,true.rho=fit$true.rho, as.list(coef(tm)),loglik=logLik(tm), conv=tm$convergence) } }) -> fits toc <-Sys.time() print(toc-tic) closeCluster(cl) mpi.quit()",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Simulation study"
        },
        {
            "text": "The codes in file profiles.R use trajectory matching and iterated filtering to estimate model parameters for all four countries and both types of data. Again, this is designed to be run on an MPI cluster. In a directory with the files hosts and ebola.R (see above), execute these computations via a command like mpirun -hostfile hosts -np 101 Rscript --vanilla profiles.R Contents of file profiles.R: require(pomp) require(plyr) require(reshape2) require(magrittr) options(stringsAsFactors=FALSE) require(foreach) require(doMPI) require(iterators) source(\"ebola.R\") foreach (country=c(\"SierraLeone\",\"Liberia\",\"Guinea\",\"WestAfrica\"), .inorder=TRUE,.combine=c) %:% foreach (type=c(\"raw\",\"cum\"),.inorder=TRUE,.combine=c) %do% { ebolaModel(country=country,type=type,na.rm=TRUE) } -> models dim(models) <-c(2,4) dimnames(models) <-list(c(\"raw\",\"cum\"), c(\"SierraLeone\",\"Liberia\",\"Guinea\",\"WestAfrica\")) noexport <-c(\"models\")",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Parameter estimation"
        },
        {
            "text": "## trajectory matching: R0 profile bake(file=\"tm-fits-R0.rds\",{ starts <-profileDesign (R0=seq(1,3,length=200) , upper=c(k=1), lower=c(k=1e-8), nprof=40)",
            "cite_spans": [],
            "ref_spans": [
                {
                    "start": 87,
                    "end": 110,
                    "text": "(R0=seq(1,3,length=200)",
                    "ref_id": null
                }
            ],
            "section": "Parameter estimation"
        },
        {
            "text": "foreach (start=iter(starts,by='row'), .combine=rbind,.inorder=FALSE, .noexport=noexport, .options.mpi=list(chunkSize=100,seed=2016138277L,info=TRUE) ) %:% foreach (type=c(\"raw\",\"cum\"),.combine=rbind,.inorder=FALSE) %:% foreach (country=c(\"SierraLeone\",\"Liberia\",\"Guinea\",\"WestAfrica\"), .combine=rbind,.inorder=FALSE) %dopar% { tm <-models[type,country][[1]] tic <-Sys.time() coef(tm,names(start)) <-unname(unlist(start)) coef(tm,\"rho\") <-0.2 tm <-traj.match(tm,est=c(\"k\",\"E_0\",\"I_0\"),transform=TRUE) if (coef(tm,\"k\")==0) coef(tm,\"k\") <-1e-8 if (coef(tm,\"E_0\")==0) coef(tm,\"E_0\") <-1e-12 if (coef(tm,\"I_0\")==0) coef(tm,\"I_0\") <-1e-12 tm <-traj.match(tm,method='subplex',control=list(maxit=1e5)) toc <-Sys.time() etime <-toc-tic units(etime) <-\"hours\" data.frame(country=country,type=type,as.list(coef(tm)), loglik=logLik(tm),conv=tm$convergence, etime=as.numeric(etime)) } %>% mutate(sum=S_0+E_0+I_0+R_0, S_0=round(N*S_0/sum), E_0=round(N*E_0/sum), I_0=round(N*I_0/sum), R_0=round(N*R_0/sum)) %>% subset(conv %in% c(0,1),select=-sum) %>% unique() }) -> profR0 ## trajectory matching: k profile bake(file=\"tm-fits-k.rds\",{ starts <-profileDesign(k=seq(0,1,length=100), upper=c(R0=1), lower=c(R0=3), nprof=40)",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Parameter estimation"
        },
        {
            "text": "foreach (start=iter(starts,by='row'), .combine=rbind,.inorder=FALSE, .noexport=noexport, .options.mpi=list(chunkSize=100,seed=2016138277L,info=TRUE) ) %:% foreach (type=c(\"raw\",\"cum\"),.combine=rbind,.inorder=FALSE) %:% foreach (country=c(\"SierraLeone\",\"Liberia\",\"Guinea\",\"WestAfrica\"), .combine=rbind,.inorder=FALSE) %dopar% { tm <-models[type,country][[1]] tic <-Sys.time() coef(tm,names(start)) <-unname(unlist(start)) coef(tm,\"rho\") <-0.2 tm <-traj.match(tm,est=c(\"R0\",\"E_0\",\"I_0\"),transform=TRUE) if (coef(tm,\"E_0\")==0) coef(tm,\"E_0\") <-1e-12 if (coef(tm,\"I_0\")==0) coef(tm,\"I_0\") <-1e-12 tm <-traj.match(tm,method='subplex',control=list(maxit=1e5)) toc <-Sys.time() etime <-toc-tic units(etime) <-\"hours\" data.frame(country=country,type=type,as.list(coef(tm)), loglik=logLik(tm),conv=tm$convergence, etime=as.numeric(etime)) } %>% mutate(sum=S_0+E_0+I_0+R_0, S_0=round(N*S_0/sum), E_0=round(N*E_0/sum), I_0=round(N*I_0/sum), R_0=round(N*R_0/sum)) %>% subset(conv %in% c(0,1),select=-sum) %>% unique() }) -> profk ## All trajectory matching computations ldply(list(R0=profR0,k=profk),.id='profile') -> profTM (st)) <-st if (coef(po,\"E_0\")==0) coef(po,\"E_0\") <-1e-5 if (coef(po,\"I_0\")==0) coef(po,\"I_0\") <-1e-5 mf <-mif(po, Nmif=10, rw.sd = c(k=0.02,E_0=1,I_0=1), ivps = c(\"E_0\",\"I_0\"), Np = 2000, var.factor = 2, method = \"mif2\", cooling.type = \"hyperbolic\", cooling.fraction = 0.5, transform = TRUE, verbose = FALSE) mf <-continue(mf, Nmif = 50, cooling.fraction = 0.1) ## Runs 10 particle filters to assess Monte Carlo error in likelihood pf <-replicate(10,pfilter(mf,Np=5000,max.fail=Inf)) ll <-sapply(pf,logLik) ll <-logmeanexp(ll, se = TRUE) nfail <-sapply(pf,getElement,\"nfail\") toc <-Sys.time() etime <-toc-tic units(etime) <-\"hours\" data.frame (country=country,type=type,as.list(coef(mf) (st)) <-st if (coef(po,\"E_0\")==0) coef(po,\"E_0\") <-1e-5 if (coef(po,\"I_0\")==0) coef(po,\"I_0\") <-1e-5 mf <-mif(po, Nmif=10, rw.sd = c(R0=0.02,E_0=1,I_0=1), ivps = c(\"E_0\",\"I_0\"), Np = 2000, var.factor = 2, method = \"mif2\", cooling.type = \"hyperbolic\", cooling.fraction = 0.5, transform = TRUE, verbose = FALSE) mf <-continue(mf, Nmif = 50, cooling.fraction = 0.1) ## Runs 10 particle filters to assess Monte Carlo error in likelihood pf <-replicate(10,pfilter(mf,Np=5000,max.fail=Inf)) ll <-sapply(pf,logLik) ll <-logmeanexp(ll, se = TRUE) nfail <-sapply(pf,getElement,\"nfail\") toc <-Sys.time() etime <-toc-tic units(etime) <-\"hours\" data.frame(country=country,type=type,as.list(coef (mf) Executing this code will result in the creation of several files, of which profiles.rds is the most important, containing as it does the results of all the parameter estimation.",
            "cite_spans": [
                {
                    "start": 1756,
                    "end": 1799,
                    "text": "(country=country,type=type,as.list(coef(mf)",
                    "ref_id": null
                }
            ],
            "ref_spans": [],
            "section": "Parameter estimation"
        },
        {
            "text": "The codes in file diagnostics.R compute the diagnostics displayed in the paper. These computations are not very heavy, but can be accelerated using doMC if run on a multi-core workstation. In a directory containing profiles.rds (see above) and ebola.R, execute these codes with a command like readRDS(\"profiles.rds\") %>% ddply(~country+type+model,subset,loglik==max(loglik)) %>% subset(type==\"raw\"&model==\"stoch\") -> mles time1 <-c(Guinea=\"2014-01-05\",Liberia=\"2014-06-01\", transform=detrend), trend=probe.trend), nsim=2000,seed=2186L ) %>% as.data.frame() -> pb pb %>% mutate(sim=rownames(pb), kind=ifelse(sim==\"data\",\"data\",\"simulation\"), type=type, country=country) } %>% melt(id=c(\"country\",\"type\",\"kind\",\"sim\"),variable.name=\"probe\") %>% arrange(country,type,probe,kind,sim) %>% saveRDS(file=\"diagnostics-addl-probes.rds\")",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Diagnostics"
        },
        {
            "text": "The codes in file forecasts.R perform all the forecasting computations. In a directory containing ebola.R, profiles.rds, and hosts, a command like mpirun -hostfile hosts -np 101 Rscript --vanilla forecasts.R will result in the execution of these computations.",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Forecasting"
        },
        {
            "text": "Contents of the file forecasts.R: require(pomp) require(plyr) require(reshape2) require(magrittr) options(stringsAsFactors=FALSE) set.seed(988077383L) require(foreach) require(doMPI) require(iterators) source(\"ebola.R\") horizon <-13",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Forecasting"
        },
        {
            "text": "x <-x[idx] weights <-weights[idx] w <-cumsum(weights)/sum(weights) rval <-approx(w,x,probs,rule=1) rval$y } starts <-c(Guinea=\"2014-01-05\",Liberia=\"2014-06-01\",SierraLeone=\"2014-06-08\") cl <-startMPIcluster() registerDoMPI(cl) bake <-function (file, expr) { if (file.exists(file)) { readRDS(file) } else { val <-eval(expr) saveRDS(val,file=file) val } } readRDS(\"profiles.rds\") %>% ddply(~country+type+model,subset,loglik>max(loglik)-6, select=-c(conv,etime,loglik.se,nfail.min,nfail.max,profile)) -> mles mles %>% melt(id=c(\"country\",\"type\",\"model\"),variable.name='parameter') %>% ddply(~country+type+model+parameter,summarize, min=min(value),max=max(value)) %>% subset(parameter!=\"loglik\") %>% melt(measure=c(\"min\",\"max\")) %>% acast(country~type~model~parameter~variable) -> ranges mles %>% ddply(~country+type+model,subset,loglik==max(loglik),select=-loglik) %>% mutate(k=round(k,4),rho=round(rho,4),R0=round(R0,4),E_0=3*round(E_0/3)) %>% unique() %>% arrange(country,type,model) -> mles ### DETERMINISTIC MODELS bake(file=\"forecasts_det.rds\",{ foreach (country=c(\"SierraLeone\"), .inorder=TRUE,.combine=rbind) %:% foreach (type=c(\"raw\",\"cum\"),nsamp=c ( , period=ifelse(time<=max(time(M1)),\"calibration\",\"projection\"), loglik=ll) } %>% subset(variable==\"cases\",select=-variable) %>% mutate(weight=exp(loglik-mean(loglik))) %>% arrange(time,rep) -> sims closeCluster(cl) mpi.quit()",
            "cite_spans": [],
            "ref_spans": [],
            "section": "Forecasting"
        }
    ],
    "bib_entries": {
        "BIBREF0": {
            "ref_id": "b0",
            "title": "SierraLeone\"),.inorder=TRUE,.combine=c) %:% foreach (type=c(\"raw",
            "authors": [],
            "year": null,
            "venue": "inorder=TRUE,.combine=c) %do% { M1 <-ebolaModel",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF1": {
            "ref_id": "b1",
            "title": "M2 <-ebolaModel(country=country,type=\"raw\", timestep=0.01,nstageE=3,na.rm=TRUE) time(M2) <-seq(from=1,to=max(time(M1))+horizon,by=1)",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF2": {
            "ref_id": "b2",
            "title": "rm=TRUE) time(M3) <-seq(from=max(time(M1))+1,to=max(time(M1))+horizon,by=1) timezero(M3) <-max(time(M1)) list(M1,M2,M3) } -> models dim(models) <-c(3,2,1) dimnames(models) <-list(c(\"fit",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF4": {
            "ref_id": "b4",
            "title": "## Weighted quantile function wquant <-function",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF6": {
            "ref_id": "b6",
            "title": "ESS det",
            "authors": [],
            "year": null,
            "venue": "ess <-1/sum(ess^2) cat",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF7": {
            "ref_id": "b7",
            "title": "dcast(period+time~prob,value.var='quantile') %>% mutate(country=country,type=type)",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF8": {
            "ref_id": "b8",
            "title": "cum\"),nsamp=c(200,200), .inorder=TRUE,.combine=rbind) %do% { params <-sobolDesign(lower=ranges[country,type,'stoch','min'], upper=ranges[country,type,'stoch','max'], nseq=nsamp) foreach(p=iter(params,by='row'), .inorder=FALSE, .combine=rbind, .noexport=noexport, .options.multicore=list(set.seed=TRUE), .options.mpi=list(chunkSize=1,seed=1568335316L,info=TRUE) ) %dopar% { M1 <-models",
            "authors": [],
            "year": null,
            "venue": "### STOCHASTIC MODEL bake(file=\"forecasts_stoch.rds\",{ foreach (country=c",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF10": {
            "ref_id": "b10",
            "title": "Np=2000,save.states=TRUE) pf$saved.states %>% tail(1) %>% melt() %>% acast(variable~rep,value.var='value') %>% apply(2,function (x) { setNames(c(x",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF11": {
            "ref_id": "b11",
            "title": "<-x simulate(M2,params=pp,obs=TRUE) %>% melt() %>% mutate",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF12": {
            "ref_id": "b12",
            "title": "select=-variable) %>% mutate(weight=exp(loglik-mean(loglik))) %>% arrange(time,rep) -> sims ess <-with",
            "authors": [
                {
                    "first": "}",
                    "middle": [],
                    "last": "%&gt;%",
                    "suffix": ""
                }
            ],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF13": {
            "ref_id": "b13",
            "title": "ess <-1/sum(ess^2) cat(\"ESS stoch",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF14": {
            "ref_id": "b14",
            "title": "%>% mutate(country=country,type=type) } }) -> fc_if ldply(list(stoch=fc_if,det=fc_tm),.id='model') %>% ddply(~country,mutate, model=factor(model,levels=c(\"stoch",
            "authors": [],
            "year": null,
            "venue": "dcast(period+time~prob,value.var='quantile')",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF15": {
            "ref_id": "b15",
            "title": "Inference for dynamic and latent variable models via iterated, perturbed Bayes maps",
            "authors": [
                {
                    "first": "E",
                    "middle": [
                        "L"
                    ],
                    "last": "Ionides",
                    "suffix": ""
                },
                {
                    "first": "D",
                    "middle": [],
                    "last": "Nguyen",
                    "suffix": ""
                },
                {
                    "first": "Y",
                    "middle": [],
                    "last": "Atchad\u00e9",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Stoev",
                    "suffix": ""
                },
                {
                    "first": "A",
                    "middle": [
                        "A"
                    ],
                    "last": "King",
                    "suffix": ""
                }
            ],
            "year": 2015,
            "venue": "Proceedings of the National Academy of Sciences of the U.S.A",
            "volume": "112",
            "issn": "",
            "pages": "719--724",
            "other_ids": {}
        },
        "BIBREF16": {
            "ref_id": "b16",
            "title": "pomp: Statistical inference for partially observed Markov processes",
            "authors": [
                {
                    "first": "A",
                    "middle": [
                        "A"
                    ],
                    "last": "King",
                    "suffix": ""
                },
                {
                    "first": "E",
                    "middle": [
                        "L"
                    ],
                    "last": "Ionides",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [
                        "M"
                    ],
                    "last": "Bret\u00f3",
                    "suffix": ""
                },
                {
                    "first": "S",
                    "middle": [],
                    "last": "Ellner",
                    "suffix": ""
                },
                {
                    "first": "B",
                    "middle": [],
                    "last": "Kendall",
                    "suffix": ""
                },
                {
                    "first": "H",
                    "middle": [],
                    "last": "Wearing",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [
                        "J"
                    ],
                    "last": "Ferrari",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Lavine",
                    "suffix": ""
                },
                {
                    "first": "D",
                    "middle": [
                        "C"
                    ],
                    "last": "Reuman",
                    "suffix": ""
                }
            ],
            "year": 2010,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": "",
            "other_ids": {}
        },
        "BIBREF17": {
            "ref_id": "b17",
            "title": "Structural and practical identifiability analysis of partially observed dynamical models by exploiting the profile likelihood",
            "authors": [
                {
                    "first": "A",
                    "middle": [],
                    "last": "Raue",
                    "suffix": ""
                },
                {
                    "first": "C",
                    "middle": [],
                    "last": "Kreutz",
                    "suffix": ""
                },
                {
                    "first": "T",
                    "middle": [],
                    "last": "Maiwald",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Bachmann",
                    "suffix": ""
                },
                {
                    "first": "M",
                    "middle": [],
                    "last": "Schilling",
                    "suffix": ""
                },
                {
                    "first": "U",
                    "middle": [],
                    "last": "Klingm\u00fcller",
                    "suffix": ""
                },
                {
                    "first": "J",
                    "middle": [],
                    "last": "Timmer",
                    "suffix": ""
                }
            ],
            "year": 2009,
            "venue": "Bioinformatics",
            "volume": "25",
            "issn": "",
            "pages": "1923--1932",
            "other_ids": {}
        },
        "BIBREF18": {
            "ref_id": "b18",
            "title": "Ebola virus disease in West Africa-the first 9 months of the epidemic and forward projections",
            "authors": [],
            "year": 2014,
            "venue": "New England Journal of Medicine",
            "volume": "371",
            "issn": "",
            "pages": "1481--1495",
            "other_ids": {
                "PMID": [
                    "25244186"
                ]
            }
        }
    },
    "ref_entries": {
        "TABREF0": {
            "text": "Model parameters, with their interpretations, and their assumed values (parameters estimated from incidence data are so indicated) together with the source of evidence for the assumption.",
            "latex": null,
            "type": "table"
        },
        "TABREF1": {
            "text": "Parameter estimates for the stochastic and deterministic models on the raw data. MLE point estimates with nominal 95% confidence intervals are shown.",
            "latex": null,
            "type": "table"
        },
        "TABREF2": {
            "text": "## Measurement model: hierarchical model for cases ## p(C_t | H_t): Negative binomial with mean rho*H_t and variance rho*H_t*(1+k*rho*H_t)rObs <-Csnippet(' if (k > 0) { cases = rnbinom_mu(1.0/k,rho*N_EI); deaths = rnbinom_mu(1.0/k,rho*cfr*N_IR); } else { cases = rpois(rho*N_EI); deaths = rpois(rho*cfr*N_IR); }') ### measurement model for ordinary least-squares From class I double transI = rbinom(I, 1.0 -exp(-gamma * dt)); // No of transitions I->R // Balance the equations S -= transS; E[0] += transS -transE[0]; for(i=1; i < nstageE; i++) { E[i] += transE[i-1] -transE[i]; } I += transE[nstageE -1] -transI; R += transI; N_EI += transE[nstageE -1]; // No of transitions from E to I N_IR += transI; // No of transitions from I to R ') ## Deterministic skeleton (an ODE), used in trajectory matching DI = nstageE * alpha * E[nstageE-1] -gamma * I; DR = gamma * I; DN_EI = nstageE * alpha * E[nstageE-1]; DN_IR = gamma * I; ')",
            "latex": null,
            "type": "table"
        },
        "TABREF4": {
            "text": "mpirun -hostfile hosts -np 41 Rscript --vanilla simstudy.R Contents of file simstudy.R:",
            "latex": null,
            "type": "table"
        },
        "TABREF5": {
            "text": "## Iterated filtering, R0 profile bake(file=\"if-fits-R0_a.rds\",{",
            "latex": null,
            "type": "table"
        },
        "TABREF6": {
            "text": "## Filter once more on maxima bake(file=\"if-fits-R0.rds\",{",
            "latex": null,
            "type": "table"
        },
        "TABREF7": {
            "text": "## Filter once more on maxima bake(file=\"if-fits-k.rds\",{ profk %>% subset(is.finite(loglik)&nfail.max==0) %>% ddply(~country+type+R0,subset,rank(-loglik)<=5) %>% subset(select=-c(loglik,loglik.se,nfail.max,nfail.min,etime)) -> pars ## Runs 10 particle filters to assess Monte Carlo error in likelihood pf <-try(replicate(10,pfilter(po,Np=5000,max.fail=Inf))) ldply(list(R0=profR0,k=profk),.id='profile') -> profIF ldply(list(det=profTM,stoch=profIF),.id='model') %>% saveRDS(file='profiles.rds')",
            "latex": null,
            "type": "table"
        },
        "TABREF8": {
            "text": "Rscript --vanilla diagnostics.RContents of file diagnostics.R:",
            "latex": null,
            "type": "table"
        },
        "TABREF9": {
            "text": "1000,3000), .inorder=TRUE,.combine=rbind) %do% { params <-sobolDesign(lower=ranges[country,type,'det',,'min'], upper=ranges[country,type,'det',,'max'], nseq=nsamp)",
            "latex": null,
            "type": "table"
        }
    },
    "back_matter": [
        {
            "text": "SierraLeone=\"2014-06-08\",WestAfrica=\"2014-01-05\") registerDoMC (4) foreach (mle=iter(mles,by='row'),.combine=rbind) %dopar% { country=as.character(mle$country) type=as.character(mle$type) M <-ebolaModel(country=country,type=type, na.rm=TRUE,nstage=3,timestep=0.01) p <-unlist(subset(mle,select=-c(country,type,model,profile, loglik,loglik.se, nfail.min,nfail.max,conv,etime) )) coef(M,names(p)) <-unname(unlist(p)) t0 <-as.Date(time1[country]) simulate(M,nsim=10,as.data.frame=TRUE,obs=TRUE,include.data=TRUE,seed=2186L) %>% mutate(date=t0+time*7,country=country,type=type) } %>% saveRDS(file=\"diagnostics-sim.rds\") foreach (mle=iter(mles,by='row'),.combine=rbind) %dopar% { country=as.character(mle$country) type=as.character(mle$type) M <-ebolaModel(country=country,type=type, na.rm=TRUE,nstage=3,timestep=0.01) p <-unlist(subset(mle,select=-c(country,type,model,profile, loglik,loglik.se, nfail.min,nfail.max,conv,etime))) coef(M,names(p)) <-unname(unlist(p)) probe(M,probes=list(probe.acf(var=\"cases\",lags=1,type=\"correlation\")), nsim=500,seed=1878812716L) %>% as.data.frame() -> pb pb %>% mutate(sim=rownames(pb), data=ifelse(sim==\"data\",\"data\",\"simulation\"), type=type, country=country) } %>% saveRDS(file=\"diagnostics-probes.rds\") ## Additional diagnostics ## Run probes for each country ## Custom probe: exponential growth rate probe.trend <-function (y -unlist(subset(mle,select=-c(country,type,model,profile, loglik,loglik.se, nfail.min,nfail.max,conv,etime) )) coef(M,names(p)) <-unname(unlist(p)) ## remove an exponential trend, give residuals on the log scale dm <-model.matrix(lm(log1p(cases)~time,data=as.data.frame(M))) rm <-diag(nrow(dm))-dm%*%solve(crossprod(dm))%*%t(dm) detrend <-function (x) log1p(x)%*%rm probe(M,probes=list( probe.acf(var=\"cases\",lags=1,type=\"correlation\"), sd=probe.sd(var=\"cases\",transform=log1p), probe.quantile(var=\"cases\",prob=c(0.9)), d=probe.acf(var=\"cases\",lags=c(1,2,3),type=\"correlation\",",
            "cite_spans": [
                {
                    "start": 267,
                    "end": 374,
                    "text": "<-unlist(subset(mle,select=-c(country,type,model,profile, loglik,loglik.se, nfail.min,nfail.max,conv,etime)",
                    "ref_id": null
                },
                {
                    "start": 1362,
                    "end": 1468,
                    "text": "-unlist(subset(mle,select=-c(country,type,model,profile, loglik,loglik.se, nfail.min,nfail.max,conv,etime)",
                    "ref_id": null
                }
            ],
            "ref_spans": [],
            "section": "annex"
        }
    ]
}